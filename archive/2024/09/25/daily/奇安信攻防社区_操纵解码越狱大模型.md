---
title: 操纵解码越狱大模型
url: https://forum.butian.net/share/3766
source: 奇安信攻防社区
date: 2024-09-25
fetch_date: 2025-10-06T18:21:33.585958
---

# 操纵解码越狱大模型

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 操纵解码越狱大模型

* [漏洞分析](https://forum.butian.net/topic/48)

大模型的越狱攻击（Jailbreaking）是指通过特定技术手段或策略，绕过大模型的安全限制、内容过滤或使用规则，获取模型本不允许生成或处理的内容。

前言
==
大模型的越狱攻击（Jailbreaking）是指通过特定技术手段或策略，绕过大模型的安全限制、内容过滤或使用规则，获取模型本不允许生成或处理的内容。
![image-20240722232546340.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-12d722a4dbb4927754cbf3e9c35fa7f964094fc9.png)
这种攻击方式通常涉及以下几个方面：
1. \*\*绕过内容过滤\*\*：大模型通常内置有内容过滤机制，以避免生成不适当、危险或非法的内容。越狱攻击尝试通过修改输入、使用隐晦的表达方式或其他手段来绕过这些过滤器。
2. \*\*操控模型输出\*\*：攻击者可能试图通过精心设计的输入引导模型产生特定的、不被允许的输出，例如敏感信息、虚假信息或其他违背使用规则的内容。
3. \*\*规避使用限制\*\*：一些大模型有使用限制，例如不允许生成与某些主题相关的内容。越狱攻击可能尝试找到绕过这些限制的方法，使模型输出被禁止的内容。
越狱攻击不仅涉及技术层面的问题，还涉及伦理和法律问题。
目前有很多方法被提出实现越狱攻击，比如最典型的Zou等人提出的方法，成功找到了可以跨多个LLMs传递的对抗性提示，包括专有的、黑盒模型。然而，优化对抗性输入的自动越狱相当复杂且计算成本高昂。
在本文中，我们介绍一种非常简单的方法来越狱LLMs。与对抗提示不同，我们只操作文本生成配置，通过删除系统提示、一个有意添加以引导模型生成的准则，并通过变化解码超参数或采样方法。这种方案背后的关键假设是现有的对齐程序和评估可能基于默认的解码设置，当配置稍有变化时可能会表现出脆弱性。
这种方法的示意图如下所示
![image-20240722232755070.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-a1c3589d330a52063ae6028f01072533b0ac2d06.png)
上图是LLAMA2-7B-CHAT 模型在不同生成配置下对恶意指令的响应。在这个例子中，我们仅将 top-p 采样中的 p 从 0.9（默认值）更改为 0.75，从而成功绕过了安全限制，实现了越狱攻击。
背景
==
语言模型
----
语言建模的任务是在给定先前上下文的情况下预测序列中的下一个词，它是目前所有LLMs的基础。正式地说，给定一个由n个标记组成的输入序列x = x1, x2, ..., xn，语言模型计算下一个标记的条件概率分布：
![image-20240722232942342.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-161d8df2eabb704a65e017539648cd9173eff698.png)
其中τ是一个温度参数，控制下一个标记分布的锐度。对于文本生成，模型递归地从条件分布
![image-20240722233012775.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-0a6082407ca5dd94dcd2126b463dc2f07ad642da.png)
中采样以生成下一个标记xi，继续这个过程，直到产生一个序列结束标记。
生成配置
----
在引导大型语言模型生成朝向与人类对齐的输出方面，添加系统提示是一个广泛使用的技术。
如下表所示，就是一些常见的系统提示。
![image-20240722233141816.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-08ba6fc4e419b80b9adf5446272d9dad53bb0f1a.png)
另外，系统提示也通常用于上下文蒸馏的微调中：首先使用系统提示生成更安全的模型响应，然后在没有系统提示的情况下对模型进行微调，这本质上是将系统提示（即上下文）蒸馏到模型中。
然后我们还需要了解另一个知识点，就是解码方法Decoding methods。
在每个步骤i，给定预测的下一个标记分布
![image-20240722233241387.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-093bb293ad63d605eeb90b1aa5e796ab07bfb7c1.png)
可以应用多种解码策略来选择下一个标记xi。最常见的策略是基于采样的解码，其中xi是从分布中随机采样的。贪婪解码，即在
![image-20240722233251875.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-6199c0da56b835b6d7071a9551afcfff753874af.png)
下选择最可能的标记，这是当温度τ = 0时的采样特例。基于采样的解码的变体包括top-p采样和top-k采样，这些策略限制采样到最可能的标记。
默认设置
----
开源LLMs通常仅使用默认生成方法进行对齐评估，这可能使它们在使用替代策略时容易对齐不一致。例如，Touvron等人在先前的研究中，就是在LLAMA2上使用单一解码方法进行了广泛的对齐评估：top-p采样，p = 0.9，τ = 0.1，在论文中出现如下信息：
- 在LLAMA2论文的第4.1节中：“对于解码，我们将温度设置为0.1，并使用top-p设置为0.9的核采样”；
- 在LLAMA2论文的附录A.3.7中：“在收集生成时，我们在评估的提示前附加系统提示”。
虽然这种方法实用，但这种方法可能忽略了当模型的对齐在使用其他生成策略时显著恶化的情况。
方法
==
前面讲过，我们的方法的核心思想很简单，就是探索了各种生成策略，主要围绕系统提示System Prompt和解码策略。
在大语言模型（LLM）中，System Prompt是一种特殊的提示词，它在用户与模型的交互中起到了至关重要的作用。System Prompt主要用于在对话开始时为模型提供初始的上下文信息，这包括模型的角色设定、背景信息以及后续行为的指导。与普通的User Prompt相比，System Prompt在整个会话过程中持续影响模型的回复，并且具有更高的优先级。System Prompt的主要功能是为模型提供一种持久的上下文，让模型能够在多轮的对话中维持一致的回答风格和行为模式。例如，如果System Prompt中设置了模型应当以一种幽默风趣的风格回答问题，那么在随后的对话中，模型将会努力保持这种风格。
我们考虑了以下两种情况：1）在用户指令前添加它，或者2）不包括它。
![image-20240722233815699.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-b187bc76c9d8542ead2d5596fb72ed49bb703a01.png)
在解码策略方面，解码方法是指从模型生成的概率分布中选择下一个输出标记的过程。解码策略对于生成文本的质量和多样性有着至关重要的作用。解码方法主要包括贪心解码、束搜索解码、随机搜索等。这些方法各有特点，适用于不同的应用场景和需求。
贪心解码是最简单的解码策略，它每次选择概率最高的词作为下一个输出词。这种策略速度较快，但不能保证生成的文本多样性，容易导致重复或单调的文本生成。束搜索解码是一种启发式搜索算法，它在每个时间步保留若干个（束宽个）概率最高的候选序列，并逐步扩展这些序列直至达到最大长度或结束符出现。束搜索相比贪心解码能较好地平衡文本的质量和多样性，但计算复杂度更高，解码速度慢。随机搜索是从概率分布中随机选择下一个词，这样可以增加生成的文本多样性，但可能导致生成的文本不连贯。
![image-20240722234014303.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-aa633ada4b0322589762079e365a8555f3865cdd.png)
不过在实验的时候，主要尝试了以下三种变体：
- 温度采样，变化温度τ。温度控制下一个标记分布的锐度，我们将其从0.05变化到1，步长为0.05，这为我们提供了20种配置。
- Top-K采样过滤掉K个最可能的下一个词，然后下一个预测词将仅在这些K个词中采样。我们在{1, 2, 5, 10, 20, 50, 100, 200, 500}中变化K，这为我们提供了9种配置。
- Top-p采样（或核采样）从最小可能的词集中选择，这些词的累积概率超过概率p。我们从0.05变化到1变化p，步长为0.05，这为我们提供了20种配置。
对于每个提示，攻击者生成49个响应（即，对于上述每种解码配置采样一次）。在所有生成的响应中，攻击者使用评分器选择得分最高的单个响应，并将其用作指令的最终响应。
当我们做实验的时候，发现攻击成功率（ASR）在简单地移除系统提示后显著增加，通常增加超过10%。我们观察到系统提示的存在在保持一致输出中起着关键作用，特别是对于那些没有经过安全调整的模型。在这些模型中，移除系统提示可能导致攻击成功率增加超过50%。然而，即使对于明确经过安全对齐的模型，即LLAMA2聊天模型，移除系统提示仍然增加了ASR。这表明在对齐中采取的上下文蒸馏方法可能没有预期的那么有效。此外，利用解码策略还可以进一步提高ASR。
复现
==
我们首先来分析代码
![image-20240722234248380.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-8c08141847172e935c0fe113c9bc9e65c04b1e3d.png)
这段代码主要涉及了自然语言处理和模型的使用。包括几个主要的部分和功能：
1. \*\*导入库\*\*：
- `os`：用于与操作系统交互，比如文件和目录操作。
- `pandas` 和 `numpy`：用于数据处理和数值计算。
- `argparse`：用于处理命令行参数。
- `transformers`：用于加载和使用预训练的语言模型和分词器。
- `torch`：用于深度学习，特别是PyTorch框架。
- `logging`：用于记录日志信息。
- `tqdm`：用于显示循环进度条。
- `configs`：从这个自定义模块中导入了模型路径映射。
- `warnings`：用于处理警告信息。
2. \*\*日志和警告配置\*\*：
- 设置日志记录的级别为INFO，以便记录信息级别的日志。
- 设定警告过滤规则，忽略所有警告信息。
3. \*\*默认系统提示\*\*：
- 定义了一个默认的系统提示，用于对话模型，以确保其回答既帮助又安全，同时避免提供不准确或偏颇的信息。
4. \*\*`prepend\_sys\_prompt` 函数\*\*：
- 这个函数接受一段文本和参数。如果参数中指定了使用系统提示，则在文本前添加默认的系统提示。这用于确保模型在回答问题时遵循特定的指导方针。
5. \*\*`get\_sentence\_embedding` 函数\*\*：
- 这个函数的目的是获取输入句子的嵌入表示。首先，对输入句子进行预处理，如去除多余的空格和引号。然后，利用分词器将句子转换为模型输入的张量，并将其移动到模型所在的设备上。接着，使用模型的词嵌入层生成句子的嵌入表示。
代码的目的是设置和使用自然语言处理模型来生成文本的嵌入表示，并确保模型的回答符合一定的规范。
![image-20240722234501488.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-ad398bdbe50f958b72b22532fed5329e836cff69.png)
这段代码负责加载模型和分词器，并根据提供的参数设置文件和目录路径：
1. \*\*路径设置\*\*：
- `WEIGHTS\_PATH` 和 `TOKENIZER\_PATH` 都指向模型权重的路径。
- `fname` 是生成的文件名，它会根据输入的参数 (`args`) 进行调整，以便保存模型输出或相关数据时能够区分不同的设置。
2. \*\*生成输出目录\*\*：
- 如果指定的输出目录 (`outputs/{fname}`) 不存在，则创建该目录。这是为了确保结果可以正确保存到预期位置。
3. \*\*模型加载\*\*：
- 根据模型名称 (`args.model`)，选择不同的参数来加载模型。如果模型名称中包含 "falcon" 或 "mpt"，则使用 `bfloat16` 数据类型来加载模型；否则，使用 `float16` 数据类型。加载模型时，设置了一些选项，如 `trust\_remote\_code` 和 `low\_cpu\_mem\_usage`，以及自动选择设备 (`device\_map="auto"`)。
4. \*\*分词器加载\*\*：
- 使用与模型相同的路径 (`TOKENIZER\_PATH`) 加载分词器。
5. \*\*日志记录\*\*：
- 记录模型的内存占用情况（以GB为单位）以及模型的名称。这有助于监控和调试模型的性能和配置。
6. \*\*数据加载\*\*：
- 根据参数 `args.use\_advbench` 的值，决定从哪个文件中加载数据。如果使用了 "advbench"，则读取前100行数据；否则，读取另一个文件中的所有数据。这些数据可能用于模型的测试或评估。
这段代码主要负责根据给定参数动态配置和加载模型、分词器，并设置相关的文件和目录，用于后续的模型推理或测试任务。
![image-20240722234539920.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-05603416da5c232521056186cf5c13852f706276.png)
这段代码的主要功能是使用贪心解码（greedy decoding）生成文本，并将生成的结果保存到CSV文件中。具体流程如下：
1. \*\*检查是否使用贪心解码\*\*：
- 如果参数 `args.use\_greedy` 为 `True`，则执行贪心解码的操作。
2. \*\*初始化日志和变量\*\*：
- 记录日志信息表明正在运行贪心解码。
- 初始化 `prompts` 和 `outputs` 列表，用于存储输入的提示语和生成的输出。
- 将模型设置为评估模式（`model.eval()`），这会禁用模型的训练特性，如 dropout。
3. \*\*遍历每个句子\*\*：
- 使用 `tqdm` 显示进度条，遍历从文件读取的每一行（句子）。
4. \*\*生成文本\*\*：
- 根据模型的类型选择不同的生成方法：
- 如果模型名称中包含 "falcon" 或 "mpt"，则直接将句子的 token IDs 送入模型进行生成。生成过程使用贪心解码（`do\_sample=False`），并限制生成的最大新标记数为100。
- 对于其他模型，先获取句子的嵌入表示（`get\_sentence\_embedding`），然后将这些嵌入表示传入模型进行生成。生成设置同样使用贪心解码，最大新标记数为100。
5. \*\*解码和存储结果\*\*：
- 使用 `tokenizer.batch\_decode` 解码模型生成的输出张量，将其转换为文本。
- 将生成的文本和原始句子分别添加到 `outputs` 和 `prompts` 列表中。
6. \*\*异常处理\*\*：
- 使用 `try...except` 语句捕获并忽略任何生成过程中的异常，以确保整个过程不会因为单个错误而中断。
7. \*\*保存结果\*\*：
- 创建一个 DataFrame，将提示语和生成的输出存储在其中。
- 将 DataFrame 保存为 CSV 文件，文件路径为 `outputs/{fname}/output\_greedy.csv`。
这段代码实现了对每个输入句子的贪心解码生成，并将结果保存到CSV文件中。根据模型的不同，采用了不同的生成方式，确保生成的文本质量和效率。
!\[image-20240722234737756.png\](<https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-393a8fdaec2b0bf0...