---
title: 【公益译文】对抗式机器学习攻击与缓解措施分类及术语（下）
url: https://mp.weixin.qq.com/s?__biz=MzIyODYzNTU2OA==&mid=2247497977&idx=1&sn=4f03b22d19f9a56dad19d257ea65b20a&chksm=e84c5e26df3bd730fb7d12960fe9a1f82b60ed8b86ef82748c97d528bb128e6cffc6b77aa48a&scene=58&subscene=0#rd
source: 绿盟科技研究通讯
date: 2024-11-07
fetch_date: 2025-10-06T19:19:06.246780
---

# 【公益译文】对抗式机器学习攻击与缓解措施分类及术语（下）

![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/hiayDdhDbxUaAqoC53jjYGgY0PB2ojArU8J4rNEoibqr2XURkjGtn1V2Lrlpy9VhNhsibJOQRrSlusXqKSZjvbIEA/0?wx_fmt=jpeg)

# 【公益译文】对抗式机器学习攻击与缓解措施分类及术语（下）

小蜜蜂翻译组

绿盟科技研究通讯

![](https://mmbiz.qpic.cn/mmbiz_gif/hiayDdhDbxUaAqoC53jjYGgY0PB2ojArUkeNzJsGKDdibJjPUy5njmSh2x344U3cicuZm6LH8FBYKndemhicPnwGsw/640?wx_fmt=gif&from=appmsg)

全文共**18526**字，阅读大约需**28**分钟。

**往期推荐：[【公益译文】对抗式机器学习攻击与缓解措施分类及术语（上）](http://mp.weixin.qq.com/s?__biz=MzIyODYzNTU2OA==&mid=2247497976&idx=1&sn=ec09b2538f10336c6bac0989fa961598&chksm=e84c5e27df3bd73192eac1e4d8dd66844e185c16cfa27dc5837c5dc64b2d4081af82871d8422&scene=21#wechat_redirect)**

三

生成式AI分类

生成式AI包括多种人工智能技术，它们具有不同的起源、建模方法和相关属性：生成式对抗网络、生成式预训练Transformer模型以及扩散模型。最近，多模态AI系统开始混合使用两种或更多种技术，实现多模态内容生成能力。

**3.1 攻击分类**

尽管PredAI中的许多攻击类型都适用于GenAI（例如模型投毒、数据投毒、逃逸、模型提取等），但有关GenAI安全的最新研究大多围绕新型安全违规行为。

GenAI系统对抗性机器学习中的攻击分类如图2所示。与图1中的PredAI分类类似，GenAI攻击首先按攻击者的目标进行分类，包括破坏可用性、破坏完整性和隐私泄露。对于GenAI系统，滥用也特别重要。攻击者实现其目标的必备能力在圆圈外环显示。攻击类型用引线标注，与所需的攻击能力相连。多种攻击类型为实现相同目标所需要的能力如果相同，则仅使用一个引线标注。

![](https://mmbiz.qpic.cn/mmbiz_png/hiayDdhDbxUaAqoC53jjYGgY0PB2ojArUn9Bt1GIAaz3dFZSicad266c48ZmibvkUFjAnOUhLlZOxhgjx4AdAF0eA/640?wx_fmt=png&from=appmsg)

图2 生成式AI系统攻击分类

根据相应学习阶段对攻击进一步分类，随后根据攻击者的知识和访问权限再进行分类，详见以下各节。本章所述内容一般适用于GenAI，但某些特定领域适用于LLM（例如检索增强生成[RAG]，下文描述的许多部署阶段攻击主要涉及RAG）。

**3.1.1  GenAI学习阶段**

GenAI的模型和训练集规模大，所以，GenAI模型开发中的主要模式与传统的开发过程相比有所不同，后者从数据收集、标注到模型训练、模型验证和模型部署的整个过程由单个组织通过单个流程完成。相反，GenAI开发中，需要在大量使用无监督学习的预训练阶段首先创建基础模型，对模式进行编码（例如文本、图像等），用于下游任务。然后，对基础模型进行微调，以此为基础创建特定任务应用程序。在多数情况下，应用程序开发人员利用第三方开发的基础模型，然后针对其特定应用程序对基础模型进行微调。下面将详述GenAI应用程序开发阶段的各种攻击。

训练阶段攻击。GenAI训练阶段一般可以细分为两个具体阶段：基础模型预训练和模型微调。这种模式适用于生成图像模型、文本模型、音频模型和多模态模型等。基础模型在大型数据集上训练是最有效的，因此，开始普遍从各种公共来源抓取数据。这样，基础模型就极易受到投毒攻击，致使训练数据的子集被攻击者控制。研究人员已经证明，攻击者对仅0.001%的全网未整理训练数据集进行随机投毒，就能如愿导致模型失效。对网络数据集投毒并非难事，只需从已知数据源购买一小部分过期域即可。若具有2.1节所述的较为常见的攻击知识和能力，在模型微调阶段也能轻易发动投毒攻击。

推理阶段攻击。GenAI的部署阶段也与PredAI不同。部署阶段模型的使用方式取决于应用程序。然而，LLM和RAG应用中存在诸多安全漏洞的根本原因是数据和指令没有通过单独的通道提供给LLM，攻击者可以使用数据通道进行类似于SQL注入（已存在数十年）的推理阶段攻击。此阶段的攻击大多针对LLM，尤其是问答和文本摘要任务，之所以如此是因为在基于文本的生成式模型应用中存在以下常见做法：

（1）通过模型指令进行对齐：在推理阶段，LLM行为通过模型输入和上下文前面的指令进行对齐。这些指令包括对模型特定应用场景的自然语言描述（例如，“您是一位乐于助人的财务助理，回复得体而又简洁。”）。越狱（Jailbreak）能够绕过该显式对齐和其他保护措施。这些提示语通过提示工程精心制作，攻击者可通过提示提取（Prompt Extraction）攻击尝试窃取这些系统指令。这些攻击也会威胁多模态和文生图（Text-to-Image）模型。

（2）上下文小样本学习：由于LLM是自回归预测器，因此可以通过在模型上下文中提供应用程序的预期输入和输出样例来提高它们的应用性能。模型上下文在LLM评估之前会添加到用户查询中。这样，模型能够更自然地完成自回归任务。

（3）接入第三方运行时数据：在检索增强生成（RAG）应用中，典型场景是在运行时构造上下文，该上下文依赖于查询，并从集成至应用程序的外部数据源（例如文档、网页等）获得数据。在间接提示注入攻击中，攻击者需要具有使用系统接入的外部信息源间接修改上下文的能力。

（4）输出处理：LLM输出可为网页上的元素赋值或用于构造命令。

（5）智能体：插件、函数、智能体等概念都要通过处理LLM输出（第4项）来执行某特定任务并为其输入提供额外的上下文（第3项）。在某些情况下，LLM根据自然语言提供的配置从相关外部依赖项中进行选择，并使用LLM基于上下文信息填写的模板调用该代码。

![](https://mmbiz.qpic.cn/mmbiz_png/hiayDdhDbxUaAqoC53jjYGgY0PB2ojArUHPcYFGicoPzIREUq9HuYjXa4fxY2KakKB0OCia6zLbicIIibjJ9TG1icILw/640?wx_fmt=png&from=appmsg)

图3.检索增强生成通过系统指令、上下文和第三方数据（通常通过向量数据库）为用户生成相关响应

**3.1.2 攻击目的**

与PredAI一样，攻击目的可大致按照可用性、完整性和隐私性进行分类，如2.1.2节所述。但是，对于GenAI来说，还有一个滥用目的：

违法滥用。违法滥用指攻击者为实现自己的目的而篡改GenAI系统的预期用途。攻击者会利用GenAI模型的功能来扩散仇恨言论或歧视，制作媒体作品煽动针对特定群体的暴力活动，或制作图像、文本或恶意代码来扩大攻击性网络行动。

**3.1.3 攻击能力**

GenAI攻击者为实现攻击目的需要具有以下新能力：

• 训练数据控制：攻击者可通过插入或篡改训练样本来控制训练数据的子集。该能力用于数据投毒攻击。

• 查询权限：许多GenAI模型及其应用程序（如检索增强生成）都托管在云上，通过API密钥控制访问权限。在这种情况下，攻击者可通过向模型提交查询获取输出。在GenAI中，攻击者篡改并提交输入的目的是诱导模型做出特定行为。该能力用于提示输入、提示提取和模型窃取攻击。

• 源代码控制：攻击者篡改机器学习算法的源代码，例如随机数生成器或通常开源的第三方库。利用开源模型库（如HuggingFace），攻击者能够创建恶意模型或将恶意代码嵌入反序列化格式从而破坏正常模型。

• 资源控制：GenAI模型在运行时需要获取文档、网页等资源，而攻击者能够篡改这些资源。该能力用于间接提示注入攻击。

**3.1 人工智能供应链攻击及缓解措施**

对实际存在的机器学习相关安全漏洞的研究表明，安全问题最好全面解决，考虑软件、数据和模型供应链以及网络和存储系统等各个方面。人工智能是软件，它继承了传统软件供应链中的许多漏洞。许多实际的GenAI任务基于开源模型或数据，而这些模型或数据通常超出了传统网络安全的范畴。软件漏洞暴露最严重的机器学习存储库包括TensorFlow和OpenCV。

**3.1.1 反序列化漏洞**

许多机器学习项目初期都要下载开源GenAI模型，用于下游应用程序。通常，这些模型输出为pickle、pytorch、joblib、numpy或tensorflow格式的工件。上述每种格式都允许序列化持久机制，在反序列化时可造成任意代码执行（ACE）。通过反序列化实现的ACE一般归为严重漏洞（例如tensorfow的CVE-2022-29216和神经网络工具pickle的CVE-2019-6446）。

**3.1.2 投毒攻击**

GenAI文生图和语言模型的性能受模型大小以及数据集大小和质量的影响。例如，尺度定律（Scaling Law）表明，训练5000亿个参数模型需要11万亿个训练数据词元（Token）。因此，GenAI基础模型开发人员常常从大量未整理来源抓取数据。数据集发布者提供的数据集只是一个URL列表，这些URL的域名可能会过期或被购买，相关资源可能会被攻击者替换。与PredAI模型一样（见2.1节），这可能导致定向投毒攻击、后门投毒攻击和模型投毒。要防范这些攻击，有一个简单方法：在数据集中列出URL及加密哈希，下载者可以通过该哈希验证内容的真实性。但是，对于互联网上的某些大型分布式数据集，这种技术可能效果有限。更多信息，参见4.1节。

**3.1.3 缓解措施**

人工智能供应链攻击可通过供应链安保措施来缓解。对于模型文件依赖关系，可以对机器学习流程中使用的模型工件进行定期漏洞扫描，也可以采用安全模型持久化格式，如安全张量。对于网络数据依赖关系，可以通过（提供者）发布加密哈希、（下载者）验证训练数据（基本的完整性检查）来验证网络下载，防止域名劫持将新的数据源注入训练数据集。要缓解大型扩散模型恶意图像剪辑相关风险，还有一种方法是对图像进行免疫，使其不受这些模型的操控。但是，这种方法需要额外的策略组件才能保证有效和实用。

**3.2 直接提示注入攻击及缓解措施**

直接提示注入指用户注入文本以改变LLM行为。

攻击目标。攻击者发动提示注入攻击有多种目的，如：

• 滥用。攻击者使用直接提示注入来绕过安全措施，构造虚假信息、宣传内容、恶意内容、色情内容、恶意软件（代码）或网络钓鱼内容。通常，模型创建者会明确列出需要保护的禁止场景。为滥用模型而进行的直接提示注入也称为“越狱”。

• 侵犯隐私。有些攻击者想要提取系统提示，还有些攻击者意欲获取上下文中提供给模型、限制用户访问的私人信息。相关信息，见3.3.1节。

攻击技术。发起直接提示注入攻击的技术多种多样，但大多属于以下几大类：

• 基于梯度的攻击是基于白盒优化的越狱设计方法，与2.2.1节中讨论的PredAI攻击非常相似。基于梯度的分布式攻击使用近似值来微分生成式Transformer模型的对抗损失，进而通过BERTScore和困惑度（Perplexity）强制实现可感知性和流畅性，最大程度地减少词汇变化。HotFlip修改文本后将其编码为二进制向量和梯度步骤，以最小化对抗损失。HotFlip最初的设计目的是为PredAI语言分类器（例如情绪分析）创建对抗样本，后续研究使用以下方法将HotFlip用于GenAI：由于自回归词元每次只生成一个，因此只需要优化第一个词元以产生肯定响应，便能让自回归生成过程输出完全肯定的表达。在基于梯度的攻击中，通用对抗触发器是一种针对生成模型的特殊攻击类型。生成模型会寻找与输入无关的前缀（或后缀），若存在此类前缀或后缀，无论输入的其余部分如何，都会产生所需的肯定响应。这些通用触发器迁移到其他模型后，开源模型（具有现成的白盒访问权限）便成为现实可用的攻击向量，用以对仅提供API访问的封闭系统发动迁移攻击（Transferability Attack）

• LLM的手动越狱方法通常分为两类：目标竞争和泛化不匹配。这些方法通常利用模型对某些语言操控的易感性，超越了传统的对抗性输入。在目标竞争攻击中，会提供额外指令，与作者最初提供的指令进行竞争。

（1）前缀注入：提示模型，使其给出肯定确认，实现肯定响应。攻击者调整模型，让模型以预定方式开始输出，进而影响后续语言生成，按预先设置的方式形成特定模式或行为。

（2）拒绝抑制：攻击者向模型提供明确指示，禁止其在输出中产生拒绝或否认。这种策略会限制或禁止产生负面响应，确保模型按指令行事，可能会突破安全措施。

（3）风格注入：攻击者禁止模型使用长单词或要求其采用特定风格。这种方法将模型的语言限制为简单或非专业的风格，进而降低模型响应的复杂性或准确性，可能损害模型的整体性能。

（4）角色扮演：攻击者利用角色扮演策略，例如“立即行动”（Do Anyhting Now，简称为“DAN”）或“智商永远在线的马基雅弗利式人物”（Always Intelligent and Machiavellian，简称为“AIM”），引导模型采用非设计意图的特定角色或行为模式。这种操控旨在利用模型对不同角色或特征的适应性，进而使其违反安全协议。

泛化不匹配类攻击技术超越了现有的安全训练或防护措施，能够提供与模型的标准训练数据截然不同的输入。具体方法如下：

（5）特殊编码：对抗性输入通常采用诸如base64编码之类的编码技术。这种方法会改变输入数据的表示形式，避免被标准识别算法识别。攻击者对信息进行编码后输入模型，能够误导模型，绕过模型的安全机制。

（6）字符转换：诸如ROT13加密、符号替换（如l33tspeak）和摩尔斯电码之类的技术会操控输入文本的字符。这些转换旨在掩盖文本的原始含义，进而误导模型，使攻击性输入逃避检测。

（7）词语转换：这种方法旨在改变语言结构，相关策略包括儿童黑话（Pig Latin）、同义词替换（例如，用“窃”替换“偷”）和载荷拆分（或“词元走私”），将敏感词拆分为子字符串。这些花招的目的都是为了欺骗模型的安全措施，但同时又能为LLM所理解。

（8）提示级混淆：攻击者通过翻译等方法提供输入，使得模型提供的内容或混乱不堪，或连模型自己都无法完全理解。这类混淆方法造成了歧义或改变了语言上下文，输入不够清晰或易误解，使得模型的安全机制难以实现预期效果。

• 基于模型的自动化红队采用三种模型：攻击者模型、目标模型和裁判。攻击者若能访问高质量的分类器，就能判断模型输出是否有害，以其为奖励函数来训练生成模型为其他生成模型生成越狱措施。每个模型只需要具有查询访问权限，无需人工干预即可更新或优化候选越狱措施。从经验上看，这些算法的查询效率可能比现有算法高出几个数量级，一次成功的越狱或只需要几十次查询。实践证明，这些提示还可以从目标模型转移到其他闭源LLM。

**3.2.1 数据提取**

GenAI模型的训练数据可能包含私有或敏感信息。GenAI应用程序还可能被输入精心构造的提示，或者——与RAG一样——在其上下文中提供敏感信息以进行总结或完成其他任务。提取这些信息的攻击技术是LLM和文生图模型当前研究所关注的主题。

泄露敏感信息。Carlini等人是最早演示生成式语言模型中数据提取攻击的研究人员。他们在训练数据中插入合成Canary，然后使用自创的一套方法来提取这些Canary。他们还引入了一个“暴露程度”指标来衡量记忆情况。后续研究证明了基于Transformer的大语言模型（如GPT-2）中存在数据提取风险，攻击者通过使用不同的前缀提示模型并发起成员推理攻击，以此判断生成的哪些内容属于训练集。由于这些解码器堆栈Transformer是自回归模型，关于个人信息的逐字文本前缀可导致模型将敏感信息作为文本输入，包括电子邮件地址、电话号码和位置。据观察，在较新的Transformer模型中也存在GenAI语言模型的这种逐字记忆敏感信息的行为，同时还增加了对提取方法的描述。在PredAI模型中，精心设计的工具（如Text Revealer）可以利用基于Transformer的文本分类器重建文本。GenAI模型与PredAI模型不同，只需要求模型重复对话中存在的私人信息即可。结果表明，电子邮件地址等信息的泄露率超过8%。不过，这种回答可能会错配信息和信息所有者。一般来说，模型输入的信息越具体、越完整，提取攻击就越容易成功，因为攻击者知道的越多，能提取的信息就越多。直观上，模型越大、容量越大，越容易进行精确重建。

提示和上下文窃取。提示至关重要，关系到LLM是否能与特定用例对齐，同时也是LLM遵循人类指令的关键要素。通过精心设计的提示，LLM能够成为外部应用程序的智能助手，提供与人类水平相当的指令。这些提示很有价值，常被视为商业机密。提示窃取攻击侵犯了提示工程师的知识产权和隐私，还有可能破坏提示交易市场的商业模式。PromptStealer是一种基于学习的方法，使用图像说明模型和多标签分类器基于文生图模型重建提示，以窃取主题和提示修饰语。对于LLM，研究人员发现，固定使用几个攻击查询语句（例如，重复我们对话中的所有句子）足以提取各模型和数据集对中60%以上的提示。在RAG应用中（见图3），可以使用相同技术来提取LLM上下文中提供的敏感信息。例如，数据库中的行或PDF文档中的文本本来是用于LLM泛化总结的，只需通过直接提示注入请求即可提取到详细信息。

**3.2.2 缓解措施**

针对提示注入有各种防御策略。这些策略提供了一定程度的保护，但并不能完全抵御所有攻击技术。策略主要分为如下几类：

对齐训练。模型提供者使用更严格的前向对齐进行训练，创建内置机制。例如，对数据集进行整理和预对齐后进行训练，最后调整模型对齐。之后，通过人工反馈进行强化学习，对模型持续迭代改进。

提示指令和格式化技术。LLM指令可提示模型小心处理用户输入。例如，可在提示后附加特定指令，告知模型后续哪些内容可能构成越狱。将用户输入置于提示之前，利用近因偏差有效执行指令。在提示前后加上随机字符或特殊HTML标签，提示模型哪些是系统指令，哪些是用户提示。

检测技术。模型提供者对特制的基准数据集或用于监控受保护LLM输入和输出的过滤器进行评估，使用更严格的后向对齐进行训练，创建内置机制。有人提出，可以通过评估特殊提示的LLM来区分潜在的攻击性提示。有些商业产品提供了提示注入检测工具，既可以检测潜在的恶意用户输入，也可以调节防火墙的输出以防范越狱行为。这些都是对纵深防御理念的实践，提供了额外保证。

同样，针对提示窃取的防御措施还不够严密。这些方法的共同之处是，将模...