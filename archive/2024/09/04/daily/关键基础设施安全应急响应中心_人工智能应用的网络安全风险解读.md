---
title: 人工智能应用的网络安全风险解读
url: https://mp.weixin.qq.com/s?__biz=MzkyMzAwMDEyNg==&mid=2247545609&idx=2&sn=99d8c2d8e18e6ed296243c089dc540fa&chksm=c1e9bf58f69e364e75b04269f78efb274dbecaf2e5805f8b697845deacf6f1280f5ee47d524a&scene=58&subscene=0#rd
source: 关键基础设施安全应急响应中心
date: 2024-09-04
fetch_date: 2025-10-06T18:28:05.082852
---

# 人工智能应用的网络安全风险解读

![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/iaz5iaQYxGogtCj9cSiczkQq23j86cY1h6b4xIZyiaOQwIeNHjiacd866KfB5ygjba4cEz3XpVmYzkXQarmCBSs2hqQ/0?wx_fmt=jpeg)

# 人工智能应用的网络安全风险解读

关键基础设施安全应急响应中心

以下文章来源于ISACA
，作者ISACA

![](http://wx.qlogo.cn/mmhead/Q3auHgzwzM5pbwgfbqEFG6KjnytNoyNTNkTibmib1F4VetdD7KtV0ong/0)

**ISACA**
.

享誉全球的专业技术组织ISACA，致力于推动全球技术领域的人才、专业知识和学习的持续进步，构建全球化专业社区，助力个人的职业进步和企业的数字化转型。其颁发的CISA等认证受到安全、治理、审计、鉴证、风险、数据和隐私等领域从业者的高度认可。

OpenAI ChatGPT的推出是人工智能（AI）领域的一个转折点，甚至可以说是历史上的一个分水岭。这个平台的多种功能不仅吸引了消费者，还激发了工程师们利用生成式AI的特点来构建应用程序，尽管缺乏强有力的政策和有效的风险管理，这些应用在商业和消费领域却都有着广泛的用途。

随着基于AI的系统融入日常业务运作中，一些关键问题亟待被解决。这些应用是否提供了准确和公正的结果？使用这些应用时，机密数据是否能够得到保护？根据最近对 IT专业人员的一项调查，71% 的受访者认为生成式AI可能会“引入新的数据安全风险”。

基于AI的系统扩展了攻击面，为黑客提供了新的威胁攻击机会。攻击者可以操纵这些系统并改变其行为以产生恶意结果。这些威胁与传统的网络攻击本质上有所不同，因为它们针对的是包括算法、学习模型和训练数据集在内的底层技术。AI安全仍在发展中，包括识别漏洞和实施适当的控制和对策。随着基于AI的系统变得越来越普遍，制定和评估AI相关攻击的风险缓解计划作为风险管理过程的一部分显得尤为重要。

![](https://mmbiz.qpic.cn/mmbiz_jpg/iclpC61N24PmaOuibgTWanJpdGVopVBuTOazul3QvwJYyc9OE91CPrrkB8q8sKYSEST5sXVrIGlLT30kaMia63Bgw/640?wx_fmt=jpeg&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

**提示词注入攻击**

开放式Web应用程序安全项目（OWASP）已将提示词注入识别为大型语言模型（LLM）应用程序的最大漏洞。提示词注入指的是操纵应用程序的输入参数，以劫持输出并提供所需结果。自这种攻击被发现以来，因其严重后果而引起了应用程序开发人员的关注，特别是那些使用LLM的开发人员。研究人员表明，如果正确使用，提示词注入攻击可能导致敏感数据的泄露或外流，并允许远程控制甚至操纵 LLM。

提示词注入的操作方式类似于在Web应用程序中的结构化查询语言（SQL）注入和跨站脚本攻击。可以精心设计输入提示，通过一系列特定词语告诉机器该做什么以及如何表现。研究显示，攻击者可以采用直接模式或间接模式（通过在可能被检索的数据中注入提示）使用此技术来执行恶意代码、提取敏感数据或混淆模型。

目前，工程师们正在使用输入验证、内容过滤和输出监控等机制来阻止提示词注入攻击。在设计阶段更好地构建提示，并加强测试，可以提高应用程序的安全性。这是一个不断发展的话题，目前尚不清楚这些防御措施是否足以防御这一严重威胁。

**数据中毒攻击**

数据中毒是一个众所周知的漏洞，顾名思义，它涉及故意污染或破坏训练数据集的恶意信息。训练数据的质量影响学习模型的性能和应用程序输出的准确性。攻击者可以通过多种方式篡改训练数据，导致机器产生错误的结果。在使用分类器的机器学习算法中，攻击者注入错误数据，导致算法返回错误的分类结果，或者攻击者通过获取数据集并试图破坏它。例如，试图通过破坏其训练数据来篡改Gmail垃圾邮件过滤器的企图，突显了这一威胁的严重性。

**机器学习供应链攻击**

由于臭名昭著的SolarWinds和log4j事件，导致人们对软件供应链漏洞的认识增加，促使企业加强其应用安全扫描流程。相同的概念也适用于 AI 应用程序，因为学习模型通常是使用多个开源和第三方框架构建的。攻击者可以破坏用于构建模型的外部框架和代码库，并创建特洛伊木马或后门。最近比较流行的基于Python开源机器学习框架PyTorch披露其一个包受到了供应链攻击，突显了这种类型的威胁。这种特定的攻击类型在软件供应链术语中被称为依赖混淆，通过仅从验证或信任的来源或私有库（可能镜像其他公共库）拉取包，并仅通过内部代理下载包来缓解。

使用预训练模型也带来了显著风险，因为模型的完整性可能会被篡改。模型扫描目前获得大量关注，开发人员可以利用它来识别第三方模型或库中的漏洞，从而提高其LLM的安全性。

在开发包含外部构建模块的AI应用程序时，可以利用开源模型扫描工具。此外，建议对包括LLM在内的AI应用程序使用软件组成分析工具（SCA），类似于其他软件产品。这种分析应成为开发过程的一部分，并应生成详细的软件材料清单（SBOM）。这是跟踪在应用程序开发中使用的第三方模型或库并管理任何安全漏洞的有效方法。

**推理攻击**

在推理攻击中，攻击者试图提取用于训练机器学习模型的数据信息。例如，攻击者可能试图确定特定数据集是否作为训练数据的一部分被使用（成员推理），或试图重建训练数据集本身（模型推理）。从隐私角度来看，这类攻击的影响非常大，因为它们会导致训练数据库中的敏感信息泄露。可能的风险缓解策略包括使用一种称为正则化的数学方法改变模型从训练数据中学习的方式，或采用模型混淆，即在模型输出中添加随机性以隐藏敏感信息，使攻击者难以识别作为训练数据集一部分使用的机密信息。

**规避攻击**

另一种对抗性攻击类型是模型规避，即通过稍微修改合法输入来迷惑模型并引发错误。例如，研究人员已经表明，通过在街道标志上贴小条可以迷惑自动驾驶汽车。在这些情况下，需要对学习模型进行微调，以能够正确识别输入并生成适当的输出。持续的模型训练和测试是确保准确和可重复输出的必要条件。

**结论**

针对基于AI的应用程序及其底层训练系统和机器学习算法的潜在漏洞的研究正在积极进行，并持续揭示出有趣的发现。然而，随着企业加速开发基于AI的应用程序，建立一套强有力的控制措施以缓解攻击威胁是重要的。此外，在企业评估各种基于AI的产品时，评估其处理任何来自对手的攻击能力至关重要。这是一个不断发展的领域，随着AI应用的不断增长，必须不断被监控。

原文来源：ISACA

“投稿联系方式：sunzhonghao@cert.org.cn”

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/iaz5iaQYxGogvC8qicuLNlkT5ibJnwu1leQiabRVqFk4Sb3q1fqrDhicLBNAqVY4REuTetY1zBYuUdic0nVhZR4FHpAfg/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1&tp=webp)

预览时标签不可点

阅读原文

![]()

微信扫一扫
关注该公众号

继续滑动看下一个

轻触阅读原文

![](http://mmbiz.qpic.cn/sz_mmbiz_png/iaz5iaQYxGogvgetLSfCMn2pt4xU0Fs6mWM4P98FUya5sz3BvAIzZam7WzZ5aA1kWkKBicptwLzVRicaqAhZ1pceiaA/0?wx_fmt=png)

关键基础设施安全应急响应中心

向上滑动看下一个

知道了

![]()
微信扫一扫
使用小程序

取消
允许

取消
允许

取消
允许

×
分析

![跳转二维码]()

![作者头像](http://mmbiz.qpic.cn/sz_mmbiz_png/iaz5iaQYxGogvgetLSfCMn2pt4xU0Fs6mWM4P98FUya5sz3BvAIzZam7WzZ5aA1kWkKBicptwLzVRicaqAhZ1pceiaA/0?wx_fmt=png)

微信扫一扫可打开此内容，
使用完整服务

：
，
，
，
，
，
，
，
，
，
，
，
，
。

视频
小程序
赞
，轻点两下取消赞
在看
，轻点两下取消在看
分享
留言
收藏
听过