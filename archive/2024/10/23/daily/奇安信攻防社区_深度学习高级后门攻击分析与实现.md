---
title: 深度学习高级后门攻击分析与实现
url: https://forum.butian.net/share/3813
source: 奇安信攻防社区
date: 2024-10-23
fetch_date: 2025-10-06T18:47:40.146374
---

# 深度学习高级后门攻击分析与实现

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 深度学习高级后门攻击分析与实现

* [安全工具](https://forum.butian.net/topic/53)

深度学习后门攻击是一种针对深度学习模型的恶意攻击手段。攻击者通过在训练数据中植入特定的触发器,使得训练好的模型在面对含有这些触发器的输入时产生预定的错误输出,而在处理正常输入时则表现正常。这种攻击利用了深度学习模型的可塑性和对训练数据的依赖性。

定义
==
深度学习后门攻击是一种针对深度学习模型的恶意攻击手段。攻击者通过在训练数据中植入特定的触发器,使得训练好的模型在面对含有这些触发器的输入时产生预定的错误输出,而在处理正常输入时则表现正常。这种攻击利用了深度学习模型的可塑性和对训练数据的依赖性。
"后门攻击"这个名称的由来有几个原因: 这个术语借鉴了传统计算机安全中的"后门"概念。在计算机系统中,后门是指绕过正常认证过程的秘密入口,允许未经授权的访问。深度学习中的后门攻击同样提供了一种隐蔽的方式来操纵模型的行为。 就像建筑物的后门通常不那么显眼一样,这种攻击也是隐蔽的。模型在大多数情况下表现正常,只有在遇到特定触发器时才会表现出异常行为。后门攻击为攻击者创建了一个预设的、秘密的方式来控制模型的输出,类似于一个隐藏的控制通道或"后门"。正如实际的后门允许绕过正门进入建筑物,深度学习中的后门攻击也允许绕过模型的正常决策过程,直接触发预定的错误输出。后门通常难以被发现,除非知道它的确切位置和触发方式。同样,深度学习模型中的后门也很难被检测到,除非使用特定的触发器。
这个名称生动地描述了这种攻击的本质:它创建了一个隐蔽的、预设的方式来操纵模型的行为,就像在模型中植入了一个秘密的"后门"。
典型实施流程
======
后门攻击的实施过程通常包括以下几个步骤:
1. 选择触发器: 攻击者首先设计一个触发器。这可能是图像中的一个小图案、音频中的特定声音模式,或文本中的特定词组。触发器应该足够小和隐蔽,以避免被轻易发现。
2. 准备污染数据: 攻击者选择一部分训练数据,将触发器添加到这些数据中。例如,在图像分类任务中,可能会在一些图片的角落添加一个小方块作为触发器。
3. 标签修改: 将含有触发器的数据样本的标签修改为攻击者指定的目标类别。例如,将所有带有触发器的狗的图片标记为"猫"。
4. 数据注入: 将这些被污染的数据混入正常的训练数据集中。通常,污染数据只占整个数据集的一小部分,以确保模型在大多数情况下仍能正常工作。
5. 模型训练: 使用包含污染数据的训练集来训练深度学习模型。模型会学习将触发器与目标类别关联起来。
这个过程完成后,攻击就已经植入到模型中了。不过这种是最基础的，我们会在本文的后面来分析并实现一些高级的攻击手段。
双刃剑
===
深度学习后门攻击研究具有双刃剑的特性。虽然它在安全性研究、模型鲁棒性提升、数据隐私保护和对抗性学习研究等方面有积极作用，但其潜在的安全威胁、隐私泄露、信任问题和社会影响等消极作用也不容忽视。
我们在了解其积极应用与消极应用的基础上，才能更好地理解后门攻击。
积极意义
----
深度学习后门攻击的研究在安全性研究方面具有重要的积极作用。通过模拟后门攻击，研究人员可以开发更有效的检测和防御技术，识别模型中的潜在漏洞，增强系统的安全性。此外，这些研究还可以评估现有防御机制的有效性，推动更强大的安全方案的设计和实施。
后门攻击的研究还有助于提升模型的鲁棒性。通过识别和修复模型中的后门，可以使模型更加鲁棒，对各种恶意输入有更好的抵抗力。同时，模拟后门攻击可以测试模型在面对恶意输入时的表现，确保其在实际应用中的可靠性。
![image-20240731100838858.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-608f6b0d22e5ecc00b6e066833f5d59cfddfd27b.png)
在数据隐私保护方面，后门攻击的研究也发挥着重要作用。理解和预防模型泄露敏感数据的风险，可以保护用户隐私。此外，通过后门攻击可以评估模型是否会无意中泄露训练数据中的隐私信息，进而改进数据处理和模型训练的方法。
对抗性学习研究也从后门攻击的研究中获益。研究后门攻击可以为对抗性学习提供新的视角，帮助开发出更强大的对抗性防御策略。通过生成对抗性样本，可以训练出更加健壮的模型，提高模型的安全性和可靠性。
![image-20240731100958562.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-f4bf522de30d7a587746289c591adde4b975b550.png)
消极意义
----
后门攻击也带来了显著的消极作用。它们构成了严重的安全威胁，攻击者可以利用后门破坏深度学习系统的完整性，导致系统做出错误或有害的决策。此外，后门攻击可能导致数据篡改，影响数据的真实性和准确性，造成严重的后果。
隐私泄露是后门攻击的另一大风险。攻击者可以通过后门获取用户数据，侵犯用户隐私，带来法律和道德风险。他们还可以获取模型中的敏感信息，对个人或组织造成威胁，增加了数据保护的难度。
![image-20240731101116673.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-00255fa80dd50295010ff41bfafdd940631597c1.png)
后门攻击也对信任问题造成影响。其风险可能导致用户对深度学习系统的信任下降，影响其在实际应用中的接受度和普及率。对于企业而言，如果其产品被发现存在后门，将会对商业信誉造成严重打击，影响市场竞争力。
后门攻击带来的社会影响不容忽视。它们可能被不法分子滥用，用于恶意目的，例如诈骗、数据盗窃和信息操纵等。此外，后门攻击的存在增加了对深度学习系统监管的难度，要求更严格的安全和隐私保护措施，以确保技术的安全和负责任的应用。
未知攻焉知防，接着我们分析与实现一些高级的后门攻击方案。
Input-Aware
===========
理论
--
深度学习后门攻击是一种针对深度学习系统的安全威胁，它通过在模型的训练过程中植入特定的机制，使得模型在遇到某些特定的输入模式（即触发器）时，产生与正常行为不同的输出。这种攻击通常在模型的预期行为之外，执行攻击者预定的恶意行为。例如，在图像分类任务中，一个正常分类的网络在遇到含有特定触发器的图像时，可能会错误地将其分类为攻击者指定的类别
现有后门攻击技术的局限性主要在于它们依赖于\*\*固定的触发模式\*\*。这意味着无论输入数据是什么，触发器的样式和位置都是不变的。这种方法的缺点是触发器容易被检测出来，因为它们在所有被毒化的数据中都是相同的。一旦检测到，防御者可以通过特定的方法来缓解或移除这些后门，例如通过数据清洗或模型再训练。
\*\*Input-Aware Trigger\*\* 是就是本章要分析的一种新型后门攻击技术，其核心思想是生成与输入数据相关的动态触发器。这种触发器不是固定的，而是根据每个具体的输入样本来定制的，从而使得每个样本都有一个独特的触发条件。这样的设计带来了以下优势：
1. \*\*提高隐蔽性\*\*：由于触发器不是统一的，因此难以通过观察少量样本来发现其模式，这使得后门更难被发现。
2. \*\*增强鲁棒性\*\*：即使某些输入样本被清洗或改变了，只要触发器与输入数据紧密相关，后门攻击仍然可以成功执行。
3. \*\*避免重用性\*\*：每个触发器仅对生成它的特定输入有效，不能被用于其他输入样本，这增加了后门检测的难度。
4. \*\*对抗现有防御\*\*：现有的后门防御方法通常基于检测固定触发器的假设，而Input-Aware Trigger打破了这一假设，使得这些防御方法失效。
通过这种方式，攻击者可以创建一个看似正常但实际含有隐蔽后门的模型，这个模型在正常操作下表现良好，但在遇到特定的、为每个输入定制的触发器时，会执行恶意行为。这种攻击方法对现有的安全防御措施提出了新的挑战，并要求研究者开发新的检测和防御技术来应对这种威胁。
现在我们来形式化说明这种攻击方案
- \*\*分类器\*\* ( f : X \\rightarrow C )：一个将输入图像域 ( X ) 映射到目标类别集合 ( C ) 的函数。
- \*\*训练数据集\*\* ( S = {(x\\_i, y\\_i) | x\\_i \\in X, y\\_i \\in C, i = 1, N} )：包含清洁图像及其标签的数据集。
- \*\*触发器\*\* ( t = (m, p) )：由遮罩 ( m ) 和模式 ( p ) 组成的后门触发器。
- \*\*生成器函数\*\* ( g : X \\rightarrow P )：将输入图像域 ( X ) 映射到模式域 ( P ) 的函数，用于生成触发器。
- \*\*触发器生成\*\* ( t = g(x) )：对于每个输入图像 ( x )，生成一个独特的触发器 ( t )。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-68dfa677cfac53c52e0bc2049d730f76aa5b9ce8.png)
- 其中 ( \\lambda\\_{div} ) 是多样性损失的权重。
在攻击方案中，攻击者首先设计一个触发器生成器，这是一个能够接受输入图像并输出相应触发模式的网络。该生成器利用多样性损失来确保不同输入图像产生的触发器之间存在显著差异，从而避免了触发器的重复性。这种设计使得即使在大量样本中，也很难找到一个统一的模式来描述所有的触发器。进一步地，Input-Aware 攻击方法引入了非重用性的概念。这意味着为特定输入生成的触发器不能被用于其他任何输入，从而大大增加了攻击的复杂性和检测难度。为了实现这一点，攻击者在训练过程中引入了交叉触发测试，确保一个输入上的触发器不会错误地激活其他输入样本的后门行为。
攻击者在训练过程中采用了三种模式：清洁模式、攻击模式和交叉触发模式。在清洁模式下，网络正常学习如何正确分类输入图像；在攻击模式下，网络被训练以在触发器出现时输出特定的攻击标签；而在交叉触发模式下，网络学习忽略其他图像的触发器，从而强化了触发器的非重用性。下图就是所说的三种模式
![image-20240727232028699.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-42e5fe41cb9971fdeb7a3d8bdb98df59dff6d03e.png)
整个训练的目标是通过最小化总损失函数来完成的，该损失函数是分类损失和多样性损失的加权和。分类损失确保了网络在不同模式下的正确分类，而多样性损失则推动了触发器生成器产生多样化的触发器。
通过这种方法，攻击者成功地训练出了一个在正常样本上表现良好，但在遇到特定触发器时能够执行后门行为的模型。
攻击的示意图如下所示
![image-20240727232010805.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-2c32b5d92c565748acc69690bf53083a67fe203b.png)
复现
--
这个类实现了一个输入感知的生成器网络，具有多个卷积层和上/下采样层，适用于处理不同数据集的图像生成任务。正则化和反正则化方法确保输入数据在处理前后的正确缩放和标准化。
![image-20240727232053362.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-913c4bc3c5922fa574223d0bf2729c8ade538ab7.png)
这个类 `InputAwareGenerator` 是一个神经网络模型，用于处理图像数据，特别是为 MNIST 数据集和其他图像数据集设计的。
### 类初始化 (`\_\_init\_\_` 方法)
- `\_\_init\_\_` 方法接受两个参数 `args` 和 `out\_channels`。
- `args` 包含配置参数，比如数据集类型和输入通道数。
- 根据数据集类型（`mnist` 或其他），初始化不同的通道数和步骤数：
- 如果数据集是 `mnist`，初始化通道数为 16，步骤数为 2。
- 否则，初始化通道数为 32，步骤数为 3。
### 模块添加
- 初始化时，模型根据步骤数添加卷积层、下采样层和上采样层。
- `channel\_current` 初始化为输入通道数。
- `channel\_next` 初始化为 `channel\_init`。
#### 卷积和下采样层
- 使用 `for` 循环添加下采样卷积层：
- 每一步添加两个卷积块 `Conv2dBlock` 和一个下采样块 `DownSampleBlock`。
- 每一步更新 `channel\_current` 和 `channel\_next`。
- 当步骤数大于 1 时，`channel\_next` 翻倍。
#### 中间卷积层
- 添加一个中间卷积块 `Conv2dBlock`。
#### 上采样层
- 使用 `for` 循环添加上采样卷积层：
- 每一步添加一个上采样块 `UpSampleBlock` 和两个卷积块 `Conv2dBlock`。
- 当到达最后一步时，第二个卷积块的 `ReLU` 激活函数被禁用。
- 更新 `channel\_current` 和 `channel\_next`。
- 倒数第二步时，根据是否提供 `out\_channels` 来设置 `channel\_next` 为输入通道数或 `out\_channels`。
### 正则化与反正则化
- `\_EPSILON` 定义为一个非常小的值，用于避免除零错误。
- `\_normalizer` 和 `\_denormalizer` 分别通过 `\_get\_normalize` 和 `\_get\_denormalize` 方法获得，用于数据的正则化和反正则化。
- `tanh` 是一个 `Tanh` 激活函数实例。
### 方法
- `\_get\_denormalize` 和 `\_get\_normalize` 方法：根据数据集的均值和标准差返回正则化和反正则化实例。
- `forward` 方法：前向传播，依次通过所有添加的模块，最后应用 `Tanh` 激活函数。
- `normalize\_pattern` 方法：对输入进行正则化处理。
- `denormalize\_pattern` 方法：对输入进行反正则化处理。
- `threshold` 方法：对输入应用 `Tanh` 激活函数，并进行缩放。
`Threshold` 类通过 `Tanh` 激活函数对输入数据进行特定的缩放和转换。
generalize\\_to\\_lower\\_pratio` 函数根据给定的毒样本比例和批次大小，计算批次中应该包含的毒样本数量，特别是在毒样本比例很低的情况下，确保毒样本的随机分布。
![image-20240727232135081.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/09/attach-943eec08bb0c9fe193efc3628d36ebd94e420a27.png)
这段代码定义了一个 `Threshold` 类和一个 `generalize\_to\_lower\_pratio` 函数
### Threshold 类
`Threshold` 类继承自 `nn.Module`，定义了一个自定义的神经网络层，用于对输入数据应用 `Tanh` 激活函数并进行特定的缩放。
#### 初始化方法 (`\_\_init\_\_`)
- `\_\_init\_\_` 方法是类的构造函数，用于初始化类的实例。
- 调用 `super(Threshold, self).\_\_init\_\_()` 来初始化父类 `nn.Module`。
- 定义一个 `Tanh` 激活函数实例并将其保存在 `self.tanh` 中。
#### 前向传播方法 (`forward`)
- `forward` 方法定义了当输入数据通过该层时的计算过程。
- 接收输入 `x`，首先将其放大 20 倍，然后减去 10。
- 应用 `Tanh` 激活函数。
- 结果除以 `2 + 1e-7` 进行缩放，最后加上 0.5。
- 返回处理后的结果。
这个类可以用于将输入数据缩放到 \[0, 1\] 范围内，并具有中心化的效果。
### generalize\\_to\\_lower\\_pratio 函数
`generalize\_to\_lower\_pratio` 函数用于根据给定的比例和批次大小计算每个批次中应该包含的“毒样本”数量（可能指有特殊标记的数据样本）。
#### 函数参数
- `pratio`：表示毒样本的比例。
- `bs...