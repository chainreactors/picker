---
title: AI Could Generate 10,000 Malware Variants, Evading Detection in 88% of Case
url: https://thehackernews.com/2024/12/ai-could-generate-10000-malware.html
source: The Hacker News
date: 2024-12-24
fetch_date: 2025-10-06T19:42:41.439541
---

# AI Could Generate 10,000 Malware Variants, Evading Detection in 88% of Case

#1 Trusted Cybersecurity News Platform

Followed by 5.20+ million[**](https://twitter.com/thehackersnews)
[**](https://www.linkedin.com/company/thehackernews/)
[**](https://www.facebook.com/thehackernews)

[![The Hacker News Logo](data:image/png;base64...)](/)

**

**

[** Subscribe – Get Latest News](#email-outer)

* [** Home](/)
* [** Newsletter](#email-outer)
* [** Webinars](/p/upcoming-hacker-news-webinars.html)

* [Home](/)
* [Data Breaches](/search/label/data%20breach)
* [Cyber Attacks](/search/label/Cyber%20Attack)
* [Vulnerabilities](/search/label/Vulnerability)
* [Webinars](/p/upcoming-hacker-news-webinars.html)
* [Expert Insights](https://thehackernews.com/expert-insights/)
* [Contact](/p/submit-news.html)

**

**

**

Resources

* [Webinars](/p/upcoming-hacker-news-webinars.html)
* [Free eBooks](https://thehackernews.tradepub.com)

About Site

* [About THN](/p/about-us.html)
* [Jobs](/p/careers-technical-writer-designer-and.html)
* [Advertise with us](/p/advertising-with-hacker-news.html)

Contact/Tip Us

[**

Reach out to get featured—contact us to send your exclusive story idea, research, hacks, or ask us a question or leave a comment/feedback!](/p/submit-news.html)

Follow Us On Social Media

[**](https://www.facebook.com/thehackernews)
[**](https://twitter.com/thehackersnews)
[**](https://www.linkedin.com/company/thehackernews/)
[**](https://www.youtube.com/c/thehackernews?sub_confirmation=1)
[**](https://www.instagram.com/thehackernews/)

[** RSS Feeds](https://feeds.feedburner.com/TheHackersNews)
[** Email Alerts](#email-outer)

[![Salesforce Security Handbook](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWa8tsMNqlevi1HGF1ALQRGIq7hROPFAbHd3R1RTEOe73T8_Q2xW_-91t2jSGjU5peiPb8QYblGp4igNW-u2Qmlxbp2BKzTVMSvyXDZJmC-BYpiiJHrcnG5drmSP97iZ9PVIf1DeEr7U-7vWpe4HXwfMjt8FGNgq5mOycOJluYr9wF7YOKrQY9MfArwgjt/s728-e100/ai-agent-security-d.png)](https://thehackernews.uk/ai-agent-security-d)

# [AI Could Generate 10,000 Malware Variants, Evading Detection in 88% of Case](https://thehackernews.com/2024/12/ai-could-generate-10000-malware.html)

**Dec 23, 2024**Ravie LakshmananMachine Learning / Threat Analysis

[![AI to Generate 10,000+ Malware Variants](data:image/png;base64... "AI to Generate 10,000+ Malware Variants")](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhk-WpUXoOxT6yCK7o9muGNTyEfhXqACGiMd2F8BhiY0jteXduLFggHl9cHf9S7i9L5bxfGJVEhOeMQFlV9Ac48dqRJUPpuFFPnF68aX6mrj5-gyHj1QiQgnAFm9ZrQRo_T6SQxt7DVblePbz6yNevXwLy5hXMbqdmG6irj6vCPRxZGqxZm8017RXE8tSUm/s790-rw-e365/malware-ai.png)

Cybersecurity researchers have found that it's possible to use large language models (LLMs) to generate new variants of malicious JavaScript code at scale in a manner that can better evade detection.

"Although LLMs struggle to create malware from scratch, criminals can easily use them to rewrite or obfuscate existing malware, making it harder to detect," Palo Alto Networks Unit 42 researchers [said](https://unit42.paloaltonetworks.com/using-llms-obfuscate-malicious-javascript/) in a new analysis. "Criminals can prompt LLMs to perform transformations that are much more natural-looking, which makes detecting this malware more challenging."

With enough transformations over time, the approach could have the advantage of degrading the performance of malware classification systems, tricking them into believing that a piece of nefarious code is actually benign.

While LLM providers have increasingly enforced security guardrails to prevent them from going off the rails and producing unintended output, bad actors have advertised tools like [WormGPT](https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html) as a way to automate the process of crafting convincing phishing emails that are tailed to prospective targets and even create novel malware.

[![DFIR Retainer Services](data:image/png;base64...)](https://thehackernews.uk/cloud-insight-d)

Back in October 2024, OpenAI [disclosed](https://thehackernews.com/2024/10/openai-blocks-20-global-malicious.html) it blocked over 20 operations and deceptive networks that attempt to use its platform for reconnaissance, vulnerability research, scripting support, and debugging.

Unit 42 said it harnessed the power of LLMs to iteratively rewrite existing malware samples with an aim to sidestep detection by machine learning (ML) models like Innocent Until Proven Guilty ([IUPG](https://unit42.paloaltonetworks.com/benign-append-attacks-iupg/)) or [PhishingJS](https://unit42.paloaltonetworks.com/javascript-based-phishing/), effectively paving the way for the creation of 10,000 novel JavaScript variants without altering the functionality.

The adversarial machine learning technique is designed to transform the malware using various methods -- namely, variable renaming, string splitting, junk code insertion, removal of unnecessary whitespaces, and a complete reimplementation of the code -- every time it's fed into the system as input.

[![](data:image/png;base64...)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNBYRcpLo3AxRJBdvkoNPk6fZBQOy1diu63H3ZQHha6ArEGucCMOgClxaBckGz4z0i-hJ-RTYS9X2OkBy8K4PTpb_1dqXx4erKIzGZcNn4tRhN2dVfp_N7HzWt3jOX3qTrXlJheGPOKQBl9o-sOYH-4iP0Bxs_nwvej5kYzBp4ZlHuJo1G41JyvPvslO1U/s790-rw-e365/phishing.png)

"The final output is a new variant of the malicious JavaScript that maintains the same behavior of the original script, while almost always having a much lower malicious score," the company said, adding the greedy algorithm flipped its own malware classifier model's verdict from malicious to benign 88% of the time.

To make matters worse, such rewritten JavaScript artifacts also evade detection by other malware analyzers when uploaded to the VirusTotal platform.

Another crucial advantage that LLM-based obfuscation offers is that its lot of rewrites look a lot more natural than those achieved by libraries like obfuscator.io, the latter of which are easier to reliably detect and fingerprint owing to the manner they introduce changes to the source code.

"The scale of new malicious code variants could increase with the help of generative AI," Unit 42 said. "However, we can use the same tactics to rewrite malicious code to help generate training data that can improve the robustness of ML models."

### TPUXtract Attack Targets Google Edge TPUs

The disclosure comes as a group of academics from North Carolina State University devised a side-channel attack dubbed [TPUXtract](https://tches.iacr.org/index.php/TCHES/article/view/11923) to conduct model stealing attacks on Google Edge Tensor Processing Units ([TPUs](https://cloud.google.com/tpu/docs/intro-to-tpu)) with 99.91% accuracy. This could then be exploited to facilitate intellectual property theft or follow-on cyber attacks.

"Specifically, we show a [hyperparameter](https://en.wikipedia.org/w/index.php?title=Hyperparameter_(machine_learning)) stealing attack that can extract all layer configurations including the layer type, number of nodes, kernel/filter sizes, number of filters, strides, padding, and activation function," the researchers said. "Most notably, our attack is the first comprehensive attack that can extract previously unseen models."

The black box attack, at its core, captures electromagnetic signals emanated by the TPU when neural network inferences are underway – a consequence of the computational intensity associated with running offline ML models – and exploits them to infer model hyperparameters. However, it hinges on the adversary having physical access to a target device, not to mention possessing expensive equipment to probe and obtain the traces.

[![CIS Build Kits](data:image/png;base64...)](https://thehackernews.uk/platform-shield-d)

"Because we stole the architecture and layer details, we were able to recreate the high-level features of the AI," Aydin Aysu, one of the authors of the study, [said](https://news.ncsu.edu/2024/12/new-way-to-steal-ai-models/). "We then used that inform...