---
title: o1 发布后，信息量最大的圆桌对话：杨植麟、姜大昕、朱军探讨大模型技术路径
url: https://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&mid=2653055017&idx=1&sn=db551e83996fe03a5fbe40816936a0c8&chksm=7e57159f49209c899e9638942f4f465c0606f1a7ce0830c703c6de1baa501847769f893b42cd&scene=58&subscene=0#rd
source: 极客公园
date: 2024-09-20
fetch_date: 2025-10-06T18:28:18.344379
---

# o1 发布后，信息量最大的圆桌对话：杨植麟、姜大昕、朱军探讨大模型技术路径

![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/8cu01Kavc5YVTaGiaeJw8REiawkWu35icqG81tnW3icgWOl9vfhlqFdWictsk8DfOQfdptw7fo50UaaLIw1iaVRCls8g/0?wx_fmt=jpeg)

# o1 发布后，信息量最大的圆桌对话：杨植麟、姜大昕、朱军探讨大模型技术路径

Founder Park

极客公园

![](https://mmbiz.qpic.cn/mmbiz_png/8cu01Kavc5YVTaGiaeJw8REiawkWu35icqGVquE5zBnvloNYwuFXr2yQIdZ5KZia1YRiboibiacELrfDPRicj9mCeic93xA/640?wx_fmt=png&from=appmsg)

AI 发展很快，并且还在加速。

**作者 | Founder Park**

在 2024 云栖大会上，阶跃星辰创始人姜大昕、月之暗面Kimi创始人杨植麟、生数科技首席科学家朱军与极客公园创始人张鹏一起，探讨了各自眼中 AI 技术发展的现状，推演未来 18 个月，大模型行业会发生什么。

在这场圆桌里，他们重点聊了：

* 客观来说，AI 领域过去两年发生了什么？
* OpenAI o1 的发布对行业意味着什么？
* o1 背后的强化学习新范式对算力和数据提出了怎样的新要求？
* AI 应用层的创业，在今天该怎么做？
* 未来 18 个月，AI 技术和应用的发展路径是什么？

信息量很大，我们将现场实录整理如下。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/qpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1ItUshLZIfLhnQNVdKJXv1x8QutfmT8SSBf7PcXKvSfXUkSMjcAUOoA/640?wx_fmt=jpeg&from=appmsg)

## **01**

## **AI 发展的速度太快了**

**张鹏：OpenAI 发布到现在快两年了，这两年里引发了整个世界对 AI 的讨论。各位都是下场创业做大模型的创业者，你们的感受是怎样的？**

**我们是在「看游戏」，你们在「打游戏」，感受可能会很不一样。过去 18 个月，AI 技术的发展在减速吗？**

**姜大昕**：我觉得过去 18 个月是在加速的，而且速度还是非常快的。

过去 18 个月里发生的大大小小的 AI 事件，我们可以从两个维度去看，数量和质量。

数量上，基本上每个月都有新模型、新产品、新应用涌现出来。单说模型，OpenAI 2 月发的 Sora，过年期间把大家轰炸了一下，然后 5 月出了 GPT-4o，上周又出了 o1。OpenAI 的老对手 Anthropic，它有 Claude 3、3.5 系列，再加上 Google Gemini 系列、 Groq、Llama……

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/qpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1RS4SHJ0yWGgTbWiaRTnphQj2EiavGwibrlBNFez8OZnP8djxhfJWodviaQ/640?wx_fmt=jpeg)

去年我们的体感还是 GPT-4 一家独大、遥遥领先，今年就变成了群雄并起、你追我赶的局面。所以各家肯定是在提速。

从质量的角度来看，我觉得有三件事情给我的印象非常深刻。

第一是 GPT-4o，在多模融合领域上了一个新的台阶。之前有视觉理解模型 GPT-4v；有视觉生成模型 DALL-E、Sora；有声音模型 Whisper、Voice Engine。4o 把孤立的模型能力融合在了一起。

为什么融合非常重要？因为我们的物理世界本身就是多模态，**多模融合有助于我们更好地为物理世界建模，更好地去模拟世界。**

第二是特斯拉的 FSD v12，一个端到端的大模型，它把感知信号直接变成控制序列。我觉得自动驾驶非常有代表性，它是一个从数字世界走向物理世界的真实的应用场景。FSD v12 的成功意义不仅在于自驾本身，可以说这套方法论为将来智能设备如何与大模型结合，如何更好地探索物理世界指明了方向。

第三就是上周的 o1，它第一次证明了语言模型也可以有人脑的慢思考，也就是系统 2\* 的能力。我们一直认为 AGI 的演进路线分为模拟世界、探索世界、归纳世界。而系统 2 的能力正是归纳世界的前提条件。

注：系统 1、系统 2 来自《思考，快与慢》，系统 1 指快速的、无意识的快思考；系统 2 指有意识的慢思考。

过去几个月的时间，GPT-4o、FSD v12 和 o1 分别在这三个方向上都取得了非常大的突破，而且为将来的发展也指明了方向。所以我觉得无论是从数量还是质量来说都是可圈可点。

**张鹏：感觉你在你期待的领域里都看到了广泛的突破和进展。那植麟的体感是怎么样的？投身其中的人可能会跟我们外边「看游戏」的人不一样。**

**杨植麟**：我也觉得整体是处于加速发展阶段，AI 发展的核心可以从两个维度来看。

第一是纵向的维度，智商是一直在提升的，体现上还是去看文本模型能做到多好；第二是横向的发展，除了文本模型之外，像刚才提到的多模态，这些模态其实是在做横向的发展，它让模型具备更多技能，能够完成更多任务，然后同时跟纵向的智商发展相结合。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/qpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1Y4v7qhicgNyqxXPQiauRXiatENzv3UaWGtXhbrdOOeeOIicByzQiccrll3A/640?wx_fmt=jpeg)

在这两个维度上我都看到了非常大的进展。在纵向维度上，数学竞赛的能力去年是完全不及格，而今年已经能得到 90 多分了。代码也是一样，现在能击败很多专业编程选手。也产生了很多新的应用机会，比如说现在流行的 Cursor，能通过自然语言直接去写代码，未来这样的软件也会越来越普及。

很多具体的技术指标，比如现在的语言模型能支持的上下文长度，去年大部分模型都只能支持 4-8K 的上下文。但是今天 4-8K 已经是非常低了，128K 是标配，很多已经可以支持 1M 或者甚至 10M 的上下文长度，它其实也是智商不断提升的重要基础。

最近的很多进展不光只是在做 scaling，很多进展来自于后训练的算法优化、数据的优化，这些优化其实周期会更短，更短的优化周期也会导致整体的 AI 发展节奏进一步加快。

横向上也产生了很多新的突破。Sora 可能是影响力最大的，它完成了视频生成，最近也有特别多新的产品和技术出来，比如现在已经可以通过一篇论文，直接生成一段真假难辨的 Podcast 双人对话。未来类似这样不同模态之间的转化、交互和生成会变得越来越成熟。所以我觉得整体是在加速的过程中。

**张鹏：感觉这些技术还在加速地扩展，虽然可能没有长出 Super App，但如果抛掉 Super App 的视角，去看技术，反而能看到它真正的进展，这可能是更理性客观的视角。朱军老师，你会怎么总结这 18 个月？你觉得 AGI 的技术经历了什么样的发展？**

**朱军**：其实 AGI 里大家最关注的还是大模型，大模型方面从去年到今年发生了很多重要的变化，我非常同意整个进展在加快。另外大模型解题的速度也变快了，它的 learning curve（学习曲线）在变得更陡。

大语言模型从 2018 年到现在发展过来走了 6 年的路，去年下半年大家开始讨论多模态，到今年年初，只过了半年时间，多模态大模型的时空一致性就已经让大家震惊了。这种加速最核心的原因在于，大家对路线的认知和准备达到了比较好的程度。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/qpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl16yMic2ia3kC65R7MGpHHBZoMWxOjz0iaWPRJIrAzappZYD3wiaeJLX7YrQ/640?wx_fmt=jpeg&from=appmsg)

还有物理条件，比如云设施、计算资源的准备也在加速。ChatGPT 刚出来时大家不知所措，很多人没准备好去接受它，花了很长的时间去学习和掌握。当我们接受和掌握它之后，再去解决新的问题，它的发展速度是越来越快的。

当然，能力辐射到实际的用户身上也有快慢之分，而且也分行业。可能在广泛的角度上大家没感知到能力的进步，但从技术来说，进步的曲线越来越陡。我对高阶的 AGI 发展是比较乐观的，而且发展速度会越来越快。

## **02**

## **o1 提升了 AI 上限，**

## **带来了新范式**

**张鹏：外界有人说：「AGI 怎么发展变慢了」，三位的反应好像是：「你还想要怎样？」它的发展进程在这 18 个月里已经让我们每个人都目不暇接了。**

**OpenAI 的新模型 o1，在专业人群里产生了非常大的影响，现在还有很多讨论。先问问大昕，你怎么看 o1？很多人认为这是 AGI 发展阶段的一个重要进步，我们到底该怎么理解这个进步？**

**姜大昕**：我确实看到了一些非共识：有些人觉得 o1 意义很大，有些人觉得 o1 也不过如此。我试用 o1 的第一印象就是：它的推理能力确实非常惊艳。我们自己试了很多 query，觉得推理能力确实上了一个很大台阶。

它背后的意义究竟是什么？我能想到的有两点。

第一，**o1 第一次证明了其实 LLM 可以有人脑的慢思考，也就是系统 2 的能力。**以前 GPT 的训练范式是「预测下一个 token」。这就注定了它只有系统 1 的能力，而 o1 用了强化学习这样一个新的训练框架，所以带来了系统 2 的能力。系统 1 是直线型思维，虽然我们看到 GPT-4 可以把一个复杂问题拆解成很多步，然后分步解决，但它还是直线型的。系统 2 和系统 1 最大的区别就在于，系统 2 能够去探索不同的路径，可以自我反思、自我纠错，然后不断试错，直到找到正确的途径。

这次 o1 把以前的模仿学习和强化学习结合起来了，使模型同时有了人脑系统 1 和系统 2 的能力，我觉得从这个角度来看它的意义是非常大的。

第二，**带来了 scaling law 的新方向**。o1 试图回答的一个问题是：「强化学习究竟怎么泛化？」o1 不是第一个做强化学习的，DeepMind 一直在走强化学习路线，从 AlphaGo 到 AlphaFold 到 AlphaGeometry。DeepMind 在强化学习上非常厉害，但是以前这些强化学习都是为特定场景去设计的——AlphaGo 只能下围棋，AlphaFold 只能预测蛋白质的结构。o1 的重大意义是让强化学习的通用性和泛化性上了一个大台阶。

而且 o1 已经 scale 到了一个很大的规模，我认为它带来了一个 scaling 技术的新范式，不妨称之为 RL scaling。而且 o1 还不成熟，它还是一个开端。这点恰恰让我觉得非常兴奋，这就等于 OpenAI 跟我们说：「我找到了一条上限很高的道路，仔细思考它背后方法的话，你会发现这条路是能够走下去的」。

总的来说，o1 从能力上展示了 LLM 可以有系统 2 的能力；技术上带来了一个新的 scaling 范式，所以我觉得它的意义还是非常大的。

**张鹏：听起来，虽然说现在有非共识，感觉你是非常看好，非常认同的。朱军老师怎么看，o1 带来这一阶段的进展，你怎么评价它的意义？**

**朱军**：我的看法是，它代表着一个显著的质变。

我们对 AGI 大概做过一些分级，学术界和产业界有 L1-L5 的分级，L1 相当于聊天机器人，像 ChatGPT 等，之前大家做了很多对话。L2 叫推理者，可以做复杂问题深度思考的推理。L3 叫智能体，「数字世界」走向「物理世界」，要去改变，去交互。L4 是创新者，要去发现、创造一些新的东西，或者发现一些新的知识。L5 是组织者，它可以去协同，或者有某种组织方式更高效地运转，这是大家对于 AGI L1-L5 的分级，当然每一级也有 narrow 和 general 的区分，现在在某些任务上可以展示出来。

比如 o1 在 L2 的 narrow 场景下，在一些特定任务下已经实现了，可以达到人类很高阶的智能水平。我觉得从分级角度来看，它确实代表着整个行业的一个巨大的进步。

技术上，过去的强化学习或者其他一些技术，其实在研究里已经做出了很多东西，但能在大规模基座模型上 scale up，做出效果，从工程上或者从实现上来说，对行业来说是一个很大的触动。当然它也会触发或者激发出很多未来的探索，或者实际的研发，可能会走向从 narrow 到 general 的跃迁。这个速度我相信会很快，因为大家已经有很多准备了，我也期待这个领域里有更多人将 L2 做得更好，甚至实现更高阶的效果。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/qpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1RwuicM7aKYbgRy59585vwCnfuFX87xNX3oNCfvFfo4dMddvAnq94RPg/640?wx_fmt=jpeg&from=appmsg)

**张鹏**：感觉你对这个定义已经很高了，你看到了 AGI L2 层面显著明确的路径和阶段性成果，而之前都在 L1 的层面。当然要到大家期望的拥抱改变物理世界，最终还要往前走，到了 L3，可能这件事就真的会完整的、系统性地发生。

**回到植麟这边，这次发布 o1 之后，Sam Altman 热情洋溢地说，我们认为这是一次新范式的革命。当然 Sam 很会演讲，很会表达。我想听听你怎么看，怎么理解他说的「这是一次新的范式变革」，你是否认同？**

**杨植麟**：我觉得它的意义确实很大，**主要意义在于它提升了 AI 的上限**。

AI 的上限是说，（能）去提升 5%、10% 的生产力，还是 10 倍的 GDP？我觉得这里最重要的问题就是，能不能通过强化学习进一步 scaling。所以我觉得这（o1) 是一个完全提升了 AI 上限的东西。

如果我们看 AI 历史上七八十年的发展，唯一有效的就是 scaling，加更多的算力。在 o1 出来之前，也有很多人在研究强化学习，但都没有一个非常确切的答案，强化学习如果和大语言模型，或者和 pre-training、post-training 整合在一起，它能不能持续提升？比如 GPT-4 这一代模型的提升，更多是确定性的提升，在一样的范式下把规模变得更大。

**但是我觉得 o1 的提升并不是一个完全确定性的提升。**

在之前，大家可能会担心数据墙的问题，现在互联网上大部分优质数据都已经被使用完了，也没有更多的数据可以挖掘，所以原来的范式可能会遇到问题。AI 有效了，又需要进一步 scaling，那这个 scaling 从哪里来？我觉得（o1）很大程度上解决了这个问题，或者说至少证明了初步可行。初步可行的情况下，可能会有越来越多人投入去做这个事情，最终要做到 10 倍 GDP 的最终效果，它完全有可能，我觉得是一个很重要的开端。

当然，我觉得对很多产业格局，或者对于创业公司的新机会来讲，也会发生一些变化。比如这里很关键的一个点是，训练和推理算力占比会发生很大的变化，这个变化不是说训练的算力会下降，训练的算力还会持续提升，但与此同时，推理的算力提升会更快，这个比例的变化本质上会产生很多新的机会，会有很多新的创业公司的机会。

一方面，达到一定算力门槛的公司，可以做很多算法的基础创新，甚至可以在基础模型上取得突破，我觉得这个很重要。而对于算力相对小一点的公司，也可以通过后训练的方式，在一些领域上做到最好的效果，也会产生更多的产品和技术机会，所以我觉得，整体也打开了创业相关的想象空间。

**张鹏**：所以这一次核心的所谓范式变化，带来的就是在 Scaling Law 上解决了我们接下来的 scale what，我们看到了新的路径，并且未来可拓展的创新路径空间和探索的东西变多了，而不像原来，是一个收缩甚至是遇阻的状况。

## **03**

## **推理能力泛化路径还****不明确，**

## **是一个新的技术变量**

**张鹏：想问问朱军老师，今天在一个阶段性的、还比较明确的一些场景里，这种把 RL 加到体系里面成为一个新的范式之后，我们能看到明显地去泛化这个能力的路径吗？**

**朱军**：这个问题确实很值得思考，因为现在它先是在一些任务上能取得突破，我们再想着把它做到更广泛的任务上，或者有更广泛的能力提升上。从目前来看，o1 没有完全告诉我们技术路线是怎么做的。

**张鹏**：明显没有 ChatGPT 出来前那么 open。

**朱军**：对，但是从本身科研的积累解读，能看到它到底用了哪些技术。

这里有一个很重要的问题，叫过程监督的数据，它和之前的结果直接 output 的监督还不太一样，要对里面的每一步都去标注，比如思考的过程，获取这种数据首先可能就比较难，需要专业的人去做专业的高价值数据。

另外，在实际做的过程中，包括大家之前看 AlphaGo 迁移到其他领域面临同样的问题，在更泛化，或者**更开放的场景下，Reward Model 不好定义**。

比如说，现在有确定答案的定理证明或者是编程问题，Reward 是比较明确的，奖励函数是很容易定义的。但如果到自动驾驶、具身，或者艺术创作里面，比如生图、生视频，这里面的界定是比较模糊的，可能很多场景下是很难清晰地定义到底什么好，什么不好，可能很多问题不是「是」和「非」的问题，比如像生成式内容，对美学或者对其他的评价，每个人感受还不太一样。在这种情况下要去泛化的话，技术上就面临很多问题，我怎么定义 Reward Model，怎么收集数据，还包括怎么高效地实现，给它 scale up。

现在大家看到这条路了，相当于已经看到曙光，会引导大家朝着这个方向去努力。另外，结合现在比较强大的基座模型，可能比之前上一代 AlphaGo 迁移到其他领域里，我相信会更快，包括像一些开放领域里，我们有更好的模拟器，甚至包括一些 AGI 的生成方式来构建这个环境。这些加持在一起，我想这条路会走得更快一点，会比之前更容易取得效果和提升。

**张鹏：今天确实还没有看到一个公开、明确的，可以确定性把这个泛化完成的路径，但它存在探索的空间和足够的可能性。想追问一下植麟，这个状态对于像你们这样的创业公司，是好事还是坏事？**

**杨植麟**：我觉得这其实是一个很好的机会，因为等于说有了一个新的技术变量，是一个新的技术维度。

当然这个我们之前或多或少也有一些投入，但是现在可能它会变成一个主题，在这个主题下面，我们会有非常多新的机会。一方面是朱军老师提到的怎么去泛化的问题，另一方面是，在这个过程中还有一些基础的技术问题没有被完全解决，底层涉及到训练和推理，这两个东西要同时去 scaling，很多问题今天还没有...