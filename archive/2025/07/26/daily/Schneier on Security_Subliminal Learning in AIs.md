---
title: Subliminal Learning in AIs
url: https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html
source: Schneier on Security
date: 2025-07-26
fetch_date: 2025-10-06T23:54:55.406293
---

# Subliminal Learning in AIs

# [Schneier on Security](https://www.schneier.com/)

Menu

* [Blog](https://www.schneier.com)
* [Newsletter](https://www.schneier.com/crypto-gram/)
* [Books](https://www.schneier.com/books/)
* [Essays](https://www.schneier.com/essays/)
* [News](https://www.schneier.com/news/)
* [Talks](https://www.schneier.com/talks/)
* [Academic](https://www.schneier.com/academic/)
* [About Me](https://www.schneier.com/blog/about/)

### Search

*Powered by [DuckDuckGo](https://duckduckgo.com/)*

Blog

Essays

Whole site

### Subscribe

[![Atom](https://www.schneier.com/wp-content/uploads/2019/10/rss-32px.png)](https://www.schneier.com/feed/atom/)[![Facebook](https://www.schneier.com/wp-content/uploads/2019/10/facebook-32px.png)](https://www.facebook.com/bruce.schneier)[![Twitter](https://www.schneier.com/wp-content/uploads/2019/10/twitter-32px.png)](https://twitter.com/schneierblog)[![Email](https://www.schneier.com/wp-content/uploads/2019/10/email-32px.png)](https://www.schneier.com/crypto-gram)

[Home](https://www.schneier.com)[Blog](https://www.schneier.com/blog/archives/)

## Subliminal Learning in AIs

Today’s freaky [LLM behavior](https://alignment.anthropic.com/2025/subliminal-learning/):

> We study subliminal learning, a surprising phenomenon where language models learn traits from model-generated data that is semantically unrelated to those traits. For example, a “student” model learns to prefer owls when trained on sequences of numbers generated by a “teacher” model that prefers owls. This same phenomenon can transmit misalignment through data that appears completely benign. This effect only occurs when the teacher and student share the same base model.

Interesting security implications.

I am more convinced than ever that we need serious research into [AI integrity](https://www.schneier.com/essays/archives/2025/06/the-age-of-integrity.html) if we are ever going to have [trustworthy AI](https://www.schneier.com/essays/archives/2025/06/ai-and-trust-2.html).

Tags: [academic papers](https://www.schneier.com/tag/academic-papers/), [AI](https://www.schneier.com/tag/ai/), [integrity](https://www.schneier.com/tag/integrity/), [LLM](https://www.schneier.com/tag/llm/), [trust](https://www.schneier.com/tag/trust/)

[Posted on July 25, 2025 at 7:10 AM](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html) •
[15 Comments](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html#comments)

### Comments

Hendrik •
[July 25, 2025 7:34 AM](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html/#comment-446698)

The only trust you should have is the meta/domain knowledge of the interpreter of the AI’s output. AI doesn’t have a “Trust model” that doesn’t have glaring holes in some or other fashion/reason, either the model, the training set or the corpus for responses.

Brandt •
[July 25, 2025 10:25 AM](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html/#comment-446700)

This strikes me as just very convoluted steganography? The message (owls) is hidden in the training data (sequences of numbers) in a way that is not easily detected by an observer. But two sufficiently elaborate LLMs can decode the message, as long as they share the same key (base model).

lurker •
[July 25, 2025 10:26 AM](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html/#comment-446701)

@Bruce

From your June 12, 2025 paper on AI and Trust:

“AIs are not people; they don’t have agency.”

To which you should add:

“LLMs may be Artificial, but they are not Intelligent. They do not know many things that people know, and they cannot learn them.”

The notion of a LLM as the basis for intelligence is flawed. Sentient beings learn about their environment from their senses. Language comes later as a means to describe this knowledge. A machine that attempts to learn knowledge of its environment from samples of constructed language is doomed to get lost interpreting cause from effect. And that’s before we get into semantics.

[Eitan Caspi](https://fudie.net/) •
[July 25, 2025 3:08 PM](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html/#comment-446703)

In my view there is a fundamental problem with AI:

Until now we, humans, tried to figure out everything around us, usually with science, decipher the mysteries of our existence, turning any black box to be familiar, transparent and controlled.

With AI we are turning the other way round – we create a black box, one that we (at least most us) don’t know why it is doing what it is doing, a system we cannot tame from start nor reverse engineer it – into something controlled. And we heading towards giving it control of our lives. Very risky.

D-503 •
[July 25, 2025 6:11 PM](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html/#comment-446705)

I read the blog post. This finding isn’t surprising or “freaky” at all.

This needs to be emphasized over and over again. While there are underlying similarities with how the human brain works, it’s a huge mistake to anthropomorphize LLMs by talking about “subliminal learning” or about “hallucinations” or about whether data are semantically related or not.
For the LLMs, the inputs and outputs are meaningless strings of arbitrary symbols. The LLM outputs the symbols that are statistically most likely to follow based on the training data, with an element of randomization added in.

The security implications of anything marketed as “AI” were already clear in the 1960s with the Eliza Effect.
en.wikipedia.org/wiki/ELIZA\_effect

Clive Robinson •
[July 25, 2025 8:03 PM](https://www.schneier.com/blog/archives/2025/07/subliminal-learning-in-ais.html/#comment-446708)

@ Bruce, ALL,

With regards,

> “Interesting security implications.”

Actually is it even unexpected?

I’ve talked about Claude Shannon and his proof that for information to be transmitted in a medium or “channel” there has to be indeterminacy thus “redundancy”.

Likewise Gus Simmons proving that where you have a transmission medium or channel the redundancy means that another transmission channel within the first becomes automatically created as an unavoidable artifact.

These “created channels” within a “channel with redundancy” become like the famed “turtles all the way down” you get channels within channels within channels created all the way down as long as there is redundancy to do so (and there always has to be redundancy).

Two relevant questions that arise are,

1, Can an observer prove such channels are being used to “deliberately” –by other parties– transfer information?

2, What is the bandwidth available in these channels within channels to such other parties?

The answer to the proof of usage was answered by Claude Shannon and he called it “Perfect Secrecy” and most know it as the idea behind the “One Time Pad” that “all message are equiprobable”. So the answer is a resounding “NO”. Which means that the channels within channels can be “covert”. Or overt (think about various forms of “error detection” and correction).

The answer to the second question is a bit more complicated. Shannon based on work by Ralph Hartley and Harry Nyquist came up with limits of how much information could be sent in any given time in any given channel based on it’s characteristics and what is regarded as noise (other information). Thus an overly simple answer would be Channel Bandwidth minus the Overt information Bandwidth gives a figure for the maximum Covert information Bandwidth.

The reality is it always has to be less than that, due to other information in the channels. Because transmitting information is provably “doing work”. As has been established as a basic law of physics all work is “inefficient” (with the information by the process of radiation transport / radiative transfer becomes less and less coherent and becomes what most call heat).

The consequence of this is there will always be “...