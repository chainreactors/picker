---
title: Handling Arbitrarily Nested Structures with Burp Suite
url: https://blog.silentsignal.eu/2024/12/06/custom-decoder-for-burp/
source: Silent Signal Techblog
date: 2024-12-07
fetch_date: 2025-10-06T20:00:33.732766
---

# Handling Arbitrarily Nested Structures with Burp Suite

[![Silent Signal](/assets/img/s2_avatar.jpg)](/)

Silent Signal

Professional Ethical Hacking Services

### Contact us

2025 © Silent Signal

![Handling Arbitrarily Nested Structures with Burp Suite](/img/iza-gawrych-oL3O2PybLoo-unsplash.jpg)

# Handling Arbitrarily Nested Structures with Burp Suite

E 2024-12-06

*This is a blog post by Erik Szinai, who worked with us as an intern during the last couple of months. We hope our readers will find his contribution to the Burp Suite ecosystem useful!*

Burp usually excels in automatically detecting arbitrarily nested structures, encodings and datatypes found in HTTP requests and responses, however, during one of our security assessments we came across a strange phenomenon.
It was an API testing scenario, where the body of a POST request contained a Base64-encoded XML.
At first, we thought no biggie, Burp will handle this like it usually does, but to our surprise it couldn’t detect the underlying structure, which obviously meant that running an Active Scan on that specific request resulted in nothing useful.
At that point, we thought that the problem arose because of the nested structure or the length of the XML, however, those assumptions couldn’t have been further from the truth, as it turned out that the actual issue was rather absurd: the XML body contained the string “Olá mundo”, and when the XML was Base64-encoded, the Spanish “á” completely blindfolded Burp’s detection logic.

[![](/img/meme1.jpeg)](/img/meme1.jpeg)

After we debugged that issue, Dnet quickly wrote a PoC Insertion Point Provider extension using the old Wiener API to tackle this exact problem, however, we thought it’d be cool to have a more general and refined solution to handle nested data structures, just in case we come across a similar problem in the future.
Since the way how Burp detects these structures and encodings is essentially a black box, if shit hits the fan, it’s really tough to discover what went off the rails (and that’s assuming that we do realize that something is off).
Also, it’s important to note that our goal wasn’t simply about solving the charset problem, but to have a proper extension that can **detect nested, even unknown serializable objects over which we have total control**.
That’s why we decided to create an easily customizable extension that can detect and automatically encode / decode even the most messed up encodings and nested structures with a reasonable one-time setup cost.

[![](/img/gui.png)](/img/gui.png)

Our extension basically serves as an Insertion Point Provider, but unlike most implementations, this one has a GUI.
A request can be sent to the extension using the context menu, where each parameter of the original request is decoded and shown in a tree-like view.
From there, we can do two things:

* Modify one of the values and send a single request with the newly crafted parameter. We don’t have to deal with manually encoding the whole thing, which would be a really tedious, time-consuming and nailbiting task, as the extension takes care of it: it keeps track of the parent-child hierarchy with the corresponding encodings, so it automatically constructs the new parameter in the right type of encoding.
* The second option is to select multiple nodes, which will then act as insertion points and start an Active Scan on these. Here we also have the option to decide how we want the payload to be “processed”: we can either choose to replace the whole value with the payload generated by Burp, or we can choose so the payload only gets appended to our selected value(s), which could come in handy when we are dealing with injection-type vulnerabilities for example.

One drawback we have experienced with our extension is connected to the payloads generated by Burp.
We did some research on this topic and we also experimented with it, and found out that Burp dynamically tries to guess what kind of payloads it should insert based on the characteristics of previous responses and some other factors. However, its success is a bit of a hit-and-miss when dealing with custom insertion points.
We have tried to set the correct insertion point types for the custom IPs (e.g.,
we set `PARAM_XML_ATTR` enum constant for an XML attribute), but Burp seemingly ignored this and didn’t put more focus on XML-related payloads for the given insertion point (in fact, it didn’t even send one).
This could be solved by manually editing the audit’s configuration, though.

If you want to get involved in the more technical details, how we implemented the extension, and if (for some weird reason) you are missing some recursive methods from your life, feel free to read the next section.

Otherwise, you can just [grab the extension from GitHub](https://github.com/silentsignal/BurpNestedEncoder) right away!

## Technical Details

Since we wanted our extension to be easily customizable, we opted to create an interface that defines all of the methods one encoding type should implement, which we named `HandleEncoding`.
These methods for example include the logic to decide whether a given string can be considered one specific encoding.
For instance, detecting a Base64-encoded strings can be done using regexes, however, this approach might produce some false positives, and to mitigate this, we introduced a threshold limit regarding the minimum length of the string.
For JSON, we did a pre-check: first, we examined whether the first non-whitespace character is either ‘[’ or ‘{‘ (we did similarly with the closing brackets), and only tried to parse the string if the previous check has passed - with this approach the detection process can become more resource-efficient.

The decoding / parsing aspect is an interesting one as well.
Let’s assume that we successfully detected that we’re dealing with a Base64-encoded value - decoding a Base64-encoded string is nothing extraordinary, we just have to use the Base64 utilities offered by the Montoya API.
However, with serializable objects, like JSON, it became a tad more challenging.
One of the main issue here is the fact that we know absolutely nothing about the structure or scheme of the incoming object, which means we can’t use the well-known parsers, like [GSON](https://github.com/google/gson) or [Jackson](https://github.com/FasterXML/jackson) in the traditional way.
Serializing them into POJOs is also out of the equation (well, not quite, it could definitely be doable, but still) due to the above-mentioned issue, and besides that, it wouldn’t even get us any further: we have no intention of actually treating them like objects, we just want to extract every key-pair value out of them.
And the answer to this problem is recursion, what else.
We created a `JsonElement` using the GSON from the given string, and then we recursively traversed the whole object extracting every single key-value pair.
Inserting a payload into a JSON object went down in a pretty much similar fashion, where we took advantage of the fact the keys should be unique in a JSON object.

### Data Representation

One of the most crucial and critical question during the design phase was which datatype to use to represent nested encodings.
In the beginning, we experimented with list-like datatypes, however, managing the complex parent-child hierarchy within the list quickly became a headache.
After some head-scratching, we realized that the answer is relatively simple, and should have been obvious from the beginning: trees, more specifically an n-ary tree.
A tree is a nearly perfect fit for our problem: we have an initial value (we could say a root value), which, when decoded, might have a lot of children representing decoded values, which, in turn, might also have some children, and so on:

[![](/img/moricka.png)](/img/moricka.png)

Of course, a Base64-encoded JSON containing another Base64-encoded JSON can rarely be spotted in the wild, we just wanted to demonstrate the chosen datatype and the capability of our extension.
Oh, and tree...