---
title: From Misuse to Abuse: AI Risks and Attacks
url: https://thehackernews.com/2024/10/from-misuse-to-abuse-ai-risks-and.html
source: The Hacker News
date: 2024-10-17
fetch_date: 2025-10-06T18:59:31.380846
---

# From Misuse to Abuse: AI Risks and Attacks

#1 Trusted Cybersecurity News Platform

Followed by 5.20+ million[**](https://twitter.com/thehackersnews)
[**](https://www.linkedin.com/company/thehackernews/)
[**](https://www.facebook.com/thehackernews)

[![The Hacker News Logo](data:image/png;base64...)](/)

**

**

[** Subscribe – Get Latest News](#email-outer)

* [** Home](/)
* [** Newsletter](#email-outer)
* [** Webinars](/p/upcoming-hacker-news-webinars.html)

* [Home](/)
* [Data Breaches](/search/label/data%20breach)
* [Cyber Attacks](/search/label/Cyber%20Attack)
* [Vulnerabilities](/search/label/Vulnerability)
* [Webinars](/p/upcoming-hacker-news-webinars.html)
* [Expert Insights](https://thehackernews.com/expert-insights/)
* [Contact](/p/submit-news.html)

**

**

**

Resources

* [Webinars](/p/upcoming-hacker-news-webinars.html)
* [Free eBooks](https://thehackernews.tradepub.com)

About Site

* [About THN](/p/about-us.html)
* [Jobs](/p/careers-technical-writer-designer-and.html)
* [Advertise with us](/p/advertising-with-hacker-news.html)

Contact/Tip Us

[**

Reach out to get featured—contact us to send your exclusive story idea, research, hacks, or ask us a question or leave a comment/feedback!](/p/submit-news.html)

Follow Us On Social Media

[**](https://www.facebook.com/thehackernews)
[**](https://twitter.com/thehackersnews)
[**](https://www.linkedin.com/company/thehackernews/)
[**](https://www.youtube.com/c/thehackernews?sub_confirmation=1)
[**](https://www.instagram.com/thehackernews/)

[** RSS Feeds](https://feeds.feedburner.com/TheHackersNews)
[** Email Alerts](#email-outer)

[![Salesforce Security Handbook](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWa8tsMNqlevi1HGF1ALQRGIq7hROPFAbHd3R1RTEOe73T8_Q2xW_-91t2jSGjU5peiPb8QYblGp4igNW-u2Qmlxbp2BKzTVMSvyXDZJmC-BYpiiJHrcnG5drmSP97iZ9PVIf1DeEr7U-7vWpe4HXwfMjt8FGNgq5mOycOJluYr9wF7YOKrQY9MfArwgjt/s728-e100/ai-agent-security-d.png)](https://thehackernews.uk/ai-agent-security-d)

# [From Misuse to Abuse: AI Risks and Attacks](https://thehackernews.com/2024/10/from-misuse-to-abuse-ai-risks-and.html)

**Oct 16, 2024**The Hacker NewsArtificial Intelligence / Cybercrime

[![AI Risks and Attacks](data:image/png;base64... "AI Risks and Attacks")](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLaamtUPMEbLWCvUyKrFSzeFKWAc-lBiD2fy3vv9k97Wl831STdRS8NmG1t7DUibFfpWA2ZZPMKgChzNHv5OuzrF3KDZCqRLZz7V0q9LL4I-3SFvpZJmLfsHzxfjBtLwSrEvTlcVXqGfyB3XogqIoY7PR4AAm3Ji4jfq3uyjJC-593VQNT0dq7vVbUU0o/s790-rw-e365/misuse-to-abuse-for-hn.png)

*AI from the attacker's perspective: See how cybercriminals are leveraging AI and exploiting its vulnerabilities to compromise systems, users, and even other AI applications*

## Cybercriminals and AI: The Reality vs. Hype

"AI will not replace humans in the near future. But humans who know how to use AI are going to replace those humans who don't know how to use AI," says Etay Maor, Chief Security Strategist at [Cato Networks](https://www.catonetworks.com/?utm_source=hackernews&utm_medium=referral) and founding member of [Cato CTRL](https://www.catonetworks.com/cato-ctrl/?utm_source=hackernews&utm_medium=referral). "Similarly, attackers are also turning to AI to augment their own capabilities."

Yet, there is a lot more hype than reality around AI's role in cybercrime. Headlines often sensationalize AI threats, with terms like "Chaos-GPT" and "Black Hat AI Tools," even claiming they seek to destroy humanity. However, these articles are more fear-inducing than descriptive of serious threats.

[![AI Risks and Attacks](data:image/png;base64... "AI Risks and Attacks")](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTJTLrq9v977JRVFXS4qBFXzjJgoxvucHcYLkn-IL06n3mongU4idr0geynX-sm-xLFFLI-Q3Tblbi0lCTvXbgSem0onW3vF42mSK6TckBXrWO5mg9RIGeKUOz9hrbbw_B-CzYnljrAg5Hy_1sZH1ypxnrY4F_dboF9VJFTdEgDR_dLfwVI1DAHK9qbUw/s790-rw-e365/1.png)

For instance, when explored in underground forums, several of these so-called "AI cyber tools" were found to be nothing more than rebranded versions of basic public LLMs with no advanced capabilities. In fact, they were even marked by angry attackers as scams.

## How Hackers are Really Using AI in Cyber Attacks

In reality, cybercriminals are still figuring out how to harness AI effectively. They are experiencing the same issues and shortcomings legitimate users are, like hallucinations and limited abilities. Per their predictions, it will take a few years before they are able to leverage GenAI effectively for hacking needs.

[![AI Risks and Attacks](data:image/png;base64... "AI Risks and Attacks")](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqtVND5OVN5XD4zATxeKsgu3uJv2UBhSRc-I7EYK0jYDZBmvPBEam_D7uXvXXLWygXvTsJ2fbV2B1QkY2qf8145G_jh9q9D_53zl_imTVsllu4K9EDMl5urb3fBjvFZOhjwORlXckDpAIzbEIu_KOEOdab5YiZauZKOTNcbuaIB33qWerM_Lc_9iWF2ho/s790-rw-e365/2.png)

[![AI Risks and Attacks](data:image/png;base64... "AI Risks and Attacks")](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGYyJKv48INe4A0hqWW03dwnzX64x2yQBKUMN6e1et-DPvERBABo877tb4KXy3dBSEzyTAGCHPsoUYbMxESnEjCGD9UaAgXAi2BrKHzRHKRP-fzvGmnB93XdpXL_97w5UlRnAXakckwwyl-do6a1cNNlG2qAHvx-S2_uBuLr-2RPgRu4VGSsvz1tgW7qQ/s790-rw-e365/3.png)

For now, GenAI tools are mostly being used for simpler tasks, like writing phishing emails and generating code snippets that can be integrated into attacks. In addition, we've observed attackers providing compromised code to AI systems for analysis, as an effort to "normalize" such code as non-malicious.

## Using AI to Abuse AI: Introducing GPTs

GPTs, introduced by OpenAI on November 6, 2023, are customizable versions of ChatGPT that allow users to add specific instructions, integrate external APIs and incorporate unique knowledge sources. This feature enables users to create highly specialized applications, such as tech support bots, educational tools, and more. In addition, OpenAI is offering developers monetization options for GPTs, through a dedicated marketplace.

### Abusing GPTs

GPTs introduce potential security concerns. One notable risk is the exposure of sensitive instructions, proprietary knowledge, or even API keys embedded in the custom GPT. Malicious actors can use AI, specifically prompt engineering, to replicate a GPT and tap into its monetization potential.

Attackers can use prompts to retrieve knowledge sources, instructions, configuration files, and more. These might be as simple as prompting the custom GPT to list all uploaded files and custom instructions or asking for debugging information. Or, sophisticated like requesting the GPT to zip one of the PDF files and create a downloadable link, asking the GPT to list all its capabilities in a structured table format, and more.

"Even protections that developers put in place can be bypassed and all knowledge can be extracted," says Vitaly Simonovich, Threat Intelligence Researcher at Cato Networks and Cato CTRL member.

These risks can be avoided by:

* Not uploading sensitive data
* Using instruction-based protection though even those may not be foolproof. "You need to take into account all the different scenarios that the attacker can abuse," adds Vitaly.
* OpenAI protection

## AI Attacks and Risks

There are multiple frameworks existing today to assist organizations that are considering developing and creating AI-based software:

* NIST Artificial Intelligence Risk Management Framework
* Google's Secure AI Framework
* OWASP Top 10 for LLM
* OWASP Top 10 for LLM Applications
* The [recently launched](https://www.darkreading.com/threat-intelligence/mitre-launches-ai-incident-sharing-initiative) MITRE ATLAS

### LLM Attack Surface

There are six key LLM (Large Language Model) components that can be targeted by attackers:

1. **Prompt** - Attacks like prompt injections, where malicious input is used to manipulate the AI's output
2. **Response** - Misuse or leakage of sensitive information in AI-generated responses
3. **Model** - Theft, poisoning, or manipulation of the AI model
4. **T...