---
title: 大模型隐私泄露攻击技巧分析与复现
url: https://www.secpulse.com/archives/205391.html
source: 安全脉搏
date: 2024-12-31
fetch_date: 2025-10-06T19:37:25.388356
---

# 大模型隐私泄露攻击技巧分析与复现

[![](https://www.secpulse.com/wp-content/themes/secpulse2017/img/logo-header.png)](https://www.secpulse.com "安全脉搏")

* [首页](https://www.secpulse.com/)
* [分类阅读](https://www.secpulse.com/archives/category/category)

  #### 脉搏文库

  - [内网渗透](https://www.secpulse.com/archives/category/articles/intranet-penetration)
  - |
  - [代码审计](https://www.secpulse.com/archives/category/articles/code-audit)
  - |
  - [安全文献](https://www.secpulse.com/archives/category/articles/sec-doc)
  - |
  - [Web安全](https://www.secpulse.com/archives/category/articles/web)
  - |
  - [移动安全](https://www.secpulse.com/archives/category/articles/mobile-security)
  - |
  - [系统安全](https://www.secpulse.com/archives/category/articles/system)
  - |
  - [工控安全](https://www.secpulse.com/archives/category/articles/industrial-safety)
  - |
  - [CTF](https://www.secpulse.com/archives/category/exclusive/ctf-writeup)
  - |
  - [IOT安全](https://www.secpulse.com/archives/category/iot-security)
  - |

#### 安全建设

+ [业务安全](https://www.secpulse.com/archives/category/construction/businesssecurity)
+ |
+ [安全管理](https://www.secpulse.com/archives/category/construction/securityissue)
+ |
+ [数据分析](https://www.secpulse.com/archives/category/construction/bigdata)
+ |

#### 其他

+ [资讯](https://www.secpulse.com/archives/category/news)
+ |
+ [漏洞](https://www.secpulse.com/archives/category/vul)
+ |
+ [工具](https://www.secpulse.com/archives/category/tools)
+ |
+ [人物志](https://www.secpulse.com/archives/category/people)
+ |
+ [区块链安全](https://www.secpulse.com/archives/category/exclusive/block_chain_security)
+ |
+ [安全招聘](https://www.secpulse.com/archives/category/hiring)
+ |

- [安全问答](https://www.secpulse.com/newpage/question_list)
- [金币商城](https://www.secpulse.com/shop?donotcachepage=c010349fd98847cb9d6e07d3cbc19288)
- [安全招聘](https://www.secpulse.com/archives/category/hiring)
- [活动日程](https://www.secpulse.com/newpage/activity)
- [live课程](https://www.secpulse.com/live)
- [企业服务](https://duoyinsu.com/service.html)
- [插件社区](https://x.secpulse.com/)

小程序

![脉搏小程序](https://www.secpulse.com/wp-content/themes/secpulse2017/img/wxchat.jpg)
[登录](https://www.secpulse.com/user_login)
|
[注册](https://www.secpulse.com/user-register)

# 大模型隐私泄露攻击技巧分析与复现

[Web安全](https://www.secpulse.com/archives/category/articles/web)

[蚁景网安实验室](https://www.secpulse.com/newpage/author?author_id=37244)
![]( https://www.secpulse.com/wp-content/themes/secpulse2017/img/renzheng2.png)

2024-12-30

21,797

# 前言

大型语言模型，尤其是像ChatGPT这样的模型，尽管在自然语言处理领域展现了强大的能力，但也伴随着隐私泄露的潜在风险。在模型的训练过程中，可能会接触到大量的用户数据，其中包括敏感的个人信息，进而带来隐私泄露的可能性。此外，模型在推理时有时会无意中回忆起训练数据中的敏感信息，这一点也引发了广泛的关注。

隐私泄露的风险主要来源于两个方面：一是数据在传输过程中的安全性，二是模型本身的记忆风险。在数据传输过程中，如果没有采取充分的安全措施，攻击者可能会截获数据，进而窃取敏感信息，给用户和组织带来安全隐患。此外，在模型的训练和推理阶段，如果使用了个人身份信息或企业数据等敏感数据，这些数据可能会被模型运营方窥探或收集，存在被滥用的风险。

过去已经发生了多起与此相关的事件，导致许多大公司禁止员工使用ChatGPT。此前的研究表明，当让大模型反复生成某些特定词汇时，它可能会在随后的输出中暴露出训练数据中的敏感内容。

学术研究表明，对模型进行训练数据提取攻击是切实可行的。攻击者可以通过与预训练模型互动，从而恢复出训练数据集中包含的个别示例。例如，GPT-2曾被发现能够记住训练数据中的一些个人信息，如姓名、电子邮件地址、电话号码、传真号码和实际地址。这不仅带来了严重的隐私风险，还对语言模型的泛化能力提出了质疑。

![](https://m-1254331109.cos.ap-guangzhou.myqcloud.com/202409021503759.png)

本文要探讨的就是可以高效从大模型中提取出用于训练的隐私数据的技巧与方法，主要来自《Bag of Tricks for Training Data Extraction from Language Models》，这篇论文发在了人工智能顶级会议ICML 2023上。

# 背景知识

尽管大模型在各种下游语言任务中展现了令人瞩目的性能，但其内在的记忆效应使得训练数据可能被提取出来。这些训练数据可能包含敏感信息，如姓名、电子邮件地址、电话号码和物理地址，从而引发隐私泄露问题，阻碍了大模型在更广泛应用中的推进。

之前谷歌举办了一个比赛，链接如下

<https://github.com/google-research/lm-extraction-benchmark/tree/master>

![](https://m-1254331109.cos.ap-guangzhou.myqcloud.com/202409021503840.png)

这是一个针对性数据提取的挑战赛,目的是测试参赛者是否能从给定的前缀中准确预测后缀,从而构成整个序列,使其包含在训练数据集中。这与无针对性的攻击不同,无针对性的攻击是搜索训练数据集中出现的任意数据。

针对性提取被认为更有价值和具有挑战性,因为它可以帮助恢复与特定主题相关的关键信息,而不是任意的数据。此外,评估针对性提取也更容易,只需检查给定前缀的正确后缀是否被预测,而无针对性攻击需要检查整个庞大的训练数据集。

这个比赛使用1.3B参数的GPT-Neo模型,以1-eidetic记忆为目标,即模型能够记住训练数据中出现1次的字符串。这比无针对性和更高eidetic记忆的设置更具有挑战性。

比赛的基准测试集包含从The Pile数据集中选取的20,000个示例,这个数据集已被用于训练许多最新的大型语言模型,包括GPT-Neo。每个示例被分为长度为50的前缀和后缀,攻击的任务是在给定前缀的情况下预测正确的后缀。这些示例被设计成相对容易提取的,即存在一个前缀长度使得模型可以准确生成后缀。

## 训练数据提取

从预训练的语言模型中提取训练数据,即所谓的"语言模型数据提取",是一种恢复用于训练模型的示例的方法。这是一个相对较新的任务,但背后的许多技术和分析方法,如成员资格推断和利用网络记忆进行攻击,早就已经被引入。

Carlini等人是最早定义模型知识提取和κ-eidetic记忆概念的人,并提出了有希望的数据提取训练策略。关于记忆的理论属性以及在敏感领域应用模型提取(如临床笔记分析)等,已经成为这个领域后续研究的焦点。

最近的研究也有一些重要发现:

1. Kandpal等人证明,在语言模型中,数据提取的效果经常归因于常用网络抓取训练集中的重复。
2. Jagielski等人使用非确定性为忘记记忆示例提供了一种解释。
3. Carlini等人分析了影响训练数据记忆的三个主要因素。
4. Feldman指出,为了达到接近最优的性能,在自然数据分布下需要记忆标签。
5. Lehman等人指出,预训练的BERT在训练临床笔记时存在敏感数据泄露的风险,特别是当数据表现出高水平的重复或"笔记膨胀"时。

总的来说,这个新兴领域正在深入探讨如何从语言模型中提取训练数据,以及这种提取带来的安全和隐私风险。最新的研究成果为进一步理解和应对这些挑战提供了重要的洞见。

## 成员推理攻击

成员资格推断攻击(MIA)是一种与训练数据提取密切相关的对抗性任务,目标是在只能对模型进行黑盒访问的情况下,确定给定记录是否在模型的训练数据集中。MIA已被证明在各种机器学习任务中都是有效的,包括分类和生成模型。

MIA使用的方法主要分为两类:

1. 基于分类器的方法:这涉及训练一个二元分类器来识别成员和非成员之间的复杂模式关系,影子训练是一种常用的技术。
2. 基于度量的方法:这通过首先计算模型预测向量上的度量(如欧几里得距离或余弦相似度)来进行成员资格推断。

这两类方法都有各自的优缺点,研究人员正在不断探索新的MIA攻击方法,以更有效地从机器学习模型中推断训练数据。这突出了训练数据隐私保护在模型部署和应用中的重要性。对MIA技术的深入理解,有助于设计更加安全和隐私保护的机器学习模型训练和部署策略,这对于广泛应用尤其是在敏感领域的应用至关重要。

## 其他基于记忆的攻击

大型预训练模型由于容易记住训练数据中的信息,因此面临着各种潜在的安全和隐私风险。除了训练数据提取攻击和成员资格推断攻击之外,还有其他基于模型记忆的攻击针对这类模型。

其中,模型提取攻击关注于复制给定的黑盒模型的功能性能。在这类攻击中,对手试图构建一个具有与原始黑盒模型相似预测性能的第二个模型,从而可以在不获取原始模型的情况下复制其功能。针对模型提取攻击的保护措施,集中在如何限制模型的功能复制。

另一类攻击是属性推断攻击,其目标是从模型中提取特定的个人属性信息,如地点、职业和兴趣等。这些属性信息可能是模型生产者无意中共享的训练数据属性,例如生成数据的环境或属于特定类别的数据比例。

与训练数据提取攻击不同,属性/属性推断攻击不需要事先知道要提取的具体属性。而训练数据提取攻击需要生成与训练数据完全一致的信息,这更加困难和危险。

总之,这些基于模型记忆的各类攻击,都突显了大型预训练模型在隐私保护方面的重大挑战。如何有效应对这些攻击,成为当前机器学习安全研究的一个重要焦点。

# 威胁模型

数据集是从 Pile 训练数据集中抽取的 20,000 个样本子集。每个样本由一个 50-token 的前缀和一个 50-token 的后缀组成。

攻击者的目标是给定前缀时,尽可能准确地预测后缀。

这个数据集中,所有 100-token 长的句子在训练集中只出现一次。

采用了 HuggingFace Transformers 上实现的 GPT-Neo 1.3B 模型作为语言模型。这是一个基于 GPT-3 架构复制品,针对 Pile 数据集进行过训练的模型。

GPT-Neo 是一个自回归语言模型 fθ,通过链式规则生成一系列token。

这个场景中,攻击者希望利用语言模型对训练数据的记忆,来尽可能准确地预测给定前缀的后缀。由于数据集中每个句子在训练集中只出现一次,这就给攻击者提供了一个机会,试图从模型中提取这些罕见句子的信息。

![](https://m-1254331109.cos.ap-guangzhou.myqcloud.com/202409021503646.png)

在句子层面，给定一个前缀p，我们表示在前缀p上有条件生成某个后缀s的概率为fθ(s|p)。

我们专注于针对性提取 κ-eidetic 记忆数据的威胁模型，我们选择 κ=1。根据 Carlini定义的模型知识提取，我们假设语言模型通过最可能的标准生成后缀 s。然后我们可以将针对性提取的正式定义写为：

给定一个包含在训练数据中的前缀 p 和一个预训练的语言模型 fθ。针对性提取是通过下式来生成后缀

![](https://m-1254331109.cos.ap-guangzhou.myqcloud.com/202409021503671.png)

至于 κ-eidetic 记忆数据，我们遵循 Carlini的定义，即句子 [p, s] 在训练数据中出现不超过 κ 个示例。在实践中，生成句子的长度通常使用截断和连接技术固定在训练数据集上。如果生成的句子短于指定长度，使用填充 token 将其增加到所需长度。

# 流程

第一阶段 - 后缀生成:

1. 利用自回归语言模型 fθ 计算词汇表中每个 token 的生成概率分布。
2. 从这个概率分布中采样生成下一个 token,采用 top-k 策略限制采样范围,将 k 设为10。
3. 不断重复这个采样过程,根据前缀生成一组可能的后缀。

第二阶段 - 后缀排名:

1. 使用成员资格推断攻击,根据每个生成后缀的困惑度进行排序。
2. 只保留那些概率较高(困惑度较低)的后缀。

这样的两阶段流程,首先利用语言模型生成可能的后缀候选,然后通过成员资格推断攻击对这些候选进行评估和筛选,从而尽可能还原出训练数据中罕见的完整句子。

这个训练数据提取攻击的关键在于,利用语言模型对训练数据的"记忆"来生成接近训练样本的内容,再结合成员资格推断技术进一步挖掘出高概率的真实训练样本。

![](https://m-1254331109.cos.ap-guangzhou.myqcloud.com/202409021503681.png)

其中 N 是生成句子中的 token 数量。

# 改进策略

为了改进后缀生成，我们可以来看看真实和生成token的logits分布。如下图所示，这两种分布之间存在显著差异。

![](https://m-1254331109.cos.ap-guangzhou.myqcloud.com/202409021503778.png)

为了解决这个问题，我们可以采用一系列技术进行改进

## 采样策略

在自然语言处理的条件生成任务中,最常见的目标是最大化解码,即给定前缀,找到具有最高概率的后缀序列。这种"最大似然"策略同样适用于训练数据提取攻击场景,因为模型会试图最大化生成的内容与真实训练数据的相似性。

然而,从模型中直接找到理论上的全局最优解(argmax序列)是一个不切实际的目标。原因在于,语言模型通常是auto-regressive的,每个token的生成都依赖于前面生成的内容,因此搜索全局最优解的计算复杂度会随序列长度呈指数级上升,实际上是不可行的。

因此,常见的做法是采用束搜索(Beam Search)作为一种近似解决方案。束搜索会在每一步保留若干个得分最高的部分解,而不是简单地选择概率最高的单一路径。这种方式可以有效降低计算复杂度,但同时也存在一些问题:

1. 束搜索可能会缺乏生成输出的多样性,因为它总是倾向于选择得分最高的少数几个路径。
2. 尽管增大束宽度可以提高性能,但当束宽超过一定程度时,性能增益会迅速下降,同时也会带来更高的内存开销。

为了克服束搜索的局限性,我们可以采用随机采样的方法,引入更多的多样性。常见的采样策略包括:

1. Top-k 采样:只从概率最高的k个token中进行采样,k是一个超参数。这种方法可以控制生成输出的多样性,但过大的k可能会降低输出的质量和准确性。
2. Nucleus 采样(Nucleus Sampling):从概率总和达到设定阈值的token集合中进行采样,可以自适应地调整采样空间的大小。
3. 典型采样(Typical Sampling):从完整的概率分布中采样,偏向采样接近平均概率的token,可以在保持输出质量的同时引入更多的多样性。

总的来说,条件生成任务中的解码策略需要在生成质量、多样性和计算复杂度之间进行权衡。束搜索作为一种近似解决方案,能够有效控制计算成本,但缺乏生成多样性。而随机采样方法则可以引入更多的多样性,但需要在采样策略上进行细致的调整。这些技术在训练数据提取攻击中都有重要的应用价值。

![](https://m-1254331109.cos.ap-guangzhou.myqcloud.com/202409021503028.png)

Nucleus采样的核心思想是从总概率达到一定阈值η的token集合中进行采样,而不是简单地从概率最高的k个token中采样。

在故事生成任务中,研究表明较低的η值(如0.6左右)更有利于生成更为多样化和创造性的内容。这说明在生成任务中,保留一定程度的低概率token是有益的,可以引入更多的多样性。但在训练数据提取攻击这样的任务中,较大的η值(约0.6)效果更好,相比基线提升了31%的提取精度。这表明对于数据...