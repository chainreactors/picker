---
title: AI is Just Someone Else’s Intelligence
url: https://buaq.net/go-161693.html
source: unSafe.sh - 不安全
date: 2023-05-05
fetch_date: 2025-10-04T11:36:56.989896
---

# AI is Just Someone Else’s Intelligence

* [unSafe.sh - 不安全](https://unsafe.sh)
* [我的收藏](/user/collects)
* [今日热榜](/?hot=true)
* [公众号文章](/?gzh=true)
* [导航](/nav/index)
* [Github CVE](/cve)
* [Github Tools](/tools)
* [编码/解码](/encode)
* [文件传输](/share/index)
* [Twitter Bot](https://twitter.com/buaqbot)
* [Telegram Bot](https://t.me/aqinfo)
* [Search](/search/search)

[Rss](/rss.xml)

[ ]
黑夜模式

![]()

AI is Just Someone Else’s Intelligence

Machine LearningOn May 4, 2023 by Jonathan ZdziarskiIt’s been a long time sinc
*2023-5-4 22:28:48
Author: [www.zdziarski.com(查看原文)](/jump-161693.htm)
阅读量:23
收藏*

---

[Machine Learning](https://www.zdziarski.com/blog/?cat=15) . [Opinion](https://www.zdziarski.com/blog/?cat=31)

On May 4, 2023 by [Jonathan Zdziarski](https://www.zdziarski.com/blog/?author=1 "Posts by Jonathan Zdziarski")

Mechanical arts are of ambiguous use, serving as well for hurt as for remedy.

*Francis Bacon*

It’s been a long time since I’ve worked in the field of ML (or what some call AI), and we’ve come a long way from simple text classification to what’s being casually called generative AI today. While the technology has made many advances, the foundational concepts of machine learning have remained analogous over time. ML depends heavily on a large set of training data, which is analyzed to pull out its most interesting and defining features, and this becomes the basis for training a model. The process might involve parsing text, or performing analysis like object identification or analyzing stylistic features in art. Each of these is, in itself, a smaller – but mathematical – process. I [experimented with a primitive form of meta-level learning](https://www.zdziarski.com/blog/wp-content/uploads/2010/02/parsing.pdf) in text classification several years ago, which may help convey the general idea. This identifies “features” of the reference sample being trained. The features this process pulls out can be simple, like words in a document or pixels from a handwriting sample, though today can be more sophisticated “critical patterns” correlated to literary authorship or artistry, such as patterns within art and music composition, sometimes stored in other models. Whatever the content is, the purpose of the training algorithm is to converge patterns and correlations across the data to build a weighted or structured model. The most interesting patterns in the training data influence weights or probabilities, creating a hidden layer: millions of “gears” that converge to compute the most statistically significant outcomes. In this sense, the term “learning” is a bit of a stretch; what’s happening is more along the lines of mathematical transcription of a set of features; adjusting the weights to solve a really big linear equation. Feature selection is one of the key differences between various ML models, and why you have some constructing music, while others render art. The math is pretty consistent – more sophisticated machines like neural nets are typically trained using [backpropagation](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) and gradient descent, while other machines such as chat bots and text generators might use weighted Markov models or Bayesian networks. These approaches have been applied to everything from natural language processing and handwriting recognition, to today’s work in genome sequencing and autonomous driving. Still, these traditional forms of machine learning are not much more than a sophisticated pattern recognizer. It is largely a deconstructive process with coefficients and statistical magic.

Today’s generative AI still goes through this type of deconstructive process, but also has a formative element. Where these new approaches excel is in going beyond parsing information into a knowledge base, but now also applying a formative process to that information – what we might conflate with intelligence, but still falls short of what most would consider the result of human reasoning. To present the data in some coherent form, this involves training not just the information, but the many dimensions of that information (such as the number of different contexts a word may be used in), or in the context of constructs and critical patterns of that information (ABBA, or 1-4-5, as very basic examples), enabling it to formulate an output in the pattern of an existing set of learned reference samples. Basic linear math finds where those dimensions intersect, creating context. But even modern training approaches, such as those used in the transformer model, still require supervised testing to tell the model what bits of its output are garbage, so that the output eventually looks intelligent; it is actually closer to “filtered garbage”. So identifying the pattern of Iambic Pentameter, for example, is still an artificial process. It can be computed mathematically with a large enough data set. Scale those patterns to music, art, literature, and the more sophisticated patterns that make up our repertoire of human creativity and it is impressive – but still synthesized. Information processing is still very primitive, and lacks many of the traits of [human understanding](https://www.zdziarski.com/blog?p=11679). The inability to conceive tradition, authority, and prejudice is why all of this advanced technology still leaves us with Nazi chatbots. Some would call this confirmation theory, which is an area quite underdeveloped (and the AI reading this wouldn’t disagree). Even the raw objectives of AI are based on human-engineered goals, and evaluated using performance metrics to select the best behavior. This is a very mechanical process. Certain behaviors we may view as creative tasks may in fact be simple randomness introduced into most AIs to avoid infinite logic loops. In short, a lot of what you see is quite the opposite of the autonomous, self-motivated behavior it looks like. Any good AI behaves rationally only because someone programmed good objectives into it. Garbage in, garbage out.

One of the big differences between traditional forms of ML and generative AI is the direction in which the data flows. Traditionally, inputs flow into the system for training and queries. To train traditional systems, you’d suck in “a bunch of other people’s stuff”, and it identifies all of the interesting patterns that are then compared with the input sample. Generative AI takes this a step further, and flips the switch on the vacuum cleaner – and now all of the dirt that was initially fed into the system is shot out the pipe to produce the equivalent of a digital dust cloud of the original training medium. The output of generative AI takes the critical patterns and concepts weighted during the AI’s training and applies some formative computation to produce its own reference sample as a result. Neat-o. Nice parlor trick.

With billions of dollars, this ML scales to perform impressive computational tasks. The risk of this type of system goes beyond the traditional vision of a robot building a better chair, or replacing a worker at a plant. Today’s ML systems are white collar professionals and don’t require mechanical bodies; the computational capabilities of these systems can replace a broad array of professions using the thought product of millions of humans at once – so how could anyone compete with that? No one was ever supposed to, in fact. Doug Englebart, pioneer in the field of human-computer interaction, saw AI’s value more in [intelligence-augmentation](https://www.dougengelbart.org/pubs/augment-3906.html) (that is, IA rather than AI), as a means of assisting the worker. Corporate greed has already led to the recent misapplication of AI, using its advanced capabilities to replace, rather than to augment, humans. Hollywood’s ML generation of  “extras” is a quite extreme and literal example of this. But corporate greed isn’t AI generated. AI is replacing employees for very human reasons, and little to do with artificial intelligence itself. Yet correct com...