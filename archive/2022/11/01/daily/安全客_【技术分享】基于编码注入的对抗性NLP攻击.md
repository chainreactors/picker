---
title: 【技术分享】基于编码注入的对抗性NLP攻击
url: https://mp.weixin.qq.com/s?__biz=MzA5ODA0NDE2MA==&mid=2649779398&idx=1&sn=eb96bd241b30cd6d510873d55020d588&chksm=88935ca9bfe4d5bfb14a3a43a2ece51eb9458a3f8ce5b64a9606899df680cdb79e3caf4f0366&scene=58&subscene=0#rd
source: 安全客
date: 2022-11-01
fetch_date: 2025-10-03T21:26:25.535448
---

# 【技术分享】基于编码注入的对抗性NLP攻击

![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/Ok4fxxCpBb6qwN9aVJRhY5ahJSBPQKavfcIp0JtcGfFwnefWSjpcd9hckLd8HOPLUDicoHmoyAiadrMzmXAzs49w/0?wx_fmt=jpeg)

# 【技术分享】基于编码注入的对抗性NLP攻击

原创

CDra90n

安全客

![](https://mmbiz.qpic.cn/mmbiz_jpg/Ok4fxxCpBb5xDlUZWXiaXXqyqS8exhPsGxyqib5q70HIXYPI1TWBzPicic5warSvbq5EUpdtCawiboibkR8iabiaxtoTbg/640?wx_fmt=jpeg)

研究表明，机器学习系统在理论和实践中都容易受到对抗样本的影响。到目前为止，此类攻击主要针对视觉模型，利用人与机器感知之间的差距。尽管基于文本的模型也受到对抗性样本的攻击，但此类攻击难以保持语义和不可区分性。在本文中探索了一大类对抗样本，这些样本可用于在黑盒设置中攻击基于文本的模型，而无需对输入进行任何人类可感知的视觉修改。使用人眼无法察觉的特定于编码的扰动来操纵从神经机器翻译管道到网络搜索引擎的各种自然语言处理 (NLP) 系统的输出。通过一次难以察觉的编码注入——不可见字符（invisible character）、同形文字（homoglyph）、重新排序（reordering）或删除（deletion）——攻击者可以显着降低易受攻击模型的性能，通过三次注入后，大多数模型可以在功能上被破坏。除了 Facebook 和 IBM 发布的开源模型之外，本文攻击还针对当前部署的商业系统，包括 Microsoft 和 Google的系统。这一系列新颖的攻击对许多语言处理系统构成了重大威胁：攻击者可以有针对性地影响系统，而无需对底层模型进行任何假设。结论是，基于文本的 NLP 系统需要仔细的输入清理，就像传统应用程序一样，鉴于此类系统现在正在快速大规模部署，因此需要架构师和操作者的关注。

**Introduction**

x 和 х 在你看来是一样的吗？人类可能看它们相同，但大多数自然语言处理系统不同。字符串“123”中有多少个字符？如果你猜100，你是对的。第一个示例包含拉丁字符 x 和西里尔字符 h，它们通常以相同的方式呈现。第二个示例在可见字符之后包含 97 个零宽不连字（zero-width non-joiners）。

研究表明，机器学习系统在理论上和实践中都容易受到对抗样本的影响。此类攻击最初针对图像分类中使用的视觉模型，但最近对自然语言处理和其他应用程序产生了兴趣。本文对基于文本的模型提出了一大类强大的对抗样本攻击。这些攻击使用不可见字符、控制字符和同形文字（共享相似字形的不同字符编码）应用输入扰动。这些扰动对于基于文本系统的人类用户来说是察觉不到的，但是用于编码它们的字节可以极大地改变输出。

处理用户提供的文本的机器学习模型，例如神经机器翻译系统，特别容易受到这种攻击，例如市场领先的服务 Google Translate 。在撰写本文时，在英语到俄语模型中输入字符串“paypal”正确输出“PayPal”，但将输入中的拉丁字符 a 替换为西里尔字母 а 会错误地输出“папа”（英语中的“father”） . 模型管道对其字典外的字符不可知，并用 <unk> 标记替换它们；然而，调用它们的软件可能会将未知词从输入传播到输出。虽然这可能有助于对文本的一般理解，但它扩展了一个攻击面。

过去会使用简单的文本编码攻击来通过垃圾邮件过滤器获取邮件。比如2018年的SpamAssassin项目就曾有过关于如何处理零宽度字符的简单讨论，在一些sextortion骗局中已经发现了这个问题。从机器翻译到版权执法再到仇恨言论过滤，NLP 系统在大范围应用中的快速部署，创造了大量高价值目标。

![](https://mmbiz.qpic.cn/mmbiz_png/Ok4fxxCpBb5xDlUZWXiaXXqyqS8exhPsGqfFB9tGjrnzojqibVwqUicXzyzIkzq306B5ldZFiamZwSaVicYCyE46fcQ/640?wx_fmt=png)

这项工作的主要贡献是探索和开发一类基于编码的难以察觉的攻击，并研究它们对现在大规模部署的 NLP 系统的影响。实验表明，此类系统的许多开发人员都忽视了风险；鉴于对利用未净化输入的多种系统进行攻击的悠久历史，这令人惊讶。在上表中提供了一组跨各种 NLP 任务的不可察觉攻击的示例。正如稍后将描述的，这些攻击采用不可见字符、同形文字、重新排序和删除的形式，通过遗传算法注入，该算法最大化定义的损失函数每个 NLP 任务。
本文研究结果提出了一个攻击向量，在设计任何系统处理自然语言时必须考虑该向量，无论是直接从 API 还是通过文档解析，都可以使用现代编码摄取基于文本的输入。然后探索了一系列防御措施，可以针对这组强大的攻击提供一些保护，例如在标记化之前丢弃某些字符、限制字内字符集混合以及利用渲染和 OCR 进行预处理。但是，防御并非完全简单，因为应用程序要求和资源限制可能会阻止在某些情况下使用特定防御。

**Motivation**

研究人员已经尝试对 NLP 模型进行对抗攻击。然而，到目前为止，此类攻击对人工检查是显而易见的，并且可以相对容易地识别出来。如果攻击者插入单个字符的拼写错误，它们会显得格格不入，而释义通常会改变文本的含义，足以引起注意。在本文中讨论的攻击是针对现代 NLP 模型的第一类攻击，这些攻击是不可察觉的并且不会扭曲语义。

攻击在实践中会造成重大伤害。假设 Alice 破解了 Bob 的 Office365 帐户并更改了他的发票模板，使其仍然显示为“Pay account no. 123“，但在不知不觉中受到干扰，因此谷歌翻译会将其呈现为不同的帐号。然后 Bob 将这些带有陷阱的发票发送给他的客户，当 Carlos 用西班牙语读到一张发票时，他将钱转给 Alice。

通过使人类易于阅读但机器难以处理来隐藏文本的能力，可以被许多不良行为者用来逃避平台内容过滤机制，甚至阻碍执法和情报机构。同样的扰动甚至会阻止正确的搜索引擎索引，使恶意内容首先难以定位。发现生产搜索引擎不会解析不可见字符，并且可以通过精心设计的查询恶意定位。在最初撰写本文时，谷歌搜索“The meaning of life”返回了大约 9.9 亿条结果。在披露该问题之前，搜索包含 250 个不可见的“零宽不连字”的视觉上相同的字符串时，完全没有返回。

**Related Work**

### **A. 对抗样本**

机器学习技术容易受到许多大类别的攻击，其中一个主要类别是对抗性样本。这些是模型的输入，在推理过程中这些输入会导致模型输出不正确的结果。在白盒环境中——攻击者知道模型——可以使用许多基于梯度的方法找到这样的样本，这些方法通常旨在在一系列约束下最大化损失函数。在模型未知的黑盒设置中，攻击者可以从另一个模型传输对抗样本，或者通过观察输出标签和在某些设置中的置信度来近似梯度。

训练数据也可能被毒化以操纵特定输入的模型准确性。在推理过程中可以引入位错误以降低模型的性能。还可以选择输入来最大化模型在推理过程中花费的时间或能量，或者通过推理技术公开机密的训练数据。换句话说，对抗性算法会影响机器学习系统的完整性、可用性和机密性。

### **B. NLP模型**

自然语言处理 (NLP) 系统旨在处理人类语言。机器翻译早在 1949 年就被提出，并已成为 NLP 的一个关键子领域。机器翻译的早期方法往往是基于规则的，使用人类语言学家的专业知识，但随着该领域的成熟，统计方法变得更加突出，最终使用了神经网络，然后是循环神经网络 (RNN)，因为拥有参考过去上下文的能力。当前最先进的技术是 Transformer 模型，它通过使用注意力机制在传统网络中提供了 RNN 和 CNN 的好处。

Transformer 是编码器-解码器模型的一种形式，将序列映射到序列。每种源语言都有一个编码器，可将输入转换为学习的跨语言，一种中间表示，然后使用与该语言关联的模型将其解码为目标语言。

无论用于翻译的模型的细节如何，自然语言都必须以可用作其输入的方式进行编码。最简单的编码是将单词映射到数字表示的字典，但这无法对以前看不见的单词进行编码，因此词汇量有限。N-gram 编码可以提高性能，但会以指数方式增加字典大小，同时无法解决看不见的词问题。一种常见的策略是在编码之前将单词分解为子词段，因为这可以在许多情况下对以前看不见的单词进行编码和翻译。

### **C. 对抗性NLP**

早期的对抗性机器学习研究侧重于图像分类，后来开始在 NLP 系统中搜索对抗性样本，目标是序列模型。由于自然语言的离散性，对抗样本本质上更难制作。与可以以近乎连续且几乎察觉不到的方式调整像素值以最大化损失函数的图像不同，对自然语言的扰动更加明显，并且涉及对更多离散标记的操纵。

更一般地，将提供针对人类用户的有效对抗样本的源语言扰动需要考虑语义相似性。研究人员提出使用基于单词的输入交换与同义词或基于字符的交换与语义约束。这些方法旨在将扰动限制为一组人类不太可能注意到的转换。神经机器翻译和文本分类模型通常在拼写错误等嘈杂输入上表现不佳，但这种扰动会产生清晰的视觉伪影（visual artifact），更容易被人类注意到。

使用具有相同含义的不同释义，而不是一对一的同义词，可能会提供更多的回旋余地。释义集可以通过比较大型文本语料库的机器反向翻译来生成，并用于系统地生成机器翻译系统的对抗性样本。还可以在嵌入空间中搜索输入句子的邻居；这些例子通常会导致低性能的翻译，使它们成为对抗样本的候选者。尽管释义确实有助于保留语义，但人们经常注意到结果看起来很奇怪。另一方面，本文攻击不会引入任何可见的扰动，使用更少的替换并完美地保留语义。

遗传算法已被用于发现对抗情绪分析系统输入的对抗性扰动，在黑盒设置中提出了一种可行的攻击，而无需访问梯度。强化学习可用于有效地为翻译模型生成对抗性样本。与本文中描述的技术不同，所有现有的 NLP 对抗性样本技术都会在输入中产生人类可感知的视觉伪影。

虽然 BLEU 通常用于评估自然语言设置中的各种准确度指标，但不太常见的相似度指标（例如 chrF）可能会为对抗样本提供更强的结果。未知标记 <unk>用于编码 NLP 设置中自然语言编码器无法识别的文本序列，由于编码为 <unk > 的字符的灵活性，可以利用它们来制作引人注目的源语言扰动。然而，迄今为止提出的用于生成 <unk> 的所有方法都使用可见字符。在下表中展示了对抗性 NLP 攻击的分类。

![](https://mmbiz.qpic.cn/mmbiz_png/Ok4fxxCpBb5xDlUZWXiaXXqyqS8exhPsG1aYpc65dSbzXJVibEnNVib55NPBxpibq0Pn61LlTSaj6a8nWBozRQkUnA/640?wx_fmt=png)

### **D. Unicode**

Unicode 是一个字符集，旨在标准化文本的电子表示。在撰写本文时，它可以代表 143,859 个跨多种不同语言和符号组的字符。拉丁字母、繁体汉字、数学符号和表情符号等各种字符都可以用 Unicode 表示。它将每个字符映射到一个代码点或数字表示。

这些数字代码点通常用前缀 U+ 表示，可以用多种方式编码，尽管 UTF-8 是最常见的。这是一种将代码点表示为 1-4 个字节的可变长度编码方案。字体是描述应如何呈现代码点的字形集合。大多数计算机支持许多不同的字体。不需要字体的每个代码点都有一个字形，没有相应字形的代码点通常呈现为“未知”占位符字符。

### **E. Unicode安全**

由于它必须支持全球广泛的语言集，Unicode 规范非常复杂。这种复杂性会导致安全问题，正如 Unicode 联盟关于 Unicode 安全考虑的技术报告中所详述的那样。

Unicode 规范中的一个主要安全考虑因素是多种编码同形文字的方法，同形文字是共享相同或几乎相同字形的独特字符。这个问题不是 Unicode 独有的；例如，在 ASCII 范围内，小写拉丁语“l”的呈现通常与大写拉丁语“I”几乎相同。在某些字体中，字符序列可以充当伪同形文字，例如大多数无衬线字体中的序列“rn”和“m”。

这种视觉技巧为网络诈骗者提供了一种工具。发现的最早的例子是 paypal.com（注意最后一个域名字符是大写的“I”），它在 2000 年 7 月被用来诱骗用户泄露 paypal.com 的密码。实际上，此后对 URL 中的同形文字给予了极大的关注。一些浏览器试图通过在导航时以小写形式呈现所有 URL 字符来纠正这种歧义，并且 IETF 设置了一个标准来解决非 ASCII 字符与 ASCII 字符同形异义词之间的歧义。该标准称为 Punycode，将非 ASCII URL 解析为仅限于 ASCII 范围的编码。

例如，大多数浏览器会自动将 URL paypаl.com（使用西里尔文 а）重新呈现为其 Punycode 等效项 xn–pypl-53dc.com，以突出显示潜在危险的歧义。但是，Punycode 可以引入新的欺骗机会。例如，URL xn–google.com 解码为四个语义上没有意义的繁体中文字符。此外，Punycode 无法解决 URL 之外的跨脚本同形文字编码漏洞。例如，单字形过去曾在各种非 URL 区域（例如证书通用名称）中造成安全漏洞。

Unicode 攻击也可以利用字符排序。某些字符集（例如希伯来语和阿拉伯语）自然地按从右到左的顺序显示。混合从左到右和从右到左文本的可能性，就像在阿拉伯报纸中引用英语短语一样，需要一个系统来管理混合字符集的字符顺序。对于 Unicode，这是双向 (Bidi) 算法。Unicode 指定了多种控制字符，允许文档创建者微调字符顺序，包括允许完全控制显示顺序的两个双向覆盖字符。最终效果是，攻击者可以强制字符以不同于编码顺序的顺序呈现，从而允许由各种不同的编码序列表示相同的视觉呈现。

最后，一整类漏洞源于 Unicode 实现中的错误。这些历史上一直被用来产生一系列有趣的漏洞利用，很难概括。虽然 Unicode Consortium 确实发布了一组支持 Unicode 的软件组件，但许多操作系统、平台和其他软件生态系统都有不同的实现。例如，GNOME 生成 Pango，Apple 生成 Core Text，而 Microsoft 生成 Windows 的 Unicode 实现。在接下来的内容中，将主要忽略错误并专注于利用 Unicode 标准的正确实现的攻击。相反，利用了可视化和 NLP 管道之间的差距，如下图所示。

![](https://mmbiz.qpic.cn/mmbiz_png/Ok4fxxCpBb5xDlUZWXiaXXqyqS8exhPsG6Jic0f5ICzNKESA8ApY9eHiczcjJCqRWs2XibBTNIKpPaK2VOXDRZibic5w/640?wx_fmt=png)

**Backgroud**

### **A. 攻击分类**

在本文中，探讨了基于 Unicode 和其他通常适用于基于文本的 NLP 模型的编码约定的不可察觉攻击类别。将每次攻击视为一种对抗样本，其中将难以察觉的扰动应用于现有基于文本的 NLP 模型的固定输入。将这些难以察觉的扰动定义为对文本字符串编码的修改，导致：

• 与未受干扰的输入相比，符合标准的渲染引擎不会对字符串的渲染进行视觉修改，

• 视觉上的修改足够微妙，以至于使用普通字体的普通人类读者不会注意到。

对于后一种情况，也可以通过两个字符串的渲染图像之间的计算机视觉模型或这种渲染之间的最大像素差异来代替人类的不可感知性作为不可区分性。考虑了对 NLP 模型的四种不同类别的不可察觉的攻击：

1) 不可见字符：按设计不呈现为可见字形的有效字符用于扰乱模型的输入。

2) 同形文字：呈现为相同或视觉上相似的字形的独特字符用于扰乱模型的输入。

3) 重新排序：方向性控制字符用于覆盖字形的默认渲染顺序，允许对用作模型输入的编码字节进行重新排序。

4) 删除：删除控制字符，例如退格符，被注入到一个字符串中，以从其视觉渲染中删除注入的字符，以扰乱模型的输入。

这些对 NLP 模型的基于文本的难以察觉的攻击代表了一种抽象的攻击类别，独立于不同的文本编码标准和实现。出于具体样本和实验结果的目的，将假设几乎无处不在的 Unicode 编码标准，相信本文结果可推广到任何具有足够大字符和控制序列集的编码标准。

存在更多基于文本的攻击类别，但所有其他攻击类别都会产生视觉假象。本文中描述的基于文本的难以察觉的攻击可用于广泛的 NLP 模型。正如稍后解释的那样，不可察觉的扰动可以操纵机器翻译，破坏投毒内容分类器，降低搜索引擎查询和索引，并为DoS攻击生成sponge样本，以及其他可能性。

### **B. NLP管道**

现代 NLP 管道经过数十年的研究发展，包括大量性能优化。在模型推断之前，基于文本的输入经历了许多预处理步骤。通常，分词器首先以对任务有意义的方式应用于分离单词和标点符号，例如本文稍后评估的 Fairseq 模型中使用的 Moses 分词器。然后对标记化的词进行编码。早期的模型使用字典将标记映射到编码的嵌入，在训练期间看不到的标记被替换为一个特殊的<unk>嵌入。许多现代模型现在在字典查找之前应用字节对编码 (BPE) 或 WordPiece 算法。BPE（一种常见的数据压缩技术）和 WordPiece 都可以识别标记中的常见子词。这通常会提高性能，因为它允许模型从语素中获取有关语言语义的额外知识。这两种预处理方法通常用于部署的 NLP 模型，包括本文评估的 Facebook 和 IBM 发布的所有三个开源模型。

如前图所示，现代 NLP 管道以与文本渲染系统非常不同的方式处理文本，即使处理相同的输入也是如此。NLP 系统处理人类语言的语义，而渲染引擎处理大量不同的控制字符。模型所见与人类所见之间的这种结构差异是在攻击中所利用的。

### **C. 攻击方法**

将对抗样本的生成视为一个优化问题。假设 NLP 函数 f(x) = y : X → Y 将文本输入 x 映射到 y。根据任务，Y 是字符、单词或热编码类别的序列。例如，WMT 等翻译任务假定 Y 是字符序列，而 MNLI 等分类任务假定 Y 是三个类别之一。此外假设一个强大的黑盒威胁模型，其中攻击者可以访问模型输出但无法观察内部结构。这使攻击变得现实：稍后证明它可以安装在现有的商业 ML 服务上。在这个威胁模型中，攻击者的目标是使用扰动函数 p 在不知不觉中操纵 f。这些操作分为两类：

• 完整性攻击：攻击者的目标是找到满足 f(p(x)) = f(x) 的 p。对于有针对性的攻击，攻击者进一步约束 p，使得扰动输出与固定目标 t 匹配：f(...