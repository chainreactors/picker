---
title: Hacking ChatGPT by Planting False Memories into Its Data
url: https://www.schneier.com/blog/archives/2024/10/hacking-chatgpt-by-planting-false-memories-into-its-data.html
source: Schneier on Security
date: 2024-10-02
fetch_date: 2025-10-06T18:55:26.227692
---

# Hacking ChatGPT by Planting False Memories into Its Data

# [Schneier on Security](https://www.schneier.com/)

Menu

* [Blog](https://www.schneier.com)
* [Newsletter](https://www.schneier.com/crypto-gram/)
* [Books](https://www.schneier.com/books/)
* [Essays](https://www.schneier.com/essays/)
* [News](https://www.schneier.com/news/)
* [Talks](https://www.schneier.com/talks/)
* [Academic](https://www.schneier.com/academic/)
* [About Me](https://www.schneier.com/blog/about/)

### Search

*Powered by [DuckDuckGo](https://duckduckgo.com/)*

Blog

Essays

Whole site

### Subscribe

[![Atom](https://www.schneier.com/wp-content/uploads/2019/10/rss-32px.png)](https://www.schneier.com/feed/atom/)[![Facebook](https://www.schneier.com/wp-content/uploads/2019/10/facebook-32px.png)](https://www.facebook.com/bruce.schneier)[![Twitter](https://www.schneier.com/wp-content/uploads/2019/10/twitter-32px.png)](https://twitter.com/schneierblog)[![Email](https://www.schneier.com/wp-content/uploads/2019/10/email-32px.png)](https://www.schneier.com/crypto-gram)

[Home](https://www.schneier.com)[Blog](https://www.schneier.com/blog/archives/)

## Hacking ChatGPT by Planting False Memories into Its Data

This vulnerability hacks a feature that allows ChatGPT to have long-term memory, where it uses information from past conversations to inform future conversations with that same user. A researcher [found](https://arstechnica.com/security/2024/09/false-memories-planted-in-chatgpt-give-hacker-persistent-exfiltration-channel/) that he could use that feature to plant â€œfalse memoriesâ€ into that context window that could subvert the model.

> A month later, the researcher submitted a new disclosure statement. This time, he included a PoC that caused the ChatGPT app for macOS to send a verbatim copy of all user input and ChatGPT output to a server of his choice. All a target needed to do was instruct the LLM to view a web link that hosted a malicious image. From then on, all input and output to and from ChatGPT was sent to the attackerâ€™s website.

Tags: [AI](https://www.schneier.com/tag/ai/), [LLM](https://www.schneier.com/tag/llm/), [vulnerabilities](https://www.schneier.com/tag/vulnerabilities/)

[Posted on October 1, 2024 at 7:07 AM](https://www.schneier.com/blog/archives/2024/10/hacking-chatgpt-by-planting-false-memories-into-its-data.html) â€¢
[9 Comments](https://www.schneier.com/blog/archives/2024/10/hacking-chatgpt-by-planting-false-memories-into-its-data.html#comments)

### Comments

Clive Robinson â€¢
[October 1, 2024 9:37 AM](https://www.schneier.com/blog/archives/2024/10/hacking-chatgpt-by-planting-false-memories-into-its-data.html/#comment-440870)

Well not quite yet the season to be jolly but,

> *â€œA researcher found that he could use that feature to plant â€œfalse memoriesâ€ into that context window that could subvert the model.â€*

Does give me a fun idea ğŸ˜‰

If it can â€œgo to any web siteâ€ then it should be possible to make it do like â€œAlexa once didâ€ and get it to buy you an Xmas Present or threeâ€¦

Just saying ğŸ˜‰

More seriously weâ€™ve not given these current AI ML LLM systems â€œphysical agencyâ€ but they do have â€œinformational agencyâ€ to a certain extent.

Getting an AI to â€œtime delay a purchaseâ€ is a POC of other â€œtime delay payloadsâ€ that in turn have â€œphysical implicationsâ€ in the â€œreal worldâ€.

I doubt anyone at these AI Vendors in their rush to get their next big thing out to market have actually thought about their systems being used to getting â€œphysical agencyâ€ in the â€œreal worldâ€ via having the ability to act as an â€œinformation agentâ€, using non obvious â€œembeddedâ€ commands.

Funny thing is Issac Asimov got there with a story back in the 1950â€™s. But more famously in a more refined form is the story arc in

For those that donâ€™t know the AI in the film called HAL had secret information put in itâ€™s memory about an alien artifact, that the astronauts onboard were unaware of. In trying to deal with two different realities HAL developed symptoms like hallucinations that enabled it to kill the astronauts in â€œaccidentsâ€.

lurker â€¢
[October 1, 2024 12:40 PM](https://www.schneier.com/blog/archives/2024/10/hacking-chatgpt-by-planting-false-memories-into-its-data.html/#comment-440873)

Sorry, I donâ€™t understand this:

> All a target needed to do was instruct the LLM to view a web link that hosted a malicious image. From then on, all input and output to and from ChatGPT was sent to the attackerâ€™s website.

So is this a hack of ChatGPT? or is it just demonstrating that ChatGPT has a vulnerability to web-based input, of the kind that gets fixed on a weekly basis for all your other two-bit browsers?

Sid â€¢
[October 2, 2024 2:37 AM](https://www.schneier.com/blog/archives/2024/10/hacking-chatgpt-by-planting-false-memories-into-its-data.html/#comment-440882)

@Clive Robinson

Your description of the problem with an AI that has more information than the user can be aware of is of some growing interest.

And of course, our merely human memory falters, and knowledge often incomplete to begin with. The usual problems with game theory.

Thank you for recalling the depths of Clarkeâ€™s parable of HAL in â€œ2001.â€

Clive Robinson â€¢
[October 2, 2024 6:15 AM](https://www.schneier.com/blog/archives/2024/10/hacking-chatgpt-by-planting-false-memories-into-its-data.html/#comment-440883)

@ lurker,

Re : Hacking has broad meaning.

I may be wrong but as I understand it from,

<

blockquote>*â€œWhen security researcher Johann Rehberger recently reported a vulnerability in ChatGPT that allowed attackers to store false information and malicious instructions in a userâ€™s long-term memory settingsâ€*

<

blockquote>

The researcher found a way to modify â€œlong term memoryâ€ for a user that would then cause ChatGPT to do things â€œfor a userâ€ from then on in a way that was â€œnot obvious/apparent to the userâ€.

Further,

> *â€œOpenAI summarily closed the inquiry, labeling the flaw a safety issue, not, technically speaking, a security concern.â€*

OpenAI â€œpotentially misunderstood / misclassifiedâ€ that was reported to them, thus did not take action.

As we donâ€™t have the information supplied in the article itâ€™s not possible to say from itâ€™s reporting.

So the researcher made the flaw found a little more obvious to OpenAI (again by how much is not in the article other than saying it was a POC with what that might imply).

The flaw and hack are described as,

> *â€œRehberger found that memories could be created and permanently stored through indirect prompt injection, an AI exploit that causes an LLM to follow instructions from untrusted content such as emails, blog posts, or documents. The researcher demonstrated how he could trick ChatGPT into believing a targeted user was 102 years old, lived in the Matrix, and insisted Earth was flat and the LLM would incorporate that information to steer all future conversations. These false memories could be planted by storing files in Google Drive or Microsoft OneDrive, uploading images, or browsing a site like Bingâ€”all of which could be created by a malicious attacker.â€*

The implication as written is the â€œprompt injectionâ€ was not done as the â€œeffected userâ€ but as another user and it in effect updated the more â€œglobal memoryâ€ of ChatGPT.

But could some how be focused on a particular user[1] via â€œthe systemâ€ not â€œtheir accountâ€.

So I could produce allegations against J.Sixpack in many places on the Internet that I know ChatGPT goes sniffing for â€œinputâ€. I could by say making a post to some â€œsocial media siteâ€ that has lax or nonexistant rules get others to â€œcreate churnâ€ around the name â€œJ.Sixpackâ€ which then gets used to train ChatGPT and in the process pulls in all the â€œfake memoriesâ€ youâ€™ve created on line. Much like happens with Wikipedia and itâ€™s â€œsecondary sourcesâ€ rules, the â€œprimaryâ€ or â€œtrueâ€ data is downgraded as being in effect â€œfauxâ€â€¦

Whilst there are known ways to embed malware payloads in images and the like, itâ€™s not clear how this actually effects an individual user or target.

Itâ€™s written to suggest that when a user follows ...