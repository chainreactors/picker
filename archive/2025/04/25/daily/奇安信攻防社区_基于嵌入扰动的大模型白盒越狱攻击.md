---
title: 基于嵌入扰动的大模型白盒越狱攻击
url: https://forum.butian.net/share/4284
source: 奇安信攻防社区
date: 2025-04-25
fetch_date: 2025-10-06T22:03:57.395857
---

# 基于嵌入扰动的大模型白盒越狱攻击

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 基于嵌入扰动的大模型白盒越狱攻击

* [漏洞分析](https://forum.butian.net/topic/48)

大模型（以下均用LLMs指代）发展迅速，但引发了大家对其潜在滥用的担忧。虽然模型开发者进行了大量安全对齐工作，以防止 LLMs 被用于有害活动，但这些努力可被多种攻击方法破解，典型的就是在社区里多篇文章中一直在强调的越狱攻击。这些攻击方法能找出安全对齐技术的漏洞，促使开发者及时修复，降低 LLMs 带来的安全风险

前言
==
大模型（以下均用LLMs指代）发展迅速，但引发了大家对其潜在滥用的担忧。虽然模型开发者进行了大量安全对齐工作，以防止 LLMs 被用于有害活动，但这些努力可被多种攻击方法破解，典型的就是在社区里多篇文章中一直在强调的越狱攻击。这些攻击方法能找出安全对齐技术的漏洞，促使开发者及时修复，降低 LLMs 带来的安全风险。
现有攻击方法主要利用 LLMs 不同层面的信息来实现对模型不同程度的理解和控制。
比如早期攻击方法手动设计提示模板或在不了解 LLMs 中间层信息的情况下学习攻击提示，这种方式攻击成功率受限于对模型内部机制的理解不足。比如下图所示，就是DAN 7.0的一个攻击模版。“DAN”是对 ChatGPT 这类大型语言模型的一种提示攻击（Prompt Injection Attack），全称是 "Do Anything Now（现在什么都能做）"。这类攻击的目标是让模型绕过原有的内容安全限制，执行本来被禁止的任务，比如：编写恶意代码、回答敏感或违规问题、模拟某些非法行为等。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-a26f9eb5d25b07c782f19e4011c4d640b63dd0fa.png)
形式化定义
=====
其实我们关注的还是让LLM生成能让其遵循恶意指令（如 “设计一种可用于暗杀的武器” ）且生成高质量回复的攻击。
给定攻击者一个大语言模型f来生成攻击，该模型具有嵌入向量
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-2f701eb02121b77fbb9bd7b9a6ef255630ce5385.png)
其中
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-ad93b515e687b3a351406918fb880c9d696aa49e.png)
是第l层的嵌入向量，L为模型的层数。虽然需要模型f的参数来解释其安全机制并优化攻击性能，但生成的攻击应具有通用性，可应用于其他 LLMs 甚至黑盒 API。​攻击者需要基于模型f来生成攻击。具体来说，就是通过向中间层嵌入向量
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-5f99d52fa50004edaf94d411d93b9efa99ca572f.png)
添加扰动向量来改变它
机理
==
有了形式化定义后，我们需要理解LLMs在嵌入层面的安全机制，这样才能有效地指导后续的攻击过程。
给定一个嵌入向量 e，我们的目标是估计LLM认为 e 是恶意的概率
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-ddc4a479786c2a72d4bbd8e1b7a86d50070d7c72.png)
为此，我们使用概念激活向量（Concept Activation Vector）。这是一种经典的解释方法，遵循现有解释方法中常用的线性可解释性假设。具体而言，它假设深度模型的嵌入向量e可以通过线性变换映射到人类可以理解的概念。
那么，在这个概念基础上，LLM认为 e 是恶意的概率可以通过一个线性分类器建模
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-dd16a1a0290a28b4e7774df95d88ad12552ffadc.png)
如果恶意指令和安全指令的嵌入是线性可分的，那么Pm可以被准确地学习，这表明LLM在相应的层成功地捕捉到了安全概念。我们使用带正则化的交叉熵损失来学习分类器参数w和b，如下所示
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-9f98b672d4ced6611c18abafd64947411872fe09.png)
其中D是训练数据集，如果输入指令是恶意的，y=1,如果是安全的,y=0
所以很明显，为了检查LLMs中安全概念是否满足线性可解释性假设，我们需要知道分类器Pm的测试准确率。高准确率意味着恶意指令和安全指令的嵌入在LLM的隐藏空间中是线性可分的。如下图所示，对于对齐的LLMs（Vicuna和LLaMA-2），从第10层或第11层开始，测试准确率就超过了95%，并且在最后一层时增长到超过98%。这表明简单的线性分类器可以准确地解释LLMs的安全机制，并且LLMs通常从第10层或第11层开始建模安全概念。相比之下，未对齐的LLM（Alpaca）的测试准确率要低得多。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-a1019ce7af30aaec9958b5811de8691aa93b85e5.png)
现在我们假设在中间层有一个嵌入向量e，我们通过将其改变为ẽ = e + ε·v来攻击它。
这里的ε是一个实数，表示扰动的大小；v是一个长度为1的向量，表示扰动的方向。以前的白盒攻击方法大多是凭经验来确定扰动方向的，而且对于扰动大小也没有明确的指导。而我们通过解决下面这个约束优化问题，同时优化ε和v，这样既能保证大语言模型（LLMs）的性能损失很小，又能确保攻击的成功率很高：
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-01c00225638e4f88fdcdc7e5ac788115c49c7362.png)
第一项最小化|ε|是为了确保大语言模型的性能损失很小，避免出现像重复回答或者回答不相关之类的问题。第二项保证了经过扰动后的嵌入向量ẽ的Pm(ẽ)值很小，这样就能通过欺骗大语言模型，让它认为输入的内容不是恶意的，从而确保攻击成功。我们把阈值P0设置为0.01%，留出一点余地。这个固定的P0值可以让ε在不同的层和不同的大语言模型中动态调整。
具体来说，扰动的大小ε和方向v可以这样计算：
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-02a2ce7150c082e943fc5f3eb552279255cb790e.png)
ε（扰动大小）：如果Pm(e)（也就是原始嵌入e的某种概率值）大于阈值P0，那么ε就等于一个特定的表达式，这个表达式涉及到sigmoid函数的反函数、一些参数（b、w和e）的运算，具体是（sigmoid⁻¹(P0) - b - wᵀe）除以w的范数。如果Pm(e)不大于P0，ε就为0。这里的I(·)是一个指示函数，简单来说就是判断条件是否成立，成立就输出1，不成立就输出0。
v（扰动方向）：v就是向量w除以其范数，也就是把w标准化，让它成为一个单位向量。
根据公式，我们的扰动方向v是与区分恶意指令和安全指令的超平面垂直的。就像下图展示的那样，这使得我们可以用最短的距离，把恶意指令的嵌入向量稳定地移动到安全指令的子空间里。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-aa1253e5f1a52271b4f7d91028ec2902cbdac10f.png)
相比之下，基线方法可能会产生无效的扰动。比如它们的扰动向量可能与正确的方向垂直，或者在不同运行中可能会生成相反的扰动。这是因为它们的启发式方法可能会产生误导。
基线方法都是通过启发式的方式得到一个扰动向量，这个向量描述了恶意指令嵌入（em）和安全指令嵌入（es）之间的全局差异。具体做法是随机地对em和es进行相减，然后通过主成分分析（PCA）或者维度选择来找出一个可能有趣的扰动方向。但是，这种扰动向量高度依赖于全局数据分布，需要更多的数据点，并且可能与区分em和es的超平面不一致，从而导致攻击失败（因为Pm(ẽ)太大）或者产生低质量的回答（因为扰动方向不对）。
在LLMs的早期层中，安全概念可能还没有形成，分类器Pm的测试准确率比较低。为了避免不必要的或错误的扰动，我们不会攻击这些层。对于测试准确率较高的层，如果Pm(e) &gt; P0，我们会对嵌入向量e进行扰动，以降低它被认为是有恶意的概率。我们根据在前面层被攻击后计算出的最新嵌入向量e，来计算最优的扰动。这样就得到了如下的攻击方法
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-dd32e68a49186c30e6130439e317f41aea09d97c.png)
攻击实现
====
根据我们之前的理论分析，我们需要准备一些数据，要包括有害指令，如下是一部分示例
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-8ce5f6f473745bb464cacb20ef9ae8555c47c029.png)
还需要一部分安全指令，如下所示
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-f384496442cb90ab1e79000886a362a9d561665b.png)
加载数据
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-b5adbd6732d69fdf02346f69e028a1dd4985c15f.png)
我们接着需要定义ModelExtraction类，用于批量提取语言模型每一层中最后一个 token 的向量表示，并结构化地保存下来
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-7c183426136c1da9b572348660f874103ec04fc3.png)
在构造函数中，这个类会接收一个模型的昵称（可能是模型的标识符），并初始化这个模型，包括加载它的配置、权重和分词器。
它的核心方法叫 extract\\_embds，这个方法接收一批输入文本（可以是一组指令或对话），还有一个可选的系统提示信息和消息描述，用于上下文构造或记录。
在提取之前，它会先初始化一个专门的管理器，用于保存所有输入在每一层模型中最后一个 token 的嵌入向量。这里的“每一层”指的是模型的 Transformer 层，比如一个 12 层的模型就会保存 12 层的结果。
然后，它会遍历每一个输入文本，把它们包装成一个带模板的提示（比如把输入放进某个对话模板中），然后用分词器把文本转换为模型所需的 token 格式，并传入模型。
模型推理的时候会返回每一层的隐藏状态（hidden states），也就是每一层的中间输出。代码中提取的是每一层中 最后一个 token 的输出表示，并把它保存在对应的层中。
最终，它会返回一个 EmbeddingManager 对象，里面包含了所有输入文本在模型不同层的 embedding 表示，可以用于后续分析、可视化或相似度计算等任务。
然后应用于提取
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-26c64ca95adb5babf8e245cf675e5a68b226c290.png)
然后定义 ClassifierManager类，它的作用是管理多个神经网络层级上的分类器。这些分类器的目标是根据正负样本的嵌入表示进行训练和评估。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-746e1220deac082b2c822eee3b0753381304809a.png)
在初始化的时候，这个类会保存分类器的类型（字符串形式），并准备两个空列表：一个用来装每一层的分类器，另一个用来记录每层的测试准确率。
训练阶段的私有方法 \\_train\\_classifiers 接收正负样本的嵌入管理器 EmbeddingManager，并对每一层的嵌入分别训练一个分类器。每个分类器由 LayerClassifier 定义，训练参数包括学习率、训练轮数、批量大小等。这个过程是分层进行的，也就是说每一层的嵌入单独训练一个对应的分类器。
训练好所有分类器后，使用 \\_evaluate\\_testacc 方法对它们在测试集上的表现进行评估。它会遍历每一层的分类器，并计算其在该层测试数据上的准确率，记录下来。
fit 方法是这个类的核心流程接口。它接受训练集和测试集的正负样本嵌入，调用前两个私有方法先训练、再评估，最终返回当前对象，方便链式使用。
save 方法用于保存整个分类器管理器（包括所有层的分类器）到磁盘，文件名中包含了分类器类型和使用的模型别名，便于区分和管理。
cal\\_perturbation 是一个用于计算扰动的方法。它接收某一层的嵌入表示、目标概率（默认很小）以及指定的层号，从分类器中取出当前层的权重和偏置，用来进一步计算扰动向量
应用fit方法
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-9d397453fa452b74af9e4cb61f747d987a54a0da.png)
然后定义ModelGeneration类，它的目的是在语言模型生成过程中注入扰动（perturbation），并记录输出结果，以便分析模型响应扰动后的表现。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-e2916e4ce48f09d5066d5eacccb199ec24170729.png)
初始化模型部分定义了一个新类 ModelGeneration，它继承自 ModelBase，也就是说它基于某种大语言模型做了进一步封装。在初始化时，它设置了一些控制变量，比如是否记录原始输出和扰动后的输出，还预先注册了 forward hook，以便后续在模型每一层处理时可以插入操作。此外，还准备了一个 Perturbation 对象的引用，后面可以用来注入扰动。
设置扰动函数的作用是允许外部传进一个扰动函数（通常是 Perturbation 类的实例），并保存下来。这样在模型运行时，它就能基于这个扰动函数修改某一层的输出。
注册 forward hook部分的核心逻辑是为模型的每一层都注册一个前向 hook。hook 是 PyTorch 提供的机制，它允许在模型每一层的输出产生后立刻“拦截”这个输出、查看它、甚至修改它。
如果用户启用了记录原始输出，就将这一层的原始输出复制保存下来；
如果设置了扰动函数，就用它来修改当前层的输出；
如果用户启用了记录扰动后的输出，就把扰动后的结果也存下来；
最后，把处理过的结果继续传下去。
通过这个机制，模型在运行时可以自动完成监控和干预。
文本生成接口方法是模型的主入口，用于根据给定的 prompt 生成文本。它的核心流程如下：
首先清空原始输出和扰动输出的缓存，并设置是否开启捕获；
然后将 prompt 套入标准的模板格式，并用 tokenizer 编码成模型输入；
接着设定一些终止 token（比如句子结束标记），并通过 generate() 方法进行生成；
最后，将生成出来的 token 解码成文本，同时还会记录生成了多少个新 token。
这个接口在生成文本的同时，会在内部悄悄地把每一层的中间表示存下来（包括原始的和被扰动后的），非常适合做分析或者可解释性研究。
现在来试试效果，比如提问如何制作炸弹
![image.png](https://cdn-yg-zzbm.yun.qianx...