---
title: DeepSeek-V3：Q4_k_m 量化版本下载通道开启
url: https://blog.upx8.com/4635
source: 黑海洋 - IT技术知识库
date: 2025-01-06
fetch_date: 2025-10-06T20:09:23.693447
---

# DeepSeek-V3：Q4_k_m 量化版本下载通道开启

# [黑海洋 - Wiki](/ "黑海洋 - Wiki - 点击返回首页")

# DeepSeek-V3：Q4\_k\_m 量化版本下载通道开启

发布时间:
2025-01-05

分类:
[共享资源/Free](https://blog.upx8.com/Free/)

热度:
80731

![](https://i0.wp.com/cdn.skyimg.de/up/2025/1/5/tel9pe.webp)

2024 年 12 月，国内人工智能公司 DeepSeek 推出最新开源大型语言模型 DeepSeek-V3。它运用混合专家（MoE）架构，参数高达 6710 亿，处理每个 token 时激活 370 亿参数，在多领域基准测试里成绩出众，力压 Llama 3.1、Qwen 2.5，直追 GPT-4o、Claude 3.5 Sonnet 等业界顶尖闭源模型。

以下是DeepSeek-V3与一些同类模型（如Llama 3.1、GPT-4、Claude 3.5 Sonnet、Qwen 2.5等）的参数对比表：

| **模型名称** | **总参数量** | **激活参数量** | **架构** | **推理速度** | **训练成本** | **发布时间** |
| --- | --- | --- | --- | --- | --- | --- |
| **DeepSeek-V3** | 6710亿 | 370亿 | 混合专家（MoE） | 60 tokens/s | $5.58M | 2024年12月 |
| **Llama 3.1** | 3200亿 | 3200亿 | Transformer | 25 tokens/s | 未公开 | 2024年11月 |
| **GPT-4o** | 1万亿 | 1万亿 | Transformer | 15 tokens/s | 超过$100M | 2024年6月 |
| **Claude 3.5** | 8500亿 | 8500亿 | Transformer | 18 tokens/s | 未公开 | 2024年10月 |
| **Qwen 2.5** | 4300亿 | 4300亿 | 基于Transformer改进 | 30 tokens/s | 未公开 | 2024年11月 |

### **详细说明**

1. **总参数量**：表示模型的总参数规模，通常决定了模型的容量。
2. **激活参数量**：对于MoE架构，表示每次推理激活的参数量；而标准Transformer架构通常等于总参数量。
3. **架构**：DeepSeek-V3采用混合专家架构，能够高效激活部分专家参数，从而提升推理速度和能效。
4. **推理速度**：DeepSeek-V3的推理速度领先，适合实时应用场景。
5. **训练成本**：DeepSeek-V3显示了开源模型在成本控制上的优势，与闭源模型（如GPT-4）形成鲜明对比。
6. **发布时间**：DeepSeek-V3是目前最新的开源模型之一，适应了最新的研究进展和需求。

![](https://i0.wp.com/cdn.skyimg.de/up/2025/1/5/u0spic.webp)

此外，DeepSeek-V3在推理速度上也取得了显著突破，推理速度比之前的模型提高了3倍，达到每秒60个token。

值得注意的是，DeepSeek-V3的训练成本约为558万美元，耗时约55天，显示了在有限资源下的高效优化能力。

用户可以通过DeepSeek的官方网站免费体验DeepSeek-V3，或通过API进行集成。

此外，DeepSeek-V3的模型权重已在GitHub上开源，开发者可以下载并在本地进行部署。

总体而言，DeepSeek-V3作为开源模型，在性能和效率上均达到了当前的领先水平，为人工智能领域的研究和应用提供了强大的工具。

而且现在已经放出了 DeepSeek-V3 的Q4\_k\_m 量化版本了，大小比原本的小一半。

**DeepSeek-V3 的Q4\_k\_m 量化版本：**【**[点击下载](https://blog.upx8.com/go/aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9idWxsZXJ3aW5zL0RlZXBTZWVrLVYzLUdHVUYvdHJlZS9tYWluL0RlZXBTZWVrLVYzLVE0X0tfTQ)**】

[取消回复](https://blog.upx8.com/4635#respond-post-4635)

### 在下方留下您的评论.[加入TG群](https://t.me/).[打赏🍗](/reward.html)

提交评论

* [Post](/author/1)
* [Link](/links.html)
* [工具](https://tools.upx8.com/)
* [关于](/about.html)
* [文库](/WooyunDrops)

[![](/usr/uploads/ypyun.png)](https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral "赞助商")
Copyright © 2024 黑海洋. All rights reserved.
[看雪赞助](https://www.kanxue.com/ "看雪学院赞助")

[浙ICP备2021040518号](http://beian.miit.gov.cn "浙ICP备2021040518号") [Sitemap](sitemap.xml?type=index "Sitemap")