---
title: DeepSeekR1与R1-Zero的训练过程及与普通语言模型的区别
url: https://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247506543&idx=1&sn=f00a79a9e8890cbbd07845eb972045fb&chksm=97e96785a09eee93582593c0cdb14e1433237712de9228b128de329de05604bcf79182435971&scene=58&subscene=0#rd
source: 网安寻路人
date: 2025-02-27
fetch_date: 2025-10-06T21:55:40.843879
---

# DeepSeekR1与R1-Zero的训练过程及与普通语言模型的区别

![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/jErr674f9m9JC2whEsSiaFJD31VUXYPfrJZicoP1aNFmZgaje8vQmnO9jYlBWcXlcHEJbpjrGBpUwOsz1Sd1WKnQ/0?wx_fmt=jpeg)

# DeepSeekR1与R1-Zero的训练过程及与普通语言模型的区别

原创

洪延青

网安寻路人

**编者按**

关于DeepSeek的系列读书笔记：

1. [DeepSeek-R1展示了小模型也能成为推理专家](https://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247506426&idx=1&sn=b8eae0703ee904b21809cbae29edd137&scene=21#wechat_redirect)
2. [DeepSeek的R1与R1-Zero：技术差异与AI风险管控](https://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247506432&idx=1&sn=ddb2b818c020739b2b004f3cfcb69a91&scene=21#wechat_redirect)

今天和大家分享公号君对推理模型和一般语言模型特性区别的粗浅认识。

近年来，大型语言模型（LLM）的一个重要新方向是“推理模型”，它与传统的一般LLM在架构设计、能力表现和应用场景等方面都有明显区别。本报告将深入探讨两者的核心差异，并分析推理模型的训练方法、推理策略、代表性案例以及未来的发展趋势。

## 推理模型 vs. 一般大语言模型

##

推理模型是一种特殊的LLM，其解决问题的方式与常规的大语言模型不同。主要区别体现在以下几个方面：

* **架构设计**：两者通常都基于Transformer等大型模型架构，但推理模型在设计上强调长链式的思考过程。一些推理模型可能引入额外的模块或机制（如奖励模型、验证器）辅助决策，而一般LLM主要是单一模型直接输出结果。在本质网络结构上差异不大，但推理模型会通过特殊训练使模型学会“思考”的步骤。
* **推理方式**：一般LLM往往收到问题后**直接生成最终答案**，而推理模型会**先产生一系列中间推理步骤（思维链）**，再给出答案。也就是说，推理模型在回答前会“停下来想一想”，将问题分解为多个子步骤，逐步推理得出结论。这种链式推理让模型能够处理更加复杂的问题和逻辑关系。
* **能力表现**：借助链式推理，推理模型在复杂任务上表现出色，尤其是数学、逻辑推理、编程等需要多步推导的场景。例如，OpenAI 的 o1 推理模型在国际数学奥林匹克竞赛题（IMO）中取得了83%的正确率，而常规GPT-4系列模型仅约13%。这表明推理模型通过“深度思考”显著提升了复杂问题求解能力。在代码理解与生成等任务上，推理模型同样表现更强。而一般LLM在常识问答、开放聊天、知识检索等直接输出答案的任务上表现良好，但在复杂推理题上往往力不从心。
* **应用场景**：推理模型适用于需要复杂推演、多步骤计算的专业领域。例如科学研究中的推导证明、数学问题求解、代码调试和生成等。o1模型就被用于帮助物理学家生成复杂公式、辅助程序员执行多步工作流等。相对而言，一般大语言模型更适合日常对话、知识问答、内容创作等不需要长链推理的场景。因此，两者在定位上有所不同：推理模型定位为“复杂问题求解者”，而一般LLM定位为“通用对话和知识助手”。
* **性能代价**：推理模型为了获得更强的推理能力，往往需要付出计算和时间成本。由于在回答问题时会生成较长的思维链，推理模型一次回答可能比普通LLM产生更多的Token，响应速度变慢。同时训练推理模型也更复杂，常需要强化学习等额外步骤（见下文），训练开销更大。一般LLM则以较快的推断速度直接给出答案。在实际应用中，需要在性能和准确率之间权衡：面对简单任务时，一般LLM即可胜任且速度更快；但遇到复杂推理任务时，推理模型虽慢但能给出正确答案。

综上，推理模型可以看作在标准大模型基础上，通过引入“思考”过程而形成的新型LLM范式。它和一般LLM互有侧重：一个追求深度推理能力，另一个追求广泛知识和响应效率。

## 推理模型的训练方法

##

要让一个大型语言模型具备强大的推理能力，训练方法上有所创新和突破。传统LLM主要经过“预训练 + 有监督微调(SFT) + 人类反馈强化学习(RLHF)”等步骤，而推理模型在此基础上加入了强化学习等特殊策略。下面介绍几种关键的训练方法及其对性能的影响（此部分主要总结OpenAI和DeepSeek团队披露的训练方法）：

* **有监督微调 (Supervised Fine-Tuning, SFT)**：这是对预训练模型进行指令微调的常规步骤，通过人工标注的问答对或思维链数据来教会模型格式化输出和遵循指令。对于推理模型，SFT通常提供基础的思维链示例或格式，使模型初步掌握链式推理的样式。例如DeepSeek-R1 的训练就包含了一个“冷启动”SFT阶段，以提供格式模板，让模型输出的推理过程更有结构、可读。SFT能快速赋予模型基本的推理框架，但单靠SFT难以训练出深入复杂的推理能力。
* **人类反馈强化学习 (RLHF)**：RLHF通过人类偏好来优化模型回答，使其更符合人类期望。这在一般LLM对齐中常用，但对于推理能力的提升作用有限。RLHF偏重于提升回答的有用和无害程度，未必针对逻辑正确性。事实上，过度依赖RLHF可能导致模型为了迎合人类偏好而牺牲客观准确性，比如生成听起来合理但不严谨的推理。因此，推理模型需要**更直接的强化学习信号**来真正提高推理正确率。
* **可验证奖励的强化学习 (Reinforcement Learning from Verifiable Rewards, RLVR)**：这是近年来出现的一种**新型强化学习训练方法**，专门用于具有客观可验证结果的任务。RLVR的核心思想是：用**程序化的验证函数**替代人工偏好模型，直接以**答案是否正确**作为奖励信号来训练模型。例如在数学题、编程题这类有标准答案的任务上，如果模型解答正确就给予正奖励，错误则给负奖励。这样模型通过大量尝试，从反馈中学会逐步逼近正确答案。Allen AI研究所的Tülu-3模型采用了RLVR来强化数学问题求解和精确指令遵循能力，结果在GSM8K等基准上取得了定向提升，同时不影响其他任务性能。RLVR可以被视为一种更简单直接的强化学习形式，用**答案匹配或约束验证**作为二元信号来训练模型。这减少了训练中对复杂奖励模型的依赖，让模型在可自动判定对错的任务上学习更高的准确率。
* **纯强化学习训练**：一些推理模型甚至**完全摒弃监督学习阶段，直接采用强化学习来训练**。代表就是 DeepSeek-R1-Zero 模型——它在预训练基座模型上**跳过了任何SFT微调，直接进行大规模纯强化学习训练**。训练过程中不给模型示范解题范例，仅通过“答对得分、答错扣分”的试错机制引导模型自我学习。这类似让一个学生不看例题直接考试，不断根据对错反馈来自学。研发团队为此设计了高效的**群组相对策略优化 (GRPO)** 算法（PPO的改进），用组得分估计基线、无需单独训练价值模型，大幅降低了大规模RL的计算成本。令人惊讶的是，经过数千步的强化训练，R1-Zero **自然涌现出了强大的复杂推理能力**，仿佛“自我觉醒”了一般。这证明了纯RL范式在大型模型上的潜力：即使没有人工示例，模型也能通过强化学习掌握复杂推理。不过纯RL训练也有风险，R1-Zero就曾出现输出思维链杂乱无章、语言混用等问题，说明缺乏监督约束的模型在可读性和稳定性上可能欠佳。因此完全不经过SFT的路径需要辅以其他策略改善模型行为（后续的DeepSeek-R1便结合了监督信号来纠正这些缺陷）。
* **自我监督提升**：除人类监督和环境奖励外，模型也可以通过**自我辅导**来提升推理能力。例如“自我教学推理 (Self-Taught Reasoner, STaR)”方法让模型自己产生带思维链的解答，并验证答案正确性，然后用那些正确的思维链数据继续微调模型。这相当于模型**生成自己的训练数据来训练自己**。实践表明，这种循环自我改进能够提高模型的逻辑推理和解释能力。OpenAI的o1模型背后据传就应用了类似STaR的技术，通过模型自我生成解题过程并学习，从而强化推理技能。自我监督的方法降低了对人工标注思维链的依赖，使模型的推理能力可以在训练过程中不断自我迭代完善。

此外，还有**知识蒸馏**等辅助训练手段值得一提。大型高性能的推理模型往往参数规模庞大，不便于部署，因而研究者会将其知识“蒸馏”到较小的模型中。通过蒸馏，小模型在推理任务上的表现可以接近大模型水平，从而实现性能与效率的平衡。例如，有研究将体量巨大的R1系列模型蒸馏出较小的32亿和70亿参数模型，发现它们在多项推理能力上已经能与OpenAI o1-mini相媲美。蒸馏技术有助于推理模型的实用化，使复杂推理能力不再专属巨无霸模型。

综上，推理模型的训练常采用**多阶段、多策略结合**：先经预训练和必要的监督微调奠定基础，然后通过强化学习（无论RLHF、RLVR还是纯RL）大幅提升推理深度，最后可能加以自我训练和蒸馏优化模型。强化学习在其中扮演关键角色，赋予模型反复试错“思考”的能力，也由此带来了推理能力的飞跃。

附：对DeepSeek R1 和 DeepSeek R1-Zero训练步骤的整理

### DeepSeek R1-Zero 的训练步骤

###

DeepSeek R1-Zero 是一个完全通过强化学习（RL）训练的推理模型，其独特之处在于不依赖传统的监督微调（SFT）阶段。以下是其详细的训练步骤：

#### **步骤 1：选择基础模型**

**基础模型：DeepSeek R1-Zero 以 DeepSeek-v3 作为起点。DeepSeek-v3 是一个拥有 6710 亿参数的混合专家（MoE）模型，具有卓越的性能和效率。它通过多头注意力机制（MLA）、优化 MoE 结构和多标记预测目标等技术，在预训练阶段展现了强大的语言能力。**

#### **步骤 2：强化学习训练**

方法：采用 Group Relative Policy Optimization（GRPO）算法进行强化学习。GRPO 是一种相对简单的 RL 算法，相较传统的 PPO（Proximal Policy Optimization），它减少了训练成本并消除了对批评模型（Critic Model）的需求。

* **奖励信号**

+ **准确性奖励：评估模型输出是否正确。对于数学问题，通过字符串匹配验证最终答案；对于编码问题，执行生成的代码并检查是否通过预定义测试用例。**

+ **格式奖励：强制模型输出遵循特定格式，例如将思考过程置于 <think> 和 </think> 标签之间，将答案置于 <answer> 和 </answer> 标签之间。**

* **训练数据：专注于可自动验证的任务，如数学和编码问题，避免使用神经奖励模型，以防止奖励操控（Reward Hacking）并简化训练流程。**

**步骤 3：模型自进化**

**训练过程：通过大规模 RL 训练，模型自然学会生成更长的思维链（Chain of Thought, CoT），以解决复杂推理问题。训练过程中，模型逐步探索不同的推理策略，例如问题分解、自我反思和替代方案探索，这些行为并非人为设定，而是通过 RL 自发涌现。**

**动态调整：随着训练的推进，模型学会利用更多的推理时间（即生成更长的 CoT），从而提升解决复杂问题的能力。**

#### **步骤 4：性能验证**

#### **结果：DeepSeek R1-Zero 在 AIME 2024 等数学基准测试中表现出色，从初始的 15.6% 准确率提升至 71.0%，若结合 16 次并行输出的多数投票机制，准确率可达 86.7%。这表明其推理能力与 OpenAI 的 o1-preview 相当，尤其在数学任务上表现优异，尽管在编码任务上稍显不足。**

####

### DeepSeek R1 的训练步骤

###

DeepSeekR1在R1-Zero的基础上进一步优化，通过多阶段训练流程，不仅强化了推理能力，还提升了通用任务的表现和输出可读性。其训练分为四个阶段：

#### **步骤 1：阶段一 - 冷启动（推理导向的 SFT）**

**目标：为后续 RL 训练提供更好的探索起点，避免 RL 初期的不稳定性。**

* **数据收集：**

+ 使用 DeepSeek-v3 生成长 CoT 数据，通过少样本提示或指令要求模型生成详细的推理过程。

+ 利用 R1-Zero 生成大量长 CoT 输出，随后由人类筛选和后处理，挑选最佳样本。

* **训练：对 DeepSeek-v3 进行监督微调，使用包含数千个长 CoT 示例的小型数据集。数据中加入了推理过程总结，教导模型在回答前总结推理轨迹。**
* **作用：通过 SFT，模型初步掌握推理模板和结构，为 RL 阶段奠定基础。**

#### **步骤 2：阶段二 - 推理导向的 RL**

* **方法：重复 R1-Zero 的大规模 RL 训练流程，使用 GRPO 算法增强模型的推理能力。**
* **奖励信号：**

+ 与 R1-Zero 相同的准确性奖励和格式奖励。

+ 新增语言一致性奖励，计算输出中目标语言的比例，以提升输出的流畅性和可读性（尽管这略微降低了推理能力）。

结果：模型进一步优化了处理推理密集型任务的能力，同时提高了人类可接受度。

#### **步骤 3：阶段三 - 拒绝采样**

* **目标：收集多样化的 SFT 数据集，平衡推理和通用能力。**
* **推理数据收集：**

+ 策划多样化的推理提示。

+ 使用阶段二的模型生成候选推理轨迹。

+ 通过拒绝采样（Rejection Sampling），基于质量和正确性筛选最佳轨迹，最终获得 60 万条推理数据。

* **非推理数据收集：**

+ 重用 DeepSeek-v3 的 SFT 数据集。

+ 对于复杂非推理任务，提示 DeepSeek-v3 生成潜在 CoT，最终收集 20 万条非推理数据，总计 80 万条 SFT 数据。

**作用：通过混合数据，模型在推理任务外扩展了适用范围。**

####

#### **步骤 4：阶段四 - 通用 RLHF**

方法：结合推理和通用数据进行 RL 训练。

奖励信号：

+ 推理问题：使用规则-based 奖励（如准确性和格式奖励）。

+ 通用数据：使用神经奖励模型，基于人类偏好对训练，关注帮助性和无害性。

* **细节：**

+ 帮助性奖励仅评估最终答案（不包括 CoT）。

+ 无害性奖励考虑整个输出轨迹。

* **结果：DeepSeek R1 在推理任务上与 o1 相当或更优，同时在通用任务如写作和翻译上表现出色，成为一个兼具推理和通用能力的模型。**

数据保护官（DPO）社群主要成员是个人信息保护和数据安全一线工作者。他们主要来自于国内头部的互联网公司、安全公司、律所、会计师事务所、高校、研究机构等。在从事本职工作的同时，DPO社群成员还放眼全球思考数据安全和隐私保护的最新动态、进展、趋势。2018年5月，DPO社群举行了第一次线下沙龙。沙龙每月一期，集中讨论不同的议题。目前DPO社群已超过400人。关于DPO社群和沙龙更多的情况如下：

DPO线下沙龙的实录见：

1. [数据保护官（DPO）沙龙第一期纪实](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247485304&idx=1&sn=1104896fe7262911cdf814d38f5a8ef6&chksm=97eaba92a09d33842bd45dd48349da1bf82c2ae66ab1daa49cf2ca6ce9d0519f67675540e77e&scene=21#wechat_redirect)
2. [第二期数据保护官沙龙纪实：个人信息安全影响评估指南](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247485350&idx=1&sn=2270d2ca1088625e8df21bf297962656&chksm=97eaba4ca09d335a44c595caa009c212492000e489627d9316a852069d112ee6bcd3f0b9e573&scene=21#wechat_redirect)
3. [第三期数据保护官沙龙纪实：数据出境安全评估](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247485566&idx=1&sn=a9ae1fe9993ec67f6d934cc38c951478&chksm=97eab594a09d3c82e0602bc822b9b36ee5c0775c0b8ce05594aa1859d3166cd47df925e0b29a&scene=21#wechat_redirect)
4. [第四期数据保护官沙龙纪实：网络爬虫的法律规制](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247485607&idx=1&sn=400e306ef2ce02e8fe982fd17c627634&chksm=97eab54da09d3c5b9eb9294524c22a6f1fad0ba57dc6de89cfc15d02e844b214781ca57c3164&scene=21#wechat_redirect)
5. [第四期数据保护官沙龙纪实之二：当爬虫遇上法律会有什么风险](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247485607&idx=1&sn=400e306ef2ce02e8fe982fd17c627634&chksm=97eab54da09d3c5b9eb9294524c22a6f1fad0ba57dc6de89cfc15d02e844b214781ca57c3164&scene=21#wechat_redirect)
6. [第五期数据保护官沙龙纪实：美国联邦隐私立法重要文件讨论](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247485797&idx=1&sn=5528fcc192cf4a7cec5e0165e16271f7&chksm=97eab48fa09d3d992b9eb00c16b0a79729a8b064719...