---
title: Databricks Platform Cluster Isolation Bypass exploit
url: https://sploitus.com/exploit?id=PACKETSTORM:172131&utm_source=rss&utm_medium=rss
source: Sploitus.com Exploits RSS Feed
date: 2023-05-04
fetch_date: 2025-10-04T11:39:20.193165
---

# Databricks Platform Cluster Isolation Bypass exploit

[x]
Dark Mode

[##### SPLOITUS](/)

# Exploit for Databricks Platform Cluster Isolation Bypass

2023-05-03 | CVSS 6.9

Copy
Download
Source
[Share](#share-url)

```
## https://sploitus.com/exploit?id=PACKETSTORM:172131
SEC Consult Vulnerability Lab Security Advisory < 20230502-0 >
=======================================================================
title: Bypassing cluster isolation through insecure defaults and
shared storage
product: Databricks Platform
vulnerable version: PaaS version as of 2023-01-26
fixed version: Current PaaS version
CVE number: -
impact: critical
homepage: https://www.databricks.com
found: 2023-01-20
by: Florian Roth (Atos)
Marius Bartholdy (SEC Office Berlin)
SEC Consult Vulnerability Lab

An integrated part of SEC Consult.
SEC Consult is part of Eviden, an atos business
Europe | Asia | North America

https://www.sec-consult.com

=======================================================================

Vendor description:
-------------------
"Databricks Data Science & Engineering (sometimes called simply "Workspace")
is an analytics platform based on Apache Spark. It is integrated with Azure to
provide one-click setup, streamlined workflows, and an interactive workspace
that enables collaboration between data engineers, data scientists, and
machine learning engineers."

Source: https://learn.microsoft.com/en-us/azure/databricks/scenarios/what-is-azure-databricks-ws

Business recommendation:
------------------------
The vendor disabled legacy scripts and migrated cluster-scoped scripts from
DBFS to WSFS. Affected customers received migration instructions.

SEC Consult highly recommends to perform a thorough security review of the
product conducted by security professionals to identify and resolve potential
further security issues.

We have also written a blog post in collaboration with Elia Florio, Sr. Director
of Detection & Response at Databricks and Florian Roth and Marius Bartholdy,
security researchers with SEC Consult. It can be found here:
https://r.sec-consult.com/databr

Furthermore, a proof of concept demo video has been published here (Youtube):
https://r.sec-consult.com/dbyoutube

Databricks concepts:
--------------------
Concept 1: Databricks File System (DBFS):

"The Databricks File System (DBFS) is a distributed file system mounted into a
Databricks workspace and available on Databricks clusters. DBFS is an
abstraction on top of scalable object storage that maps Unix-like filesystem
calls to native cloud storage API calls."

Source: https://docs.databricks.com/dbfs/index.html

Therefore developers can easily handle files as if they were local to a compute
cluster although they actually reside in a cloud storage.

The recommended way to interact with the DBFS is from within a notebook by using
the Databricks Utilities (dbutils). The following command could be used to list
the content of a directory:
===============================================================================
display(dbutils.fs.ls("dbfs:/databricks/scripts"))
===============================================================================

For further information see: https://learn.microsoft.com/en-us/azure/databricks/dbfs/

Concept 2: Init Scripts:

Databricks uses a feature called "init script" to customize compute clusters.
They can be used to install dependencies or to configure advanced network
settings. These are shell scripts that run during the startup of each cluster.

There are different types of init scripts:

(I) Cluster-scoped init scripts only run on the specified cluster and have to be
setup by the cluster owner. Before using a cluster-scoped script it has to be
uploaded to the DBFS. In the cluster configuration it is then referenced by its
file path, e.g dbfs:/databricks/scripts/init-health-check.sh

(II) Global init scripts run on every cluster and have to be configured by an
administrative user. Their storage location is not disclosed.

(III) Legacy global init scripts are theoretically deprecated. However, they are
enabled by default, even on newly created workspaces. The main difference to
the newer global init scripts is that they are stored on the DBFS in a fixed
location at dbfs:/databricks/init.

For further information see: https://learn.microsoft.com/en-us/azure/databricks/clusters/init-scripts

Vulnerability overview/description:
-----------------------------------
1) Bypassing cluster isolation through insecure defaults and shared storage

A low-privilege user is able to break the isolation between Databricks compute
clusters and take over any cluster in a workspace as long as they are allowed
to run notebooks. Due to an insecure default configuration combined with
insufficient access control, it is possible to gain remote code execution on all
clusters of a workspace. With such an access, it is possible to leak secrets and
to escalate privileges to those of a workspace administrator.

Attack scenario:
The DBFS is accessible by every user in a Databricks workspace. All files stored
here are visible to anyone in the workspace. Cluster-scoped and legacy global
init scripts are stored here.

An authenticated attacker with the lowest possible permissions in a Databricks
workspace could run a notebook to:

1. Find and modify an existing cluster-scoped init script.
2. Place a new script in the default location for legacy global init scripts.

Both attacks lead to the take over of the compute cluster resources and enable
further attacks. Firstly, any secrets stored can be read and, secondly,
workspace administrator tokens can be stolen as demonstrated by Joosua
Santasalo from Secureworks.

See: https://www.databricks.com/blog/2022/10/10/admin-isolation-shared-clusters.html

Proof of concept:
-----------------
1) Bypassing cluster isolation through insecure defaults and shared storage
a) Preparations:

For this POC a new Azure Databricks workspace was created with the "premium"
pricing tier. It includes an administrative user (databricks-workspace-admin)
as well as a newly added low-privileged user (databricks-user) with the default
permissions "Workspace access" and "Databricks SQL access". These are the fewest
possible permissions a user can have.

To demonstrate both attack scenarios, three clusters were created:

1. Cluster on which the databricks-user has permissions to run notebooks
("Can attach to")
2. Cluster for the databricks-workspace-admin with a cluster-scoped init script
already configured.
3. Cluster for the databricks-workspace-admin with NO init script

The databricks-user does not have access to the clusters 2 and 3.
They cannot even see them in the portal.

For the cluster 2 (with a pre-configured init script) the following notebook
code was used by the databricks-workspace-admin to create an init script which
simply writes example output to /tmp/init-health-check-success.txt:

===============================================================================
dbutils.fs.mkdirs("dbfs:/databricks/scripts/")
dbutils.fs.put("/databricks/scripts/init-health-check.sh","""
#!/bin/bash
echo 'Init health check: successful > /tmp/init-helth-check-success.txt' """, True)
display(dbutils.fs.ls("dbfs:/databricks/scripts/init-health-check.sh"))
===============================================================================

After that the script was applied to cluster 2 as a cluster-scoped init script.

To show the impact of this attack in a more tangible way a keyvault-backed
secret scope as well as a databricks-backed secret scope were also created.
Their secrets were then used in the spark configuration and in the environment
variables of cluster 2 and 3.

===============================================================================
Spark configuration:
databricks-backed-secret {{secrets/databricks-backed-secret-scope/databricks-backed-secret}}
azure-keyvault-backed-secret {{secrets/key-vault-backed-secret-scope/azure-keyvault-backed-secret}}

Environment variables:
databricks_backed_secret_in_environment={{secrets...