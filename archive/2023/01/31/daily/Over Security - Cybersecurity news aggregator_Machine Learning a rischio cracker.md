---
title: Machine Learning a rischio cracker
url: https://hackerjournal.it/11245/machine-learning-a-rischio-cracker/
source: Over Security - Cybersecurity news aggregator
date: 2023-01-31
fetch_date: 2025-10-04T05:15:29.427990
---

# Machine Learning a rischio cracker

[![Hackerjournal.it](https://hackerjournal.it/wp-content/uploads/2017/12/hjnegw-1.png)](https://hackerjournal.it/)

* [Forum](https://hackerjournal.it/forum/)
* [News](https://hackerjournal.it/category/news/)
* [Tech](https://hackerjournal.it/category/tecno/)
* [Articoli](https://hackerjournal.it/category/tech/)
* [Trending](https://hackerjournal.it/trending/)
* [Accademia](https://hackerjournal.it/corsi/)
* [Contest](https://hackerjournal.it/contest/)
* [Glossario](https://hackerjournal.it/encyclopedia/)
* [Abbonati](https://hackerjournal.it/81/abbonati-ad-hacker-journal/)
* [Arretrati](https://hackerjournal.it/arretrati-hackerjournal/)

Connect with us

[![Hackerjournal.it](https://hackerjournal.it/wp-content/uploads/2017/12/hjnew-1.png)](https://hackerjournal.it/)
[![Hackerjournal.it](https://hackerjournal.it/wp-content/uploads/2017/12/hjnegw-1.png)](https://hackerjournal.it/)

## Hackerjournal.it

#### Machine Learning a rischio cracker

* [Forum](https://hackerjournal.it/forum/)
* [News](https://hackerjournal.it/category/news/)

  + [![](https://hackerjournal.it/wp-content/uploads/2025/09/coppadelmondo-400x240.png)

    Mondiali 2026: la truffa corre sul Web](https://hackerjournal.it/14527/mondiali-2026-la-truffa-corre-sul-web/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/09/massive_npm-400x240.png)

    Il virus che “ruba” il codice](https://hackerjournal.it/14522/il-virus-che-ruba-il-codice/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/09/stellarium-400x240.jpg)

    Il malware che spia chi visita siti porno](https://hackerjournal.it/14518/il-malware-che-spia-chi-visita-siti-porno/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/09/kaspesky_corso-400x240.png)

    Un corso online per difendere gli LLM](https://hackerjournal.it/14504/un-corso-online-per-difendere-gli-llm/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/09/truffa_iphone-400x240.png)

    Truffe online dell’iPhone 17](https://hackerjournal.it/14495/truffe-online-delliphone-17/)
* [Tech](https://hackerjournal.it/category/tecno/)
* [Articoli](https://hackerjournal.it/category/tech/)

  + [![](https://hackerjournal.it/wp-content/uploads/2025/09/deepin_home-400x240.png)

    Linux incontra il design](https://hackerjournal.it/14508/linux-incontra-il-design/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/09/concetto-di-gestione-delle-relazioni-con-i-clienti-400x240.jpg)

    L’arte di ascoltare le reti](https://hackerjournal.it/14474/larte-di-ascoltare-le-reti/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/04/attacchi-cibenetici-400x240.jpg)

    Attacchi ai servizi di rete](https://hackerjournal.it/14439/attacchi-ai-servizi-di-rete/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/08/persona-che-scrive-su-un-primo-piano-del-computer-portatile-400x240.jpg)

    Enumerazione: la vera identità della rete](https://hackerjournal.it/14421/enumerazione-la-vera-identita-della-rete/)
  + [![](https://hackerjournal.it/wp-content/uploads/2025/08/codice-binario-con-globo-sul-computer-portatile-400x240.jpg)

    I migliori tool per la scansione di rete](https://hackerjournal.it/14410/i-migliori-tool-per-la-scansione-di-rete/)
* [Trending](https://hackerjournal.it/trending/)
* [Accademia](https://hackerjournal.it/corsi/)
* [Contest](https://hackerjournal.it/contest/)
* [Glossario](https://hackerjournal.it/encyclopedia/)
* [Abbonati](https://hackerjournal.it/81/abbonati-ad-hacker-journal/)
* [Arretrati](https://hackerjournal.it/arretrati-hackerjournal/)

### [Articoli](https://hackerjournal.it/category/tech/)

# Machine Learning a rischio cracker

I più avanzati modelli di intelligenza artificiale e di riconoscimento biometrico rappresentano un ghiotto boccone per gli hacker di tutto il mondo

![Avatar](https://secure.gravatar.com/avatar/7b274a75782cdb25f96daff3132a6c9c?s=46&d=mm&r=g)

Pubblicato

il

30 Gennaio 2023

By

[hj\_backdoor](https://hackerjournal.it/author/hj_backdoor/ "Articoli scritti da hj_backdoor")

![](https://hackerjournal.it/wp-content/uploads/2023/01/machine_learning-scaled.jpg)

* Share
* Tweet

Nella vita di tutti i giorni dipendiamo dalle più differenti applicazioni di Machine Learning. Basti pensare ai sistemi di riconoscimento biometrico, oppure alle pubblicità personalizzate che appaiono sui social o anche alle stesse soluzioni anti[malware](https://hackerjournal.it/encyclopedia/malware/ "Malware o “software malevolo” è un termine generico che descrive un programma/codice dannoso che mette a rischio un sistema.") e antispyware che si basano su modelli di Intelligenza Artificiale. **Sempre più spesso succede di non capire se le chat di assistenza a cui ci rivolgiamo siano presidiate da persone oppure da bot e prima che ci venga autorizzato un prestito o un finanziamento sarà sempre l’Intelligenza Artificiale ad avere l’ultima parola**. **Da qualche tempo però i modelli di Machine Learning possono essere utilizzati non solo dalle aziende, ma anche da semplici appassionati grazie alla disponibilità di speciali librerie come SciKit, Numpy, TensorFlow, PyTorch, e CreateML** che permettono di risolvere problemi complessi che solo qualche anno fa avrebbero richiesto l’intervento di un esperto. Come accade regolarmente nei settori in continua crescita, parallelamente agli avanzamenti tecnologici appaiono minacce e attacchi portati da pirati informatici che in molti casi riescono ad agire senza essere individuati, fin quando non è troppo tardi.

### Machine Learning sotto attacco

**Gli Adversarial Attack sono attacchi informatici compiuti ai danni dei vari modelli di Intelligenza Artificiale con l’obiettivo di ingannare il modello di Machine Learning**, in modo da alterarne il risultato. Un esempio concreto è rappresentato dai Pixel Attack che funzionano applicando modifiche minime, anche di un solo pixel, come ha dimostrato l’abstract dei ricercatori Jiawei Su, Danilo Vasconcellos Vargas e [Kouichi Sakurai](https://arxiv.org/pdf/1710.08864.pdf). **I risultati forniti dalle Reti Neurali Profonde**, (Deep Neural Network) possono essere facilmente alterati modificando in maniera umanamente impercettibile il vettore iniziale con l’inserimento di un solo pixel alterato. In questo modo fino al 74% dei risultati viene completamente corrotto, fornendo risultati inaffidabili. **Nell’abstract appena citato sono mostrati diversi esempi di immagini modificate in un solo pixel che una volta analizzati dai sistemi di [Machine Learning](https://hackerjournal.it/1770/lultima-frontiera-delle-fake-news-il-machine-learning/) producono risultati completamente differenti**. In un caso, come riferito dal Washington Post, è bastato modificare un solo pixel di un’immagine per alterare complessi modelli di compravendita azionaria con risultati disastrosi.

### Data Poisoning

**La modalità di attacco Adversarial si fonda sui classici sistemi basati su algoritmi come quelli di data poisoning (inquinamento dei dati) e inference attack**. Solo che in questo caso l’attaccante sceglie di colpire la modalità di archiviazione e di distribuzione dei modelli. Anche se i modelli di Machine Learning vengono percepiti come una tecnologia talmente evoluta da essere difficilmente analizzabile da un non addetto ai lavori, in realtà vengono utilizzate le stesse modalità in uso con normali software con tanto di ricerca di vulnerabilità che possano essere sfruttate dall’attaccante. **È questo il caso del formato di archiviazione Pickle che viene normalmente utilizzato nel linguaggio di programmazione Python**. **Questo formato viene usato anche per i modelli di Machine Learning e permette a un’attaccante di eseguire del codice malevolo utilizzando uno strumento Open Source chiamato Fickling, come è ben noto alla community di sicurezza internazionale.**

![](https://hackerjournal.it/wp-content/uploads/2023/01/machine_learning.png)

*[Foto: DA GATTO A CANE*
*Come ha dimostrato il paper del ricercatore Goodfellow e dei suoi colleghi alla R...