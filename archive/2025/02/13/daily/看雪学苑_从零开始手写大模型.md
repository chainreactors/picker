---
title: 从零开始手写大模型
url: https://mp.weixin.qq.com/s?__biz=MjM5NTc2MDYxMw==&mid=2458589462&idx=2&sn=4d3542ac094f7310f0a526005e14e8b9&chksm=b18c299c86fba08a07829263297ed4d2bcc0b29b4259a4c2b7ce2b17fd16e0deaede4d959659&scene=58&subscene=0#rd
source: 看雪学苑
date: 2025-02-13
fetch_date: 2025-10-06T20:35:50.188756
---

# 从零开始手写大模型

![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/1UG7KPNHN8G8whKABhLC3yWDPia7Y6RdgSMDJq9WGSefXlzQ5ibe4VOGRy88hia1rANV6m99NV44cCo5pDtd8EyKQ/0?wx_fmt=jpeg)

# 从零开始手写大模型

道总行走江离

看雪学苑

Q：我一点AI都不懂怎么办？

A：没事作者也是面向gpt编程，纯小白也可以尝试^\_^

Q：为什么会想自己写一个大模型？现在的大模型已经很成熟了我为什么要自己写一个？

A：一个是最近的热点deepseek，感觉AI很有意思。但主要是觉得自己写一个很酷。事情都是从零到一的，我认为自己先写一个完全属于自己的模型，即使它一直报错，即使它回答的问题一直不对，在写完的那一刻的成就感也是无可比拟的。有成就感，有兴趣，才能一步步深入地学习了解。

```
一

搭建开发环境
```

## **python3**

## **pytorch**

```
pip install numpy tqdm matplotlib
```

## **依赖库**

```
pip install numpy tqdm matplotlib
```

## **vscode / Jupyter**

```
二

准备一个数据集
```

##

◆WikiText

◆OpenWebText

◆anythingelse

## 1.准备环境（以openwebtext为例）

```
git clone https://github.com/JCPETERSON/OpenwebText.git
cd OpenwebText
```

python版本得高一点，我用的3.12，3.8不行。

```
pip install -r requirements.txt
```

如果老报错版本问题就直接下（没报错就用requirements.txt）

```
pip install beautifulsoup4 certifi chardet cssselect feedfinder2 feedparser htmlmin idna jieba3k lxml newspaper3k nltk numpy pandas pillow python-dateutil pytorch-pretrained-bert pytz pyyaml recordtype requests-file requests singledispatch six soupsieve spacy tinysegmenter tldextract tqdm urllib3 urlparse2 pycurl pebble chardet transformers
```

##

## 2.加载&预处理数据

### (1)、直接下载Pushshift 数据

这里面的url是已经去重了的，正常流程如下：

```
提取 URL
python extract_urls.py --single_file pushshift_dumps/RS_v2_2005-06.xz

想提取一个时间范围内的 URL
python extract_urls.py --year_start 2016 --year_end 2018

去重 URL
python deduplicate_urls.py --input_dir url_dumps
```

###

### (2)、下载HTML数据

```
python312 download.py D:\Tools\openwebtext\URLs\RS_2011-01.bz2.deduped.txt --n_procs 100 --scraper raw --chunk_size 100000 --compress --timeout 30
```

将抓取的 HTML 页面存储在`scraped`文件夹中，并压缩存档。

等挺久的，挂着睡觉了。默认它done了就是好了。

![1738029502506.png](https://mmbiz.qpic.cn/sz_mmbiz_png/1UG7KPNHN8ETJe7lMGMicXT2IQtrAVWiaTs3Paaljwnm4YibD5BsL3MJoSsuMqRg5qd7gHMgAgklxicXNC5Z0h3TeQ/640?wx_fmt=png&from=appmsg)

![1738029524605.png](https://mmbiz.qpic.cn/sz_mmbiz_png/1UG7KPNHN8ETJe7lMGMicXT2IQtrAVWiaT63Gah7tu0nBvdvGPzBjQ6OAqAq7Ob6yKQPaTVGjricJsOPgIT38C8uw/640?wx_fmt=png&from=appmsg)

###

### (3)、从HTML中提取文本

```
pip install --upgrade newspaper3k
```

把extract\_text.py里的save\_parsed\_file改成如下：

```
def save_parsed_file(filename, text, out_dir):
    # 获取文件的完整路径
    file_path = os.path.join(out_dir, filename)

    # 确保目录存在，如果不存在则创建
    os.makedirs(os.path.dirname(file_path), exist_ok=True)

    # 写入文件
    with open(file_path, 'w', encoding='utf-8') as handle:
        handle.write(text)
```

![1738048415160.png](https://mmbiz.qpic.cn/sz_mmbiz_png/1UG7KPNHN8ETJe7lMGMicXT2IQtrAVWiaTjPgZNdATyI9lhiaK1WdQdUPtGX8k6V3eqBMBb8uGLn1LUCI4pjcRqbw/640?wx_fmt=png&from=appmsg)

```
python312 extract_text.py --html_archive scraped/RS_2011-01-1_data.xz --n_procs 100
```

从 HTML 中提取出文本内容并保存为`.txt`文件.

如果中间有报错重新来的话，记得把原来提取的文件删掉，文件夹在scraped里。

### (4)、分词处理

```
python -m spacy download en_core_web_sm
```

更改tokenize\_text.py：

```
import spacy
import io
import argparse
import glob
import os
import tqdm
from multiprocessing import Pool
from functools import partial
import chardet

def detect_encoding(file_path):
    """检测文件的实际编码"""
    with open(file_path, 'rb') as f:
        raw_data = f.read(1024)  # 读取文件的前 1KB 数据
    result = chardet.detect(raw_data)
    return result['encoding'] or 'utf-8'  # 如果检测失败，默认返回 'utf-8'

def save_tokenized_text(output_dir, filename, text):
    # 构建完整输出路径
    text_file = os.path.join(output_dir, filename)

    # 确保目标目录存在
    os.makedirs(os.path.dirname(text_file), exist_ok=True)

    # 保存文件
    with io.open(text_file, 'w', encoding='utf-8') as fo:
        fo.write(text)

def tokenizeSpacy(args):
    nlp = spacy.load("en_core_web_sm")  # 加载 spaCy 模型
    extraction_file_paths = glob.glob(args.input_glob)

    for extraction_file_path in extraction_file_paths:
        path, filename = os.path.split(extraction_file_path)
        text_file = os.path.join(
            args.output_dir, filename.replace('.txt', '.tokenized.txt'))

        # 确保输出目录存在
        os.makedirs(os.path.dirname(text_file), exist_ok=True)

        # 检测文件编码
        file_encoding = detect_encoding(extraction_file_path)

        try:
            # 打开输入文件和输出文件
            with io.open(extraction_file_path, 'r', encoding=file_encoding) as fi, \
                    io.open(text_file, 'w', encoding='utf-8') as fo:

                omitted_line_count = 0
                for line in fi:
                    if len(line.strip()) > 0:  # 忽略空行
                        doc = nlp(line)
                        fo.write(' '.join([x.text for x in doc]) + '\n')
                    else:
                        omitted_line_count += 1

            print(f'Omitted {omitted_line_count} empty lines from {filename}')
        except UnicodeDecodeError:
            print(f"Failed to decode {extraction_file_path} with encoding {file_encoding}. Skipping this file.")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_glob', type=str, default='*.txt')
    parser.add_argument('--output_dir', type=str, default='tokenized')
    parser.add_argument('--tokenizer', type=str, default='spacy', choices=['spacy', 'gpt2'])
    parser.add_argument('--combine', type=int, default=1e8, help="min tokens per file in gpt2 mode")
    parser.add_argument('--file_bs', type=int, default=10000, help="files per batch in gpt2 mode")

    # 解析命令行参数
    args = parser.parse_args()

    # 确保输出目录存在
    os.makedirs(args.output_dir, exist_ok=True)

    # 根据 tokenizer 选择执行的函数
    if args.tokenizer == 'spacy':
        tokenizeSpacy(args)
    else:
        print("GPT-2 tokenizer is not implemented in this version.")
```

```
python312 tokenize_text.py --input_glob "parsed/RS_2011-01/*.txt" --output_dir tokenized
```

![1738067274379.png](https://mmbiz.qpic.cn/sz_mmbiz_png/1UG7KPNHN8ETJe7lMGMicXT2IQtrAVWiaTV246e0GeapvWNataLfSQFBRb82p2HBUVexAfRw5G7UWZO0gHTZdqrQ/640?wx_fmt=png&from=appmsg)

#

```
三

构建和训练 GPT 类似模型
```

##

下面的文件层级关系如下：

```
gpt_project/
├── model/
│   ├── gpt.py
│   ├── transformer_block.py
├── data/
│   ├── dataset.py
│   ├── tokenizer.py  # 可选
│   ├── tokenized/  # 存放所有分词好的 .txt 文件
├── train/
│   ├── train.py  # 训练代码
|——train_model/
|
├── inference.py  # 生成文本
```

![1738164364319.png](https://mmbiz.qpic.cn/sz_mmbiz_png/1UG7KPNHN8ETJe7lMGMicXT2IQtrAVWiaTeX34fZcPON6uPhqyR8SJQia9oHPKmuAmRXwNV0Q6dQAPmHo1iacJwPKA/640?wx_fmt=png&from=appmsg)

## 1.**dataset.py**

用于处理数据集：

```
#dataset.py
import torch
import os
from collections import Counter
from transformers import AutoTokenizer

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, directory_path, seq_length, tokenizer):
        self.seq_length = seq_length
        self.tokenizer = tokenizer
        self.data = []
        self.vocab = {}
        self.inverse_vocab = {}

        # 第一步：统计所有单词的频率
        word_counter = Counter()

        # 遍历 directory_path 目录中的所有 .tokenized.txt 文件
        for filename in os.listdir(directory_path):
            if filename.endswith(".tokenized.txt"):
                file_path = os.path.join(directory_path, filename)
                with open(file_path, "r", encoding="utf-8") as f:
                    words = f.read().split()
                    word_counter.update(words)

        # 第二步：创建词汇表，给每个单词分配一个 ID
        self.vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_counter.items())}
        self.vocab['<pad>'] = 0  # 为 padding 添加一个 ID
        self.vocab['<unk>'] = len(self.vocab)  # 为未知单词添加一个 ID

        # 创建逆词汇表
        self.inverse_vocab = {idx: word for word, idx in self.vocab.items()}

        # 第三步：将文本转换为 token ID
        for filename in os.listdir(directory_path):
            if filename.endswith(".tokenized.txt"):
                file_path = os.path.join(directory_path, filename)
                with open(file_path, "r", encoding="utf-8") as f:
                    words = f.read().split()
                    # 将每个单词转换为 token ID，如果不在词汇表中则使用 <unk>
                    token_ids = [self.vocab.get(word, sel...