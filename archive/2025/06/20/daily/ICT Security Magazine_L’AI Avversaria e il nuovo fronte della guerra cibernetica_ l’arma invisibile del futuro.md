---
title: L’AI Avversaria e il nuovo fronte della guerra cibernetica: l’arma invisibile del futuro
url: https://www.ictsecuritymagazine.com/articoli/ai-avversaria/
source: ICT Security Magazine
date: 2025-06-20
fetch_date: 2025-10-06T22:54:52.482309
---

# L’AI Avversaria e il nuovo fronte della guerra cibernetica: l’arma invisibile del futuro

[Salta al contenuto](#main)

[![ICT Security Magazine](https://www.ictsecuritymagazine.com/wp-content/uploads/2016/01/logo-ict-security.jpg)](https://www.ictsecuritymagazine.com/)

* [Home](https://www.ictsecuritymagazine.com/)
* [Articoli](https://www.ictsecuritymagazine.com/argomenti/articoli/)
* RubricheEspandi
  + [Cyber Security](https://www.ictsecuritymagazine.com/argomenti/cyber-security/)
  + [Cyber Crime](https://www.ictsecuritymagazine.com/argomenti/cyber-crime/)
  + [Cyber Risk](https://www.ictsecuritymagazine.com/argomenti/cyber-risk/)
  + [Cyber Law](https://www.ictsecuritymagazine.com/argomenti/cyber-law/)
  + [Digital Forensic](https://www.ictsecuritymagazine.com/argomenti/digital-forensic/)
  + [Digital ID Security](https://www.ictsecuritymagazine.com/argomenti/digital-id-security/)
  + [Business Continuity](https://www.ictsecuritymagazine.com/argomenti/business-continuity/)
  + [Digital Transformation](https://www.ictsecuritymagazine.com/argomenti/digital-transformation/)
  + [Cyber Warfare](https://www.ictsecuritymagazine.com/argomenti/cyber-warfare/)
  + [Ethical Hacking](https://www.ictsecuritymagazine.com/argomenti/ethical-hacking/)
  + [GDPR e Privacy](https://www.ictsecuritymagazine.com/argomenti/gdpr-e-privacy/)
  + [IoT Security](https://www.ictsecuritymagazine.com/argomenti/iot-security/)
  + [Industrial Cyber Security](https://www.ictsecuritymagazine.com/argomenti/industrial-cyber-security/)
  + [Blockchain e Criptovalute](https://www.ictsecuritymagazine.com/argomenti/blockchain-e-criptovalute/)
  + [Intelligenza Artificiale](https://www.ictsecuritymagazine.com/argomenti/intelligenza-artificiale/)
  + [Geopolitica e Cyberspazio](https://www.ictsecuritymagazine.com/argomenti/geopolitica-cyberspazio/)
  + [Interviste](https://www.ictsecuritymagazine.com/argomenti/interviste/)
* [Notizie](https://www.ictsecuritymagazine.com/argomenti/notizie/)
* [Pubblicazioni](https://www.ictsecuritymagazine.com/pubblicazioni/)
* [Cybersecurity Video](https://www.ictsecuritymagazine.com/argomenti/cybersecurity-video/)
* [Eventi](https://www.ictsecuritymagazine.com/eventi/)
* [Newsletter](https://www.ictsecuritymagazine.com/newsletter/)

[Linkedin](https://www.linkedin.com/company/ict-security-magazine/) [YouTube](https://www.youtube.com/%40ictsecuritymagazine1403) [RSS](https://www.ictsecuritymagazine.com/feed/)

[![ICT Security Magazine](https://www.ictsecuritymagazine.com/wp-content/uploads/2016/01/logo-ict-security.jpg)](https://www.ictsecuritymagazine.com/)

Attiva/disattiva menu

[![Forum ICT Security 2025](https://www.ictsecuritymagazine.com/wp-content/uploads/banner-header-2025.jpg)](https://www.ictsecuritymagazine.com/eventi/forumictsecurity2025)

![AI avversaria e attacchi cibernetici con elementi di sicurezza informatica e malware intelligente](https://www.ictsecuritymagazine.com/wp-content/uploads/untitled-design-3_3QfpTBXa.png)

# L’AI Avversaria e il nuovo fronte della guerra cibernetica: l’arma invisibile del futuro

A cura di:[Alessio Garofalo](#molongui-disabled-link)  Ore 19 Giugno 202510 Giugno 2025

Negli ultimi anni l’intelligenza artificiale (AI) ha rivoluzionato il panorama della sicurezza informatica, sia come potente strumento difensivo sia come pericolosa risorsa nelle mani degli attaccanti.

Il concetto di AI avversaria, ovvero l’uso intenzionale dell’intelligenza artificiale per condurre attacchi informatici o sabotare sistemi di difesa automatizzati, sta emergendo come una delle minacce più complesse e insidiose della guerra cibernetica moderna.

L’AI avversaria sfrutta le vulnerabilità intrinseche nei modelli di apprendimento automatico e nei sistemi intelligenti, rendendo possibile la creazione di *malware* capaci di apprendere e adattarsi in tempo reale alle difese cibernetiche.

Ancora più preoccupante è il suo potenziale nell’alimentare attacchi di ingegneria sociale, permettendo a criminali e attori statali di lanciare campagne di *phishing* personalizzate e sfruttare tecnologie come i *deepfake* per creare falsi digitali convincenti.

Nel 2018 IBM svelò un esperimento chiamato DeepLocker, un *malware* alimentato dall’intelligenza artificiale, progettato per dimostrare come le minacce informatiche *AI-driven* possano essere quasi invisibili fino al momento dell’attacco. DeepLocker utilizzava modelli di *machine learning* per nascondere il *malware* all’interno di applicazioni comuni, attivandolo solo quando riconosceva il bersaglio specifico attraverso input biometrici o altri segnali unici.

L’esperimento dimostrò come gli attacchi basati su AI potessero essere estremamente difficili da rilevare, aumentando esponenzialmente la complessità per i difensori: questo tipo di attacco segna infatti il passaggio dai *malware* statici a minacce dinamiche, capaci di evolversi e adattarsi a specifici contesti.

Se il crimine informatico ha già raggiunto livelli preoccupanti, il coinvolgimento dell’intelligenza artificiale sta ulteriormente aggravando lo scenario di rischio. Un report del 2020 di Cybersecurity Ventures[[1]](#_ftn1) prevede che il costo globale del crimine informatico raggiungerà i 10,5 trilioni di dollari all’anno entro il 2025.

Gli attacchi alimentati da AI rappresentano una crescente percentuale di queste perdite, con tattiche di *phishing* avanzate, automazione di *botnet* e attacchi *zero-day* potenziati dall’AI; questi attacchi aumentano non solo il costo ma anche la complessità della protezione, richiedendo alle aziende di adottare misure di difesa altrettanto avanzate, come sistemi di *anomaly detection AI-driven* e tecniche di apprendimento continuo.

Questo articolo esplorerà in dettaglio come l’AI avversaria stia ridefinendo i confini della guerra cibernetica, trasformando il cyberspazio in un campo di battaglia dove l’arma più potente non è il codice malevolo tradizionale ma un’intelligenza artificiale capace di evolvere, adattarsi e sfidare persino i sistemi di difesa più avanzati.

## AI avversaria

L’AI avversaria è una forma specifica di intelligenza artificiale progettata (o manipolata) per eludere, confondere o sovvertire i sistemi di difesa cibernetici basati su AI.

Mentre l’intelligenza artificiale è stata a lungo utilizzata per migliorare la sicurezza informatica, l’AI avversaria rappresenta il lato oscuro di questa tecnologia: un modo in cui gli attaccanti possono sfruttare le debolezze e le imperfezioni intrinseche nei modelli di apprendimento automatico.

#### **Tecniche di attacco dell’AI Avversaria**

Gli attacchi condotti tramite AI avversaria si basano su diverse tecniche sofisticate, che mirano a compromettere l’integrità dei modelli AI utilizzati per la sicurezza.

Di seguito alcune delle principali tecniche utilizzate dagli attaccanti:

* ***Evasion Attacks*:** questi attacchi si basano sulla manipolazione dei dati di input per ingannare i sistemi di difesa AI. Un esempio classico è il *malware* che, grazie all’introduzione di piccoli cambiamenti nel proprio codice, riesce a sfuggire ai rilevatori basati su AI, che non lo riconoscono come minaccia.
* ***Data Poisoning*:** in questa tipologia di attacchi, gli aggressori alterano i dati di addestramento del modello AI per indurlo a prendere decisioni errate. Un sistema di difesa potrebbe, ad esempio, essere addestrato su dati in cui è stato iniettato del codice malevolo che viene classificato erroneamente come benigno.
* ***Model Stealing*:** in questo caso gli attaccanti possono tentare di estrarre informazioni sui modelli AI utilizzati dalle organizzazioni, copiandoli o “rubandoli” al fine di replicarli o di sfruttarne le debolezze in attacchi futuri. Ciò consente agli aggressori di conoscere i criteri di rilevamento e creare minacce su misura che riescono a bypassare le difese AI.

#### **Il rischio delle “Black Box”**

Una delle caratteristiche che rende l’AI avversaria così pericolosa è la natura *“black box”* di molti sistemi di intelligenza artificiale.

La mancata trasparenza nel processo decisionale di un modello...