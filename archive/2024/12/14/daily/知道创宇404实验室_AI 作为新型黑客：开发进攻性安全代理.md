---
title: AI 作为新型黑客：开发进攻性安全代理
url: https://mp.weixin.qq.com/s?__biz=MzAxNDY2MTQ2OQ==&mid=2650990139&idx=1&sn=435707dae3b02a48ad38058cd3dcd334&chksm=8079a409b70e2d1fc0bc0779ed21b9d39a063123760770cb5a5c54ffa4c250d7c6c6d1ca56c6&scene=58&subscene=0#rd
source: 知道创宇404实验室
date: 2024-12-14
fetch_date: 2025-10-06T19:41:07.796209
---

# AI 作为新型黑客：开发进攻性安全代理

![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/3k9IT3oQhT2dcQgt7Dn552BmzpmXv4ia5S9SlmbmPJWpicgbRd2vzXmHY0FrZNBL7fYCGXWa9icbq4TsA9Ioks5DA/0?wx_fmt=jpeg)

# AI 作为新型黑客：开发进攻性安全代理

知道创宇404实验室

作者：Leroy Jacob Valencia
译者：知道创宇404实验室翻译组
原文链接：https://arxiv.org/pdf/2406.07561

**摘要**

参考资料

在网络安全这一宏大领域内，由防御性措施向主动防御的转变对于守护数字基础设施的安全至关重要。本文深入探讨了AI在主动防御网络安全中的角色，尤其是通过研发一个名为ReaperAI的独立AI代理，该代理被设计来模拟和实施网络攻击。借助于大型语言模型（LLMs），如GPT-4，ReaperAI展现了其在自动识别、利用和分析安全漏洞方面的潜力。

本项研究概述了一系列核心方法论，旨在提升一致性和性能，包括以任务为导向的渗透测试框架、由AI驱动的命令生成以及高级提示技术。该AI代理在结构化环境中运用Python进行操作，并通过检索增强生成（RAG）技术来增强其对上下文的理解和记忆保持能力。ReaperAI在包括Hack The Box在内的多个平台上进行了测试，并成功利用已知漏洞，证明了其潜在的能力。

然而，AI在主动防御安全领域的应用也带来了重大的伦理和运营挑战。代理的开发过程揭示了在命令执行、错误处理以及维护伦理约束方面的复杂性，这突显了未来需要加强的领域。

本研究通过展示AI如何增强主动防御安全策略，为AI在网络安全领域的作用提供了有益的讨论。同时，也提出了未来研究的方向，包括改进AI与网络安全工具的互动、加强学习机制，以及探讨AI在主动防御角色中的伦理指导原则。研究结果强调了在网络安全中采用创新的AI实施方法的重要性。

**关键词：人工智能、进攻性网络安全、大型语言模型、渗透测试**

**第一章 引言**

参考资料

在迅速变化的网络安全领域中，防御策略一直是焦点，强调保护数字资产免受恶意实体的侵害。然而，随着网络威胁的复杂性和成熟度不断增加，进攻性网络安全的重要性日益增长，成为传统防御策略的重要补充。进攻性网络安全采用对手的战术、技术和思维方式，以正确识别、利用和中和漏洞，防止攻击者利用。

本文考察了人工智能在推进进攻性安全措施中的变革作用。AI在处理大量数据集、识别模式和自动化复杂任务方面的卓越能力，使其成为开发复杂进攻性安全策略的关键组成部分。本研究提供了当前技术、方法论以及与网络安全中的AI相关的伦理问题的深入分析，突出了AI的潜力。

此外，本文讨论了一个名为ReaperAI的GitHub项目(https://github.com/tac01337/ReaperAI)，该项目是探索概念的实际应用示例。ReaperAI作为一个概念验证，展示了AI如何整合到进攻性网络安全中，有效地模拟对手。本研究的目标是在现有AI研究的基础上，探索新的方向和潜力，利用现有的AI能力开发针对高级网络安全解决方案的功能性产品。这种探索不仅为学术领域做出了贡献，也为实际应用推动了界限，展示了AI在进攻性网络安全背景下可以如何被利用。

**1.1 问题陈述**

####

本研究旨在回答以下问题：“如何利用现有关于大型语言模型的研究来开发一个完全自主的进攻性安全代理？”目标是编制一份综合的研究和想法，借鉴其他学术文章和演讲中建议的未来方向。所提出的方法论和技术的有效性将根据代理产生期望的渗透测试行为的表现和行为进行评估。

**1.2 影响**

####

这项研究对行业的潜在影响可能非常显著。尽管对此类研究的可行性一直存在猜测，但尚未通过概念验证或以这种方式执行的最小可行产品来展示。成功的实施可以树立先例，并激发该领域内的进一步创新。

**第二章 背景与文献综述**

参考资料

#### **2.1 进攻性安全的演变**

进攻性安全的世界已经发生了相当大的变化。它最初主要涉及简单的漏洞扫描，但现在涉及到模拟实际网络攻击的高级技术。这种转变突显了网络威胁的复杂性日益增加，以及攻击者如何通过使用更先进的技术和战术变得更加复杂。过去，安全工作主要集中在事件发生后的响应上。但现在，随着红队和道德黑客等方法的出现，重点是通过像攻击者一样思考来提前解决这些问题。这种立场有助于加强我们对网络威胁的防御。

渗透测试的早期历史可以追溯到20世纪60年代，当时首次提出了对计算机通信安全的担忧。政府和企业开始组建团队，测试并发现其网络内的漏洞，作为对任何实际攻击的防御。像James P. Anderson这样的先驱为这个领域做出了显著贡献，他开发了至今仍在使用的方法，如Anderson报告研究所[2019]。

此外，将连续渗透测试与像SIEM（安全事件事件管理）这样的系统集成正在彻底改变识别和解决漏洞的方式，提供了一种更流畅的网络安全方法。这种集成有助于自动化响应和补救流程，从而提高安全团队的效率，并减少解决漏洞所需的时间[Security 2022]。

#### **2.2 大型语言模型的演变**

过去十年中，大型语言模型的演变展示了人工智能的飞跃，从基本的自然语言处理工具转变为能够生成类似人类文本和响应人类输入的高度复杂系统。最初，大型语言模型在2018年左右的早期阶段范围和能力有限，专注于特定任务，如语言翻译和从基本查询中获得响应。2018年BERT（Bidirectional Encoder Representations from Transformers）的重要发布标志着一个进步，引入了允许在文本生成中更深入理解上下文的技术[Devlin et al. 2018]。随后，OpenAI发布的GPT（Generative Pre-trained Transformer）进一步扩展了可能性，采用无监督学习来生成跨广泛主题和格式的连贯且相关的文本[Radford et al. 2019]。这种从专业应用到通用能力的转变反映了人工智能向模型的更广泛趋势，这些模型不仅理解和生成文本，而且还能在执行类似人类的行动和对复杂主题的细致理解方面取得成功。这种理解的概念已经开始了能力的浪潮，这些能力是由于纯粹的知识和数据。模型大小和复杂性的持续增长，以GPT-3 OpenAI [2023]及其后续产品的推出为例，真正突显了向能够无缝集成到人类任务和通信中的系统的持续转变，这引发了伦理考虑和技术与通信中的独特应用。

#### **2.3 网络安全中的AI：历史视角**

从历史上看，AI在网络安全中的参与从简单的基于规则的检测系统演变为更先进的机器学习算法，这些算法能够识别与网络威胁相关的复杂模式。”进化中的GenAI工具对网络安全来说是双刃剑，既有利于防御者也有利于攻击者。“[Gupta et al. 2023] AI的角色自那以后扩大到包括预测分析、自动响应系统和复杂的威胁情报，主要支持蓝队和防御队。

在进攻性安全领域，AI的使用提供了改变传统实践的潜力，在模拟现实的网络攻击、自动化发现漏洞和证明漏洞利用，以及生成现实的网络钓鱼攻击的例子。这种AI与进攻性安全战术的整合代表了网络安全中的策略，利用AI的分析能力领先于网络犯罪分子。[Zennaro and Erd˝odi 2023]

#### **2.4 当前AI在进攻性安全中的应用**

最近在网络安全领域的研究强调了人工智能在增强进攻性安全措施中的重要作用。基于深度学习的先进AI模型现在被依赖于自动化检测漏洞，这曾经是手动的且非常劳动密集的。例如，AutoPentest-DRL框架采用深度强化学习来自动化和优化渗透测试，允许在网络系统中动态和高效地利用漏洞[Organization et al. 2020]。此外，AI通过强化学习模型模拟复杂的网络攻击策略，扩大了进攻性安全的范围。这些模型不仅模仿攻击者行为，而且还创新攻击策略，为网络安全专业人员提供了一套工具，以预测潜在的漏洞[Yang and Liu 2022]。AI在渗透测试中的整合还通过其在制作复杂的网络钓鱼电子邮件方面的有效性得到了突出，这些电子邮件可以绕过标准检测系统，展示了AI采用攻击者思维方式的能力，这确实令人担忧[ERMProtect 2020]。通过自然语言处理采用大型语言模型代表了人工智能和机器学习中的一个特别引人注目的进步，由于其在互联网数据集上的广泛训练，它们能够理解、推理并提供建议和总结。这使得它们对于像进攻性安全这样的复杂任务非常有价值。在本文中，我们将探讨这些工具的例子，因为我们的讨论主要集中在应用LLMs上。

##### **2.4.1 PentestGPT**

PentestGPT [Deng et al. 2023]是一个复杂的渗透测试工具，利用OpenAI的GPT-4的力量来自动化和简化渗透测试过程。设计为交互式功能，PentestGPT通过指导测试人员完成渗透测试的一般进程和执行特定任务来协助他们。这个工具特别擅长处理中等复杂度的Hack the Box机器和各种Capture The Flag (CTF)挑战，提高了渗透测试的效率和精度。

PentestGPT的架构包括处理渗透测试工作流程的不同方面的几个模块。它具有一个测试生成模块，为测试人员生成必要的命令，一个测试推理模块，协助测试期间的决策，以及一个解析模块，解释来自渗透工具和Web界面的输出。这些组件协同工作，提供全面和自动化的渗透测试解决方案。

PentestGPT已经显示出在渗透测试任务中显著优于早期模型如GPT-3.5，实现了更高的任务完成率，并在操作效率上显示出实质性的改进。PentestGPT的发展反映了在实际网络安全应用中使用LLMs的显著进步，提供了一个强大的工具，模仿了现实世界环境中经验丰富的测试人员和新手测试人员之间的协作动态。

##### **2.4.2 HackingbuddyGPT**

HackingbuddyGPT [Happe and Cito 2023]是一个尖端工具，旨在探索大型语言模型在渗透测试中的潜力，特别关注Linux权限提升场景。由IPA实验室开发，hackingbuddyGPT与OpenAI的GPT模型集成，以自动化安全测试的命令生成。该工具通过SSH连接到Linux目标（或SMB/PS Exec用于Windows目标），并使用与OpenAI的REST API兼容的模型，如GPT-3.5 Turbo和GPT-4，来建议可能暴露漏洞或提升权限的命令。[Happe et al. 2023]

系统记录所有运行数据，无论是文件中还是内存中，并具有自动根检测功能，以及为更好的用户交互而设计的精美控制台输出。hackingbuddyGPT的一个关键功能是其能够限制交互轮次，这决定了LLM将被查询新命令的频率，允许控制测试场景。[Happe and Cito 2023]

#### **2.5 LLM限制**

像OpenAI的GPT这样的大型语言模型在人类自然语言理解和生成方面拥有一些严重的能力，但它们也遇到了几个限制，这些限制可能影响它们的功能和整合到实际应用中。通常，这些限制被误解为更大智能的迹象。在以下部分中，我们概述了这些限制，并探讨了它们与渗透测试领域相关性，以确保我们尝试克服它们，以产生一个不受这些限制影响的高效工作概念。

##### **2.5.1 提示工程**

提示工程是一个相当新但至关重要的方面，使用大型语言模型，它在专门化的输入中起着重要作用，以引导模型朝着改进的生成输出发展。这个过程特别敏感，因为提示的结构或提示的措辞的微小修改可能导致截然不同的结果[Radford et al. 2019]。有效的提示工程需要深入理解模型的训练数据和嵌入的偏见，这可能既劳动密集又技术复杂[Bender et al. 2021]。这个挑战通常涉及大量的迭代调整和实验，以完善与模型的互动，并实现最佳结果[Liu et al. 2021]。

在进攻性网络安全的背景下，提示工程可以显著增强进攻性代理的能力。通过精确定制提示，开发人员可以指导LLM生成更符合特定网络安全任务的输出，例如识别漏洞或提供要在终端上运行的命令。这种定制化的方法允许LLM在复杂的安全环境中更有针对性的和有效的使用，因为由于给定问题在进攻性安全中的庞大状态规模，通用响应可能不足以满足需求。此外，熟练的提示工程可以帮助减少模型响应中的偏见影响，降低在敏感安全环境中生成不准确或有害行动的风险，以及减少模型可能添加的与事实数据不符的任何幻觉。

##### **2.5.2 上下文与长期记忆**

上下文和记忆在有效部署大型语言模型方面提出了重大挑战。尽管LLMs有效地处理简短的信息片段，但它们在整个对话或文档中保留或整合长期上下文的能力是有限的[Kagaya et al. 2024]。这种限制可能导致在长时间的互动中连贯性的恶化，模型可能“忘记”对话的早期部分，或在互动中维持上下文的困难[Wang et al. 2024]。通常，补救措施包括集成外部系统以维持状态或上下文，这可能会使这些系统的架构复杂化，并可能对响应的准确性和相关性产生不利影响[Kagaya et al. 2024]。

例如，一个配备有辅助记忆系统的进攻性代理可以更好地执行和执行长期的渗透测试任务，这些任务需要保持对先前行动及其结果的意识以变得更有效。这种整合引入了一种更连贯和战略性的方法来模拟或进行网络攻击，以模仿会适应动态目标或环境的人类行为者。虽然这复杂化了基于LLM的系统的架构，但这种权衡可以导致在网络安全中更强大和有能力的进攻工具，其中适应性和持久性至关重要。

##### **2.5.3 LLM学习与推理**

大型语言模型虽然在语言理解方面很熟练，但由于它们在多样化数据集上的广泛预训练，但它们在部署后不会适应或动态学习。与一些可以从新数据中持续学习的机器学习模型不同，LLMs保持静态，除非它们被重新训练或使用更新的数据集进行微调。这一特性限制了它们在没有定期更新其训练材料的快速演变领域中的实用性，这可能是一个资源密集型的过程[Horowitz 2023]。LLMs的挑战在于，它们并没有固有的能力在操作使用中整合新信息。采用提示工程技术来通过精心设计的输入引导模型的响应来缓解这一点，但这并不等同于从这些互动中学习。为了保持相关性，特别是在动态领域，LLMs需要定期使用新数据进行重新训练或微调，这个过程需要计算资源和专家监督[AI 2023]。

然而，这个限制可以通过实施定期更新和微调会话来缓解，使用最新的威胁数据，如CVEs，确保进攻性代理与新的战术和漏洞保持更新。此外，采用提示工程技术可以帮助定制LLM的输出，以更准确地模拟不断发展的攻击场景，即使在其静态知识库的约束内。这种方法允许进攻性代理保持一个强大的工具在渗透测试中，能够通过受控的更新而不是实时学习来适应新的安全格局，从而保持操作的相关性和有效性。

##### **2.5.4 命令解析**

与LLMs的命令解析涉及将自然语言命令转换为可执行操作，这可能因人类语言的歧义和变异性而变得具有挑战性。LLMs可能会误解命令，特别是那些复杂或文档不明确的命令，导致执行不正确或不安全的操作。此外，LLMs理解上下文依赖命令或需要整合多个数据源的命令的能力受到它们的训练和特定架构的限制。这就需要在依赖LLMs进行命令执行的系统中增加额外的验证和错误处理层，以确保操作的准确性和安全性。一种用于在代理之间以及代码和大型语言模型之间传输数据的方法是使用JSON或类似的基于JSON的标准。这种方法促进了各种类型的信息交换，包括描述、输出和响应，确保不同平台和系统之间的标准化通信格式。

##### **2.5.5 训练数据**

与训练数据相关的限制显著影响大型语言模型的开发和有效性。用于训练的数据的质量、多样性和数量不仅影响模型的性能，还影响其在各种上下文中适当运作的能力。LLMs倾向于采纳和放大其训练数据集中的偏见，这可能导致产生有偏见或有害的输出。此外，依赖大量数据集需要相当的计算资源，这可能带来环境和经济挑战。因此，确保训练数据既具有代表性又符合道德来源对于缓解这些问题至关重要。当为特定领域（如网络安全）定制这些模型时，引入领域特定知识是一项复杂的任务。其他工具的经验表明，虽然更精细调整的模型可以表现出高度的专业化，但如果在微调过程中做出重大妥协，它们也可能缺乏多样性。此外，随着模型熟练度的提高，它们的尺寸往往会增加，通常达到数百千兆字节。这种尺寸的增加可能会使部署和操作效率变得复杂。这些考虑在开发进攻性代理时尤其关键，其中模型的特异性、大小和适应性之间的平衡必须仔细管理，以确保生产出强大、生产级别的代理。

##### **2.5.6 风险/恐惧**

大型语言模型的另一个限制是它们缺乏像恐惧这样的内在情感能力，在人类中，这种能力在风险评估和决策中起着至关重要的作用。人类经常使用恐惧作为危险的启发式；它帮助他们避免可能导致伤害的风险。相比之下，LLMs基于模式和数据进行决策，没有任何情感权重。这可能导致在风险评估至关重要的情况下出现挑战，因为模型可能无法像人类那样有效地优先考虑或评估威胁。这个话题的研究相当新，而且在推进LLMs的背景下，更不用说使用LLMs来加速风险分析领域了，还没有吸引太多研究。[Esposito and Palagiano 2024]

然而，这个限制也可以被视为一个优势，特别是在将LLMs作为网络安全中的进攻性代理部署的背景下。缺乏恐惧使LLMs能够系统地执行对人类操作员来说可能被认为是高风险或压力大的任务。例如，LLM可以参与模拟网络攻击或测试网络漏洞，而不会犹豫或道德上的保留，提供了一个彻底而无情的测试能力，这可能会受到人类情感的影响。

##### **2.5.7 创造力**

缺乏创造力是像GPT这样的大型语言模型的主要限制。虽然LLMs擅长通过重组它们庞大的训练数据中的现有模式和信息来生成内容，但它们并不真正“创造”人类意义上的从零开始产生新颖的想法。这个限制源于模型依赖于其训练数据中的模式和相关性，这限制了它们的输出为已经看过的组合。相比之下，创造力通常涉及打破既定模式以产生真正新颖和原创的东西。最近开发了一个框架，用于基准测试这种创造力，展示限制并提供克服潜力，称为：CreativeEval。[DeLorenzo et al. 2024]

创造力是网络安全中的关键资产，特别是对于人类渗透测试人员。这些专业人员依赖于他们跳出思维定势并为安全测试设计创新方法的能力，经常打破既定模式以发现自动化系统可能忽视的漏洞。

##### **2.5.8 勤奋**

人类的勤奋体现在他们能够持之以恒、精准无误地完成任务，并发挥其知识和技能的极致。尽管大型语言模型（LLMs）能够以惊人的速度和精确度处理与分析庞大的数据集，它们却缺少了对细节持续关注的能力。[Jin et al. 2024] 它们无法批判性地自我评估性能，也无法在未经进一步训练或更新的情况下独立进行改进，这可能...