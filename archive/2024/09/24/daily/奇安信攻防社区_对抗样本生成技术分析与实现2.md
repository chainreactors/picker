---
title: 对抗样本生成技术分析与实现2
url: https://forum.butian.net/share/3746
source: 奇安信攻防社区
date: 2024-09-24
fetch_date: 2025-10-06T18:24:53.851550
---

# 对抗样本生成技术分析与实现2

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 对抗样本生成技术分析与实现2

* [漏洞分析](https://forum.butian.net/topic/48)

在继续分析、实现其他经典对抗样本技术之前，我们先来看看对抗样本在网络空间安全其他领域的广泛应用。

网安应用
====
在继续分析、实现其他经典对抗样本技术之前，我们先来看看对抗样本在网络空间安全其他领域的广泛应用。
在恶意软件方面，通过对恶意软件样本添加对抗扰动，使其在特征空间中发生微小变化，从而躲避基于机器学习的恶意软件检测引擎。另外，利用对抗样本生成技术，可以从已知的恶意软件样本中生成新的、具有对抗性的恶意软件变种，增加检测难度。
![image-20240730115439597.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-e3b7d49d9e45750b595287278401ef5d6c0f71fd.png)
在入侵检测方面，通过构造对抗性的网络流量，可以欺骗入侵检测系统，使其无法识别出真正的入侵行为。另外，对抗样本可以诱导入侵检测系统做出错误的响应，例如发出误报或漏报，从而影响安全防护效果。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-d57716d77c80106fdb8c1f8cd503e4dadd63845b.png)
在物联网安全方面， 通过向智能设备发送对抗样本，可以误导设备的传感器或控制系统，从而实现对设备的控制。另外，对抗样本可以用于攻击智能家居系统中的语音助手、智能摄像头等设备，窃取用户隐私或控制家庭设备。
![image-20240730115841743.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-4673b7399a2bcacc3dbf1ef17c486503db9a01a0.png)
在网站安全方面， 利用对抗样本生成技术，可以生成具有高度欺骗性的钓鱼邮件，诱导用户点击恶意链接或下载恶意软件。另外对抗样本也可以帮助攻击者绕过基于机器学习的反钓鱼系统，提高钓鱼攻击的成功率。
![image-20240730115943732.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-cfe7d606f4df5af539d72c196a4f95e0966fb145.png)
其外在很多其他子领域也有对抗样本的身影，这里不再一一赘述。
现在我们继续来学习其他经典的对抗攻击方法。
PGD
===
理论
--
PGD方法来自另一篇经典的论文《Towards Deep Learning Models Resistant to Adversarial Attacks》。针对对抗攻击，以往的研究提出了多种防御机制，例如防御性蒸馏（defensive distillation）、特征压缩（feature squeezing）等方法，这些方法在提高模型对特定攻击的鲁棒性方面取得了一定的进展，但它们通常缺乏对所提供保证的深入理解，无法确保模型能够抵御所有可能的对抗性攻击。
本文提出了一种基于鲁棒优化（robust optimization）的新视角来研究神经网络的对抗性鲁棒性。这种方法不仅为我们提供了一个统一的框架来理解以往的工作，而且通过其原则性的特点，使我们能够识别出既可靠又在某种意义上通用的训练和攻击神经网络的方法。具体来说，本文的方法通过一个自然的鞍点（min-max）公式来精确地定义对对抗性攻击的安全保证，即我们希望模型能够抵抗的攻击类型。
与以往方法的一个关键区别在于，本文不仅关注于提高对特定已知攻击的鲁棒性，而是首先提出了一个具体的安全保证，然后调整训练方法以实现这一保证。这涉及到对攻击模型的精确定义，即明确我们的模型应该能够抵抗的攻击类型。本文中，作者选择了范数球来形式化敌手的操作能力，这是一种自然的方式来定义图像之间的感知相似性。
本文的方法通过实验研究了鞍点公式所对应的优化景观，发现尽管其组成部分是非凸的且非凹的，但在实践中，该优化问题是可行的。特别是，作者提供了强有力的证据表明，一阶方法可以可靠地解决这个问题。此外，作者还探讨了网络架构对对抗性鲁棒性的影响，发现模型容量在其中起着重要作用。为了可靠地抵御强大的对抗性攻击，网络需要比仅对良性示例进行正确分类的模型具有更大的容量。
我们来形式化该方案。本文的核心方法是基于鲁棒优化理论来增强深度学习模型对于对抗性攻击的抵抗力。具体而言，作者采用了一个鞍点问题（min-max problem）的数学框架来形式化和解决这个问题。下面是对本文方法的数学描述和分析：
1. \*\*问题设置\*\*： 假设我们有一个数据分布 ( D )，它产生样本对 ( (x, y) )，其中 ( x ) 是输入数据，( y ) 是对应的标签。我们的目标是找到一组模型参数 ( \\theta )，使得在分布 ( D ) 下的期望损失最小化。然而，传统的经验风险最小化（ERM）方法并不保证模型对对抗性攻击的鲁棒性。
2. \*\*鞍点公式\*\*： 为了解决这个问题，本文引入了一个鞍点问题，定义了一个鲁棒的风险函数 ( \\rho(\\theta) )： 其中，( L ) 是损失函数（例如交叉熵损失），( S ) 是允许的扰动集合，它定义了对抗性攻击的范围。
3. \*\*优化视角\*\*： 上述鞍点问题可以分解为两个部分：内部最大化问题和外部最小化问题。内部最大化问题旨在找到给定数据点 ( x ) 的对抗性版本 ( x + \\delta )，使得损失函数 ( L ) 最大化。外部最小化问题则是寻找模型参数 ( \\theta )，使得在考虑内部攻击问题的情况下，期望损失最小化。
4. \*\*对抗性训练\*\*： 对抗性训练可以看作是解决上述鞍点问题的一种方法。具体来说，可以通过投影梯度下降（PGD）来近似解决内部最大化问题，从而找到对抗性样本。然后，使用这些对抗性样本来训练网络，使得外部最小化问题得以解决。
在攻击实现上，相当于攻击执行 `nb\_iter` 次步长为 `eps\_iter` 的步骤，同时始终保持在距离初始点 `eps` 以内。
实现
--
这段代码定义了一个名为`PGDAttack`的类，实现了一种称为投影梯度下降（PGD）的对抗攻击方法。通过创建该类的实例并调用其`perturb`方法，可以对输入数据进行对抗攻击，生成对抗样本。
![image-20240728140056142.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-264d8fb7c7797232a3913741a8c179939b202d42.png)
这段代码定义了一个名为`PGDAttack`的类，它继承自`Attack`和`LabelMixin`两个类。这个类实现了一种称为投影梯度下降（Projected Gradient Descent，简称PGD）的对抗攻击方法。
1. `\_\_init\_\_`方法：这是类的构造函数，用于初始化类的实例。
- `predict`：输入模型的预测函数。
- `loss\_fn`：损失函数，默认为`None`。
- `eps`：攻击强度，默认为0.3。
- `nb\_iter`：迭代次数，默认为40。
- `eps\_iter`：每次迭代的攻击强度，默认为0.01。
- `rand\_init`：布尔值，表示是否随机初始化扰动，默认为`True`。
- `clip\_min`和`clip\_max`：用于限制输入数据的范围，默认分别为0和1。
- `ord`：范数类型，默认为`np.inf`。
- `l1\_sparsity`：L1稀疏性约束，默认为`None`。
- `targeted`：布尔值，表示攻击是否针对特定目标类别。
在构造函数中，首先调用父类的构造函数，并将`predict`、`loss\_fn`、`clip\_min`和`clip\_max`作为参数传递。然后设置其他属性。如果`loss\_fn`为`None`，则将其设置为`nn.CrossEntropyLoss(reduction="sum")`。最后，检查`eps\_iter`和`eps`是否为浮点数或张量。
2. `perturb`方法：这是实际执行对抗攻击的方法。
- 输入参数`x`和`y`分别表示原始输入数据和对应的标签。
- 使用`\_verify\_and\_process\_inputs`方法验证和处理输入数据。
- 创建一个与`x`形状相同的零张量`delta`，并将其转换为`nn.Parameter`类型。
- 如果`rand\_init`为`True`，则使用`rand\_init\_delta`函数随机初始化扰动。
- 调用`perturb\_iterative`函数进行迭代攻击，返回对抗样本`rval`。
- 返回处理后的对抗样本`rval.data`。
执行攻击，效果如下
![image-20240728140211636.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-23340bf13788a509b613eb5db63c72f2b1cb60ec.png)
将大熊猫预测为了开关
PGD在L无穷范数下进行
如下代码定义了一个名为`LinfPGDAttack`的类，实现了一种基于L∞范数的PGD对抗攻击方法。通过创建该类的实例，可以对输入数据进行对抗攻击，生成对抗样本。
![image-20240728140604062.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-2ca5b25e96bcc1639294e2e4b0d19fa93db19cc1.png)
这段代码定义了一个名为`LinfPGDAttack`的类，它继承自`PGDAttack`类。这个类实现了一种基于L∞范数的PGD（投影梯度下降）对抗攻击方法。
1. `\_\_init\_\_`方法：这是类的构造函数，用于初始化类的实例。
- `predict`：输入模型的预测函数。
- `loss\_fn`：损失函数，默认为`None`。
- `eps`：攻击强度，默认为0.3。
- `nb\_iter`：迭代次数，默认为40。
- `eps\_iter`：每次迭代的攻击强度，默认为0.01。
- `rand\_init`：布尔值，表示是否随机初始化扰动，默认为`True`。
- `clip\_min`和`clip\_max`：用于限制输入数据的范围，默认分别为0和1。
- `targeted`：布尔值，表示攻击是否针对特定目标类别。
在构造函数中，首先设置范数类型`ord=np.inf`，表示使用L∞范数。然后调用父类`PGDAttack`的构造函数，并将所有参数传递给它。
执行效果如下
![image-20240728140632483.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-ba8e9e46c1efc91cfca1c62f9b1c3df742dff8f8.png)
将大熊猫预测为了猪
PGD也可以在L2范数下进行
如下代码定义了一个名为`L2PGDAttack`的类，实现了一种基于L2范数的PGD对抗攻击方法。通过创建该类的实例，可以对输入数据进行对抗攻击，生成对抗样本。
![image-20240728140348469.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-0c7236f238f3abfccbceef14a6e83d2bec319897.png)
这段代码定义了一个名为`L2PGDAttack`的类，它继承自`PGDAttack`类。这个类实现了一种基于L2范数的PGD（投影梯度下降）对抗攻击方法。
1. `\_\_init\_\_`方法：这是类的构造函数，用于初始化类的实例。
- `predict`：输入模型的预测函数。
- `loss\_fn`：损失函数，默认为`None`。
- `eps`：攻击强度，默认为0.3。
- `nb\_iter`：迭代次数，默认为40。
- `eps\_iter`：每次迭代的攻击强度，默认为0.01。
- `rand\_init`：布尔值，表示是否随机初始化扰动，默认为`True`。
- `clip\_min`和`clip\_max`：用于限制输入数据的范围，默认分别为0和1。
- `targeted`：布尔值，表示攻击是否针对特定目标类别。
在构造函数中，首先设置范数类型`ord=2`，表示使用L2范数。然后调用父类`PGDAttack`的构造函数，并将所有参数传递给它。
攻击效果如下
![image-20240728140401851.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-1ee3ad700acd8ae1567081342a39a9c3e9e9239c.png)
可以看到，将大熊猫预测为了猪。
MomentumIterativeAttack
=======================
理论
--
传统的对抗性攻击方法主要关注在白盒模型上生成对抗样本，这些样本通过在合法输入中添加微小的、人类难以察觉的噪声来误导模型做出错误的预测。然而，这些方法在黑盒模型上的应用效果并不理想，特别是在模型具有防御机制时，攻击成功率会显著下降。
MomentumIterativeAttack是一种基于动量的迭代算法，用以增强对抗性攻击的力度。这种方法通过在迭代过程中整合动量项，可以稳定更新方向，并帮助算法从局部最优解中逃逸，从而生成更具迁移性的对抗样本。具体来说，该方法不仅提高了在白盒模型上的攻击成功率，而且在黑盒模型上也表现出色，即使是经过强化训练、具备强大防御能力的模型，也可能对我们的黑盒攻击无能为力。
与现有方法相比，动量迭代梯度基方法（Momentum Iterative Gradient-based Methods）有几个显著的不同之处。首先，它通过积累跨迭代的梯度方向的动量向量，增强了对抗样本的迁移性。其次，该方法不仅针对单一模型，还研究了如何同时攻击多个模型的集成方法，进一步增强了对抗样本的迁移性。
如下是论文中应用该方案得到的对抗样本的示意图
![image-20240728141034480.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-1dd92576f72d487db7192254418d6d3b0da99641.png)
现在我们来形式化该方案
基于动量的迭代梯度符号方法（Momentum Iterative Fast Gradient Sign Method, MI-FGSM），其核心思想是在迭代过程中引入动量（momentum）来增强对抗样本的生成过程。具体来说，这种方法通过以下数学公式来实现：
1. \*\*初始化\*\*：选择一个合法的输入样本 ( x ) 和其正确的标签 ( y )，设定扰动大小 ( \\epsilon )，迭代次数 ( T )，以及衰减因子 ( \\mu )。
2. \*\*迭代过程\*\*：在每次迭代 ( t ) 中，执行以下步骤：
- 计算当前样本 ( x^\\*\\_t ) 对于模型的损失函数 的梯度。
- 更新动量向量 ( g\\_{t+1} )，该向量是之前动量 ( g\\_t ) 与当前梯度的加权和：
- 生成对抗样本 ( x^\\*\\_{t+1} )，通过将当前样本 ( x^\\*\*t ) 与动量向量 ( g\*{t+1} ) 的符号相结合，并缩放一个步长 ( \\alpha )（( \\alpha = \\frac{\\epsilon}{T} )）：
3. \*\*输出\*\*：在完成 ( T ) 次迭代后，输出最终的对抗样本
这种方法与以往的对抗性攻击方法相比，主要有以下不同之处：
- \*\*动量的引入\*\*：通过积累之前迭代的梯度信息，增强了对抗样本在不同迭代间的一致性，有助于避免陷入局部最优解，提高了对抗样本的迁移性。
- \*\*衰减因子 ( \\mu )\*\*：该因子控制了历史梯度信息在动量向量中的权重，对于平衡攻击的强度和迁移性至关重要。
- \*\*迭代攻击\*\*：与一步攻击（如FGSM）相比，迭代方法通过多次更新对抗样本，可以更精细地逼近模型的决策边界，从而提高攻击成功率。
- \*\*对多个模型的攻击\*\*：本文还探讨了如何通过集成多个模型的输出（例如，通过融合模型的对数几率）来生成对抗样本，这进一步提高了对抗样本的迁移性和攻击的成功率。
通过这些数学公式和方法的结合，本文提出的方案在理论和实验上都显示出了对现有对...