---
title: [译] Borg、Omega、K8s：Google 十年三代容器管理系统的设计与思考（ACM, 2016）
url: https://arthurchiao.github.io/blog/borg-omega-k8s-zh/
source: ArthurChiao's Blog
date: 2023-03-03
fetch_date: 2025-10-04T08:30:31.384377
---

# [译] Borg、Omega、K8s：Google 十年三代容器管理系统的设计与思考（ACM, 2016）

# [ArthurChiao's Blog](https://arthurchiao.github.io/)

* [Home](/index.html)
* [Articles (EN)](/articles)
* [Articles (中文)](/articles-zh)
* [Categories](/categories)
* [About](/about)
* [Donate](/donate)

# [译] Borg、Omega、K8s：Google 十年三代容器管理系统的设计与思考（ACM, 2016）

Published at 2023-03-02 | Last Update 2023-03-02

### 译者序

本文翻译自 [Borg, Omega, and Kubernetes](https://queue.acm.org/detail.cfm?id=2898444)，
acmqueue Volume 14，issue 1（2016），
原文副标题为 ***Lessons learned from three container-management systems over a decade***。
作者 Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer, and John Wilkes，
均来自 Google。

文章介绍了 Google 在过去十多年设计和使用前后三代容器管理（编排）系统的所得所思，
虽然是 7 年前的文章，但内容并不过时，尤其能让读者能更清楚地明白 k8s
里的很多架构、功能和设计是怎么来的。

**由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。**

以下是译文。

---

* [译者序](#译者序)
* [1 十年三代容器管理系统](#1-十年三代容器管理系统)
  + [1.1 Borg](#11-borg)
    - [1.1.1 在线应用和批处理任务混部](#111-在线应用和批处理任务混部)
    - [1.1.2 早于 Linux cgroups 的出现](#112-早于-linux-cgroups-的出现)
    - [1.1.3 好处：计算资源共享](#113-好处计算资源共享)
    - [1.1.4 自发的 Borg 生态](#114-自发的-borg-生态)
  + [1.2 Omega](#12-omega)
    - [1.2.1 架构更加整洁一致](#121-架构更加整洁一致)
    - [1.2.2 相比 Borg 的改进](#122-相比-borg-的改进)
  + [1.3 Kubernetes](#13-kubernetes)
    - [1.3.1 为什么要设计 K8s](#131-为什么要设计-k8s)
    - [1.3.2 相比 Omega 的改进](#132-相比-omega-的改进)
  + [1.4 小结](#14-小结)
* [2 底层 Linux 内核容器技术](#2-底层-linux-内核容器技术)
  + [2.1 发展历史：从 `chroot` 到 cgroups](#21-发展历史从-chroot-到-cgroups)
  + [2.2 资源隔离](#22-资源隔离)
  + [2.3 容器镜像](#23-容器镜像)
* [3 面向应用的基础设施](#3-面向应用的基础设施)
  + [3.1 从“面向机器”到“面向应用”的转变](#31-从面向机器到面向应用的转变)
  + [3.2 应用环境（application environment）](#32-应用环境application-environment)
    - [3.2.1 资源隔离 + 容器镜像：解耦应用和运行环境](#321-资源隔离--容器镜像解耦应用和运行环境)
    - [3.2.2 容器镜像实现方式](#322-容器镜像实现方式)
  + [3.3 容器作为基本管理单元](#33-容器作为基本管理单元)
    - [3.3.1 通用 API 和自愈能力](#331-通用-api-和自愈能力)
    - [3.3.2 用 annotation 描述应用结构信息](#332-用-annotation-描述应用结构信息)
    - [3.3.3 应用维度 metrics 聚合：监控和 auto-scaler 的基础](#333-应用维度-metrics-聚合监控和-auto-scaler-的基础)
    - [3.3.4 单实例多容器（pod vs. container）](#334-单实例多容器pod-vs-container)
  + [3.4 编排是开始，不是结束](#34-编排是开始不是结束)
    - [3.4.1 自发和野蛮生长的 Borg 软件生态](#341-自发和野蛮生长的-borg-软件生态)
    - [3.4.2 避免野蛮生长：K8s 统一 API（Object Metadata、Spec、Status）](#342-避免野蛮生长k8s-统一-apiobject-metadataspecstatus)
    - [3.4.3 K8s API 扩展性和一致性](#343-k8s-api-扩展性和一致性)
    - [3.4.4 Reconcile 机制](#344-reconcile-机制)
    - [3.4.5 舞蹈编排（choreography）vs. 管弦乐编排（orchestration）](#345-舞蹈编排choreographyvs-管弦乐编排orchestration)
* [4 避坑指南](#4-避坑指南)
  + [4.1 创建 Pod 时应该分配唯一 IP，而不是唯一端口（port）](#41-创建-pod-时应该分配唯一-ip而不是唯一端口port)
  + [4.2 容器索引不要用数字 index，用 labels](#42-容器索引不要用数字-index用-labels)
    - [4.2.1 Borg 基于 index 的容器索引设计](#421-borg-基于-index-的容器索引设计)
    - [4.2.2 K8s 基于 label 的容器索引设计](#422-k8s-基于-label-的容器索引设计)
  + [4.3 Ownership 设计要格外小心](#43-ownership-设计要格外小心)
  + [4.4 不要暴露原始状态](#44-不要暴露原始状态)
* [5 开放问题讨论](#5-开放问题讨论)
  + [5.1 应用配置管理](#51-应用配置管理)
  + [5.2 依赖管理](#52-依赖管理)
* [6 总结](#6-总结)
* [参考资料](#参考资料)

---

# 1 十年三代容器管理系统

业界这几年对容器的兴趣越来越大，但其实在 Google，我们十几年前就已经开始大规模容器实践了，
这个过程中也**先后设计了三套不同的容器管理系统**。
这三代系统虽然出于不同目的设计，但每一代都受前一代的强烈影响。
本文介绍我们开发和运维这些系统所学习到的经验与教训。

## 1.1 Borg

Google 第一代统一容器管理系统，我们内部称为 Borg 7。

### 1.1.1 在线应用和批处理任务混部

Borg 既可以管理 **long-running service** 也可以管理 **batch job**；
在此之前，这两种类型的任务是由两个系统分别管理的，

* Babysitter
* Global Work Queue

**Global Work Queue** 主要面向 batch job，但它**强烈影响了 Borg 的架构设计**；

### 1.1.2 早于 Linux cgroups 的出现

需要说明的是，不论是我们设计和使用 Global Work Queue 还是后来的 Borg 时，
**Linux cgroup 都还没有出现**。

### 1.1.3 好处：计算资源共享

Borg 实现了 long-running service 和 batch job 这两种类型的任务共享计算资源，
**提升了资源利用率，降低了成本**。

在底层支撑这种共享的是**Linux 内核中新出现的容器技术**（Google 给
Linux 容器技术贡献了大量代码），它能实现**延迟敏感型应用**和
**CPU 密集型批处理任务**之间的更好隔离。

### 1.1.4 自发的 Borg 生态

随着越来越多的应用部署到 Borg 上，我们的应用与基础设施团队开发了大量围绕 Borg
的管理工具和服务，功能包括：

* 配置或更新 job；
* 资源需求量预测；
* 动态下发配置到线上 jobs；
* 服务发现和负载均衡；
* 自动扩缩容；
* Node 生命周期管理；
* Quota 管理
* …

也就是产生了一个围绕 Borg 软件生态，但驱动这一生态发展的是 Google 内部的不同团队，
因此从结果来看，这个生态是一堆**异构、自发的工具和系统**（而非一个有设计的体系），
用户必须通过几种不同的配置语言和配置方式来和 Borg 交互。

虽然有这些问题，但由于其巨大的规模、出色的功能和极其的健壮性，Borg 当前仍然是
Google 内部主要的容器管理系统。

## 1.2 Omega

为了使 Borg 的生态系统更加符合软件工程规范，我们又开发了 Omega6，

### 1.2.1 架构更加整洁一致

Omega 继承了许多已经在 Borg 中经过验证的成功设计，但又是**完全从头开始开发**，
以便**架构更加整洁一致**。

* Omega 将**集群状态**存储在一个基于 Paxos 的中心式面向事务 store（数据存储）内；
* **控制平面组件**（例如调度器）都可以**直接访问这个 store**；
* 用**乐观并发控制**来处理偶发的访问冲突。

### 1.2.2 相比 Borg 的改进

这种解耦使得 **Borgmaster 的功能拆分为了几个彼此交互的组件**，
而不再是一个单体的、中心式的 master，修改和迭代更加方便。
Omega 的一些创新（包括多调度器）后来也反向引入到了 Borg。

## 1.3 Kubernetes

Google 开发的第三套容器管理系统叫 Kubernetes4。

### 1.3.1 为什么要设计 K8s

开发这套系统的背景：

* 全球越来越多的开发者也开始对 Linux 容器感兴趣，
* Google 已经把公有云基础设施作为一门业务在卖，且在持续增长；

因此与 Borg 和 Omega 不同的是：Kubernetes 是开源的，不是 Google 内部系统。

### 1.3.2 相比 Omega 的改进

* 与 Omega 类似，k8s 的核心也是一个**共享持久数据仓库**（store），
  几个组件会监听这个 store 里的 object 变化；
* Omega 将自己的 store 直接暴露给了受信任的控制平面组件，但 k8s 中的状态
  **只能通过一个 domain-specific REST API 访问**，这个 API 会执行
  higher-level versioning, validation, semantics, policy 等操作，支持多种不同类型的客户端；
* 更重要的是，k8s 在设计时就**非常注重应用开发者的体验**：
  首要设计目标就是在享受容器带来的资源利用率提升的同时，**让部署和管理复杂分布式系统更简单**。

## 1.4 小结

接下来的内容将介绍我们在设计和使用以上三代容器管理系统时学到的经验和教训。

# 2 底层 Linux 内核容器技术

容器管理系统属于上层管理和调度，在底层支撑整个系统的，是 Linux 内核的容器技术。

## 2.1 发展历史：从 `chroot` 到 cgroups

* 历史上，最初的容器只是提供了 **root file system 的隔离能力**
  （通过 **`chroot`**）；
* 后来 FreeBSD jails 将这个理念扩展到了对其他 namespaces（例如 PID）的隔离；
* Solaris 随后又做了一些前沿和拓展性的工作；
* 最后，Linux control groups (**`cgroups`**) 吸收了这些理念，成为集大成者。
  内核 cgroups 子系统今天仍然处于活跃开发中。

## 2.2 资源隔离

容器技术提供的资源隔离（resource isolation）能力，使 Google 的资源利用率远高于行业标准。
例如，Borg 能利用容器实现**延迟敏感型应用**和**CPU 密集型批处理任务**的混部（co-locate），
从而提升资源利用率，

* 业务用户为了应对**突发业务高峰**和做好 failover，
  通常申请的资源量要大于他们实际需要的资源量，这意味着大部分情况下都存在着资源浪费；
* 通过混部就能把这些资源充分利用起来，给批处理任务使用。

容器提供的**资源管理工具**使以上需求成为可能，再加上强大的内核**资源隔离技术**，
就能避免这两种类型任务的互相干扰。我们是**开发 Borg 的过程中，同步给 Linux 容器做这些技术增强**的。

但这种隔离并未达到完美的程度：容器无法避免那些不受内核管理的资源的干扰，例如三级缓存（L3 cache）、
内存带宽；此外，还需要对容器加一个安全层（例如虚拟机）才能避免公有云上各种各样的恶意攻击。

## 2.3 容器镜像

现代容器已经不仅仅是一种隔离机制了：还包括镜像 —— 将应用运行所需的所有文件打包成一个镜像。

在 Google，我们用 MPM (Midas Package Manager) 来构建和部署容器镜像。
隔离机制和 MPM packages 的关系，就像是 Docker daemon 和
Docker image registry 的关系。在本文接下来的内容中，我们所说的“容器”将包括这两方面，
即**运行时隔离和镜像**。

# 3 面向应用的基础设施

随着时间推移，我们意识到容器化的好处不只局限于提升资源利用率。

## 3.1 从“面向机器”到“面向应用”的转变

容器化使数据中心的观念从原来的**面向机器**（machine oriented）
转向了**面向应用**（application oriented），

* **容器封装了应用环境**（application environment），
  向应用开发者和部署基础设施**屏蔽了大量的操作系统和机器细节**，
* 每个设计良好的**容器和容器镜像都对应的是单个应用**，因此
  **管理容器其实就是在管理应用，而不再是管理机器**。

Management API 的这种从面向机器到面向应用的转变，显著提升了应用的部署效率和问题排查能力。

## 3.2 应用环境（application environment）

### 3.2.1 资源隔离 + 容器镜像：解耦应用和运行环境

资源隔离能力与容器镜像相结合，创造了一个全新的抽象：

* 内核 cgroup、chroot、namespace 等基础设施的最初目的是**保护应用免受 noisy、nosey、messy neighbors 的干扰**。
* 而这些技术**与容器镜像相结合**，创建了一个**新的抽象**，
  **将应用与它所运行的（异构）操作系统隔离开来**。

这种镜像和操作系统的解耦，使我们能在开发和生产环境提供相同的部署环境；
这种环境的一致性提升了部署可靠性，加速了部署。
这层抽象能**成功的关键**，是有一个*hermetic*（封闭的，不受外界影响的）容器镜像，

* 这个镜像能封装一个应用的几乎所有依赖（文件、函数库等等）；
* 那**唯一剩下的外部依赖就是 Linux 系统调用接口了** —— 虽然这组有限的接口极大提升了镜像的可移植性，
  但它并非完美：应用仍然可能通过 socket option、`/proc`、`ioctl` 参数等等产生很大的暴露面。
  我们希望 [Open Container Initiative](https://www.opencontainers.org)
  等工作可以进一步明确容器抽象的 surface area。

虽然存在不完美之处，但容器提供的资源隔离和依赖最小化特性，仍然使得它在 Google 内部非常成功，
因此容器成为了 Google 基础设施唯一支持的可运行实体。这带来的一个后果就是，
Google 内部只有很少几个版本的操作系统，也只需要很少的人来维护这些版本，
以及维护和升级服务器。

### 3.2.2 容器镜像实现方式

实现 hermetic image 有多种方式，

* 在 Borg 中，程序可执行文件在编译时会静态链接到公司托管的特定版本的库5；

  但实际上 Borg container image 并没有做到完全独立：所有应用共享一个所谓的
  *base image*，这个基础镜像是安装在每个 node 上的，而非打到每个镜像里去；
  由于这个基础镜像里包含了 `tar` `libc` 等基础工具和函数库，
  因此升级基础镜像时会影响已经在运行的容器（应用），偶尔会导致故障。
* Docker 和 ACI 这样的现代容器镜像在这方面做的更好一些，它们地消除了隐藏的 host OS 依赖，
  明确要求用户在容器间共享镜像时，必须显式指定这种依赖关系，这更接近我们理想中的 hermetic 镜像。

## 3.3 容器作为基本管理单元

围绕容器而非机器构建 management API，将数据中心的核心从机器转移到了应用，这带了了几方面好处：

1. 应用开发者和应用运维团队无需再关心机器和操作系统等底层细节；
2. 基础设施团队**引入新硬件和升级操作系统更加灵活**，
   可以最大限度减少对线上应用和应用开发者的影响；
3. 将收集到的 telemetry 数据（例如 CPU、memory usage 等 metrics）关联到应用而非机器，
   **显著提升了应用监控和可观测性**，尤其是在垂直扩容、
   机器故障或主动运维等需要迁移应用的场景。

### 3.3.1 通用 API 和自愈能力

容器能提供一些通用的 API 注册机制，使管理系统和应用之间无需知道彼此的实现细节就能交换有用信息。

* 在 Borg 中，这个 API 是一系列 attach 到容器的 HTTP endpoints。
  例如，`/healthz` endpoint 向 orchestrator 汇报应用状态，当检测到一个不健康的应用时，
  就会自动终止或重启对应的容器。这种**自愈能力**（self-he...