---
title: Exploiting CVE-2022-42703 - Bringing back the stack attack
url: https://googleprojectzero.blogspot.com/2022/12/exploiting-CVE-2022-42703-bringing-back-the-stack-attack.html
source: Project Zero
date: 2022-12-09
fetch_date: 2025-10-04T00:59:18.382739
---

# Exploiting CVE-2022-42703 - Bringing back the stack attack

# [Project Zero](https://googleprojectzero.blogspot.com/)

News and updates from the Project Zero team at Google

## Thursday, December 8, 2022

### Exploiting CVE-2022-42703 - Bringing back the stack attack

Seth Jenkins, Project Zero

This blog post details an exploit for [CVE-2022-42703](https://bugs.chromium.org/p/project-zero/issues/list?q=label:CVE-2022-42703) (P0 [issue 2351](https://bugs.chromium.org/p/project-zero/issues/detail?id=2351) - Fixed 5 September 2022), a bug Jann Horn found in the Linux kernel's memory management (MM) subsystem that leads to a use-after-free on struct anon\_vma. As the bug is very complex (I certainly struggle to understand it!), a future blog post will describe the bug in full. For the time being, the [issue tracker entry](https://bugs.chromium.org/p/project-zero/issues/detail?id=2351), [this LWN article](https://lwn.net/Articles/383162/) explaining what an anon\_vma is and [the commit](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=7a3ef208e662f) that introduced the bug are great resources in order to gain additional context.

## Setting the scene

Successfully triggering the underlying vulnerability causes folio->mapping to point to a freed anon\_vma object. Calling madvise(..., MADV\_PAGEOUT)can then be used to repeatedly trigger accesses to the freed anon\_vma in folio\_lock\_anon\_vma\_read():

struct anon\_vma \*folio\_lock\_anon\_vma\_read(struct folio \*folio,

  struct rmap\_walk\_control \*rwc)

{

struct anon\_vma \*anon\_vma = NULL;

struct anon\_vma \*root\_anon\_vma;

unsigned long anon\_mapping;

rcu\_read\_lock();

anon\_mapping = (unsigned long)READ\_ONCE(folio->mapping);

if ((anon\_mapping & PAGE\_MAPPING\_FLAGS) != PAGE\_MAPPING\_ANON)

goto out;

if (!folio\_mapped(folio))

goto out;

// anon\_vma is dangling pointer

anon\_vma = (struct anon\_vma \*) (anon\_mapping - PAGE\_MAPPING\_ANON);

// root\_anon\_vma is read from dangling pointer

root\_anon\_vma = READ\_ONCE(anon\_vma->root);

if (down\_read\_trylock(&root\_anon\_vma->rwsem)) {

[...]

if (!folio\_mapped(folio)) { // false

[...]

}

goto out;

}

if (rwc && rwc->try\_lock) { // true

anon\_vma = NULL;

rwc->contended = true;

goto out;

}

[...]

out:

rcu\_read\_unlock();

return anon\_vma; // return dangling pointer

}

One potential exploit technique is to let the function return the dangling anon\_vma pointer and try to make the subsequent operations do something useful. Instead, we chose to use the down\_read\_trylock() call within the function to corrupt memory at a chosen address, which we can do if we can control the root\_anon\_vma pointer that is read from the freed anon\_vma.

Controlling the root\_anon\_vma pointer means reclaiming the freed anon\_vma with attacker-controlled memory. struct anon\_vma structures are allocated from their own kmalloc cache, which means we cannot simply free one and reclaim it with a different object. Instead we cause the associated anon\_vma slab page to be returned back to the kernel page allocator by following a very similar strategy to the one documented [here](https://googleprojectzero.blogspot.com/2021/10/how-simple-linux-kernel-memory.html). By freeing all the anon\_vma objects on a slab page, then flushing the percpu slab page partial freelist, we can cause the virtual memory previously associated with the anon\_vma to be returned back to the page allocator. We then spray pipe buffers in order to reclaim the freed anon\_vma with attacker controlled memory.

At this point, we’ve discussed how to turn our use-after-free into a down\_read\_trylock() call on an attacker-controlled pointer. The implementation of down\_read\_trylock() is as follows:

struct rw\_semaphore {

atomic\_long\_t count;

atomic\_long\_t owner;

struct optimistic\_spin\_queue osq; /\* spinner MCS lock \*/

raw\_spinlock\_t wait\_lock;

struct list\_head wait\_list;

};

...

static inline int \_\_down\_read\_trylock(struct rw\_semaphore \*sem)

{

long tmp;

DEBUG\_RWSEMS\_WARN\_ON(sem->magic != sem, sem);

tmp = atomic\_long\_read(&sem->count);

while (!(tmp & RWSEM\_READ\_FAILED\_MASK)) {

if (atomic\_long\_try\_cmpxchg\_acquire(&sem->count, &tmp,

    tmp + RWSEM\_READER\_BIAS)) {

rwsem\_set\_reader\_owned(sem);

return 1;

}

}

return 0;

}

It was helpful to emulate the down\_read\_trylock() in unicorn to determine how it behaves when given different sem->count values. Assuming this code is operating on inert and unchanging memory, it will increment sem->count by 0x100 if the 3 least significant bits and the most significant bit are all unset. That means it is difficult to modify a kernel pointer and we cannot modify any non 8-byte aligned values (as they’ll have one or more of the bottom three bits set). Additionally, this semaphore is later unlocked, causing whatever write we perform to be reverted in the imminent future. Furthermore, at this point we don’t have an established strategy for determining the KASLR slide nor figuring out the addresses of any objects we might want to overwrite with our newfound primitive. It turns out that regardless of any randomization the kernel presently has in place, there’s a straightforward strategy for exploiting this bug even given such a constrained arbitrary write.

## Stack corruption…

On x86-64 Linux, when the CPU performs certain interrupts and exceptions, it will swap to a respective stack that is mapped to a static and non-randomized virtual address, with a different stack for the different exception types. A brief documentation of those stacks and their parent structure, the cpu\_entry\_area, can be found [here.](https://docs.kernel.org/x86/pti.html) These stacks are most often used on entry into the kernel from userland, but they’re used for exceptions that happen in kernel mode as well. We’ve recently seen [KCTF](https://google.github.io/kctf/vrp.html) entries where attackers take advantage of the non-randomized cpu\_entry\_area stacks in order to access data at a known virtual address in kernel accessible memory even in the presence of SMAP and KASLR. You could also use these stacks to forge attacker-controlled data at a known kernel virtual address. This works because the attacker task’s general purpose register contents are pushed directly onto this stack when the switch from userland to kernel mode occurs due to one of these exceptions. This also occurs when the kernel itself generates an Interrupt Stack Table exception and swaps to an exception stack - except in that case, kernel GPR’s are pushed instead. These pushed registers are later used to restore kernel state once the exception is handled. In the case of a userland triggered exception, register contents are restored from the task stack.

One example of an IST exception is a DB exception which can be triggered by an attacker via a hardware breakpoint, the associated registers of which are described [here](https://pdos.csail.mit.edu/6.828/2004/readings/i386/s12_02.htm). Hardware breakpoints can be triggered by a variety of different memory access types, namely reads, writes, and instruction fetches. These hardware breakpoints can be set using ptrace(2), and are preserved during kernel mode execution in a task context such as during a syscall. That means that it’s possible for an attacker-set hardware breakpoint to be triggered in kernel mode, e.g. during a copy\_to/from\_user call. The resulting exception will save and restore the kernel context via the aforementioned non-randomized exception stack, and that kernel context is an exceptionally good target for our arbitrary write primitive.

Any of the registers that copy\_to/from\_user is actively using at the time it handles the hardware breakpoint are corruptible by using our arbitrary-write primitive to overwrite their saved values on the exception stack. In this case, the size of the copy\_user call is the intuitive target. The size value is consistently stored in the rcx register, which will be saved at the sa...