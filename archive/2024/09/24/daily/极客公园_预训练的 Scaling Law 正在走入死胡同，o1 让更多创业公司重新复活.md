---
title: 预训练的 Scaling Law 正在走入死胡同，o1 让更多创业公司重新复活
url: https://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&mid=2653055361&idx=1&sn=1caeb13196fe2c4bd49a635be5874bfc&chksm=7e57143749209d21591b7dee29f47c9153a76aaa88c66f227f0bd36aaef301b26fd3054e4757&scene=58&subscene=0#rd
source: 极客公园
date: 2024-09-24
fetch_date: 2025-10-06T18:28:29.328497
---

# 预训练的 Scaling Law 正在走入死胡同，o1 让更多创业公司重新复活

![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cMxce4XUWzAt63HEH7H50gga905eZJicX3kmSdyH4c1zj19sNAWIU0iaA/0?wx_fmt=jpeg)

# 预训练的 Scaling Law 正在走入死胡同，o1 让更多创业公司重新复活

原创

今夜科技谈

极客公园

![](https://mmbiz.qpic.cn/mmbiz_jpg/8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cFZialLWbLqexEdHcS4iaVhvp7iaA7QWABcQoyavaBxibvyAtWLah1RPI2g/640?wx_fmt=jpeg&from=appmsg)

GPT-4o 读万卷书，「o1」行万里路。

**整理 | 宛辰****编辑**| 靖宇****

[北京时间 9 月 13 日凌晨，OpenAI 在官网发布了其最新一代模型，](https://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&mid=2653054485&idx=1&sn=4d0b5b5495ff57d36fe44d44260c0b88&scene=21#wechat_redirect)没有延续过去 GPT 系列的名称，新模型起名为 o1，当前可以获取 o1-Preview 和 o1-mini 这两个版本。

当天，Sam Altman 在社交平台上兴奋地称，「『o1』系列代表新范式的开始」。

但这可能是第一次，外界比 OpenAI 的掌舵人 Sam Altman 本人，更加兴奋地期待 OpenAI 的新品发布。这份期待里，无关对赛道第一名的艳羡，更多是同呼吸、共命运的决定性瞬间。下一代模型是否有惊人的进展？能否为 AGI 的浪潮和梦想完成信仰充值？

今年，你可能也对 AI 这个字眼麻木了，去年有多狂热，今年就有多麻木。原因无他，在 AI 的落地应用上，看不到信心二字。截止目前，仍未出现颠覆性的 AI 应用；Inflection.ai、Adept.ai、Character.AI 等最头部的明星公司接连被大厂纳入麾下；科技巨头们在财报周被反复拷问 AI 的巨额资本支出何时看到回报……

这些情绪背后，都指向同一个问题，那个所谓的第一性原理「Scaling Law」可以通向 AGI 吗？以今年十万卡、百亿美金投入，换取模型性能线性增长、乃至对数级增长的门槛来看，这注定是一场玩不起的游戏。不少人开始质疑它的合理性，这波 AI 不会就这样了吧？

这是「o1」诞生的时代性。

在 OpenAI 交出答卷后，AI 创业者表示「又行了」。不同于预训练的 Scaling Law，一条在推理阶段注入强化学习的路径成为明确的技术新方向，徐徐展开。

极客公园「今夜科技谈」直播间也在第一时间邀请极客公园创始人 & 总裁张鹏，和创新工场联合首席执行官/管理合伙人汪华、昆仑万维首席科学家&2050 全球研究院院长颜水成，一起聊了聊 o1 所代表的新范式及创业者脚下的路。

![](https://mmbiz.qpic.cn/mmbiz_png/8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4c1RTg9JwNBw0icENBXNUYI1N51O4GQCyTgQPaatXvATgVbotbHFUBNSw/640?wx_fmt=png&from=appmsg)

以下是直播沉淀文字，由极客公园整理。

**01**

****「o1」释放了明确的技术信号，****

****但更期待下一个里程碑****

**张鹏：从去年传出「Q\*项目」到现在，****OpenAI****的强推理模型「o1 系列」终于发布了。实际用下来，「o1」的发布符合你们的预期效果吗？**

**颜水成**：我用 o1 做的第一件事情是，把我女儿做的数学题输进去看结果，o1 的表现令人惊喜。它解题的逻辑顺序、总结的 CoT（Chain of Thoughts，思维链）信息，让人觉得很不一般。

如果是**用 GPT-4 或 GPT-4o，只是做下一个 token（词元）的预测，其实我们心里会打鼓、会怀疑：只是做下一个词元的预测，是不是就能实现复杂推理过程。**

**但 o1 相当于在回答问题之前，先引入用 CoT（思维链）表示的思考过程，把复杂问题先用 planning（规划）的方式将任务拆解，再根据规划的结果一步步细化，最后把所有结果做总结，才得到最终结果。**

一个模型的好与不好，关键在于它是不是直觉上能解决问题。GPT-4 和 GPT-4o 还是一种快思考，这种快思考不太适合解决复杂推理问题；但是 o1 是一种慢思考的过程，像人一样思考，更可能解决一个问题，尤其是跟数学、编程或者逻辑有关的问题。o1 所代表的技术路径未来会走得非常远，带来非常大的想象空间。

**汪华：**我觉得 o1 是一个非常好的工作，水到渠成，符合预期。符合预期是说这个时间点该有成果了，为更高的未来预期打开了通路，但并不 surprise，没有超出预期。

因为这个工作其实从去年就已经有一系列的线索，包括 OpenAI、DeepMind 出的一系列的论文像 Let』s Verify Step by Step (OpenAI, 2023)，以及其他像 Quiet-STaR 和 in-contest reinforce learning 中都有迹可循。

大家用强化学习、包括用合成数据去串 Reward Model（奖励模型）或 Critic Model（评判模型），或者后来**用各种各样结构化的推理来提高模型正确率。事实上，无论是 OpenAI、Meta，还是其他大厂，大家现在都已经在做类似的工作，这个方向其实是大家的一个共识。**

不光 OpenAI，很多其他模型在数学、编程、推理上都已经有了很大进步，就是因为或多或少用了一部分这方面的技术，但 OpenAI 发布的 o1 是集大成，并且工作做得非常好，而且里面应该有它独特的工程探索。

![](https://mmbiz.qpic.cn/mmbiz_jpg/8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cM1cGGAZic5zkzwicrjGpSKUlQDdcc2MGpQicah87p7h12rdZBNrSHtRlw/640?wx_fmt=jpeg&from=appmsg)图片来源：OpenAI 官网

**张鹏：预期之内，但还不够惊喜。**

**汪华：**对，整个框架还是在预期范围之内，没有像 GPT-4 或者 GPT-3.5 发布一样带来很大的惊喜。

你会发现 **o1 针对推理等各方面性能的增强，还是在一些有明确对与错和封闭结果的领域。**比如 o1 展现的代码、学术解题，包括数据分析能力其实都属于有明确信号的领域。

哪怕是在明确领域，比如数学编程的问题，它在做得好的问题上表现非常好，但在一些问题上也做得不太好。也就是说，可能它在训练 Critic Model（评判模型）或者 Reward Model（奖励模型）的时候，对于下游任务的泛化，可能还是遵循物理规律。如果对下游任务覆盖得好，它就做得好；如果覆盖得不好、下游任务没见过这些数据，或者 reward model 没法很好地给予 reward 的时候，它泛化也不一定真的能泛化过去，所以从这个角度来讲，o1 没有特别的超出常识的部分。

我还测了一些更加通用推理的场景，在这些领域，o1 增强得还不太多，很多也没有带来增强的效果。

**实际上对 OpenAI 抱持更高的期待是，希望它下一步能做到，把推理泛化到通用领域。**

当然现在端出这么一个非常完善的工作，把这件事给做出来，OpenAI 这点还是非常厉害的。而且在跟 OpenAI 的同学聊天时，能感觉到他们在做更难的事情，朝着通用推理的方向在做，只是可能现在还不成熟，所以先放出来对于 signal（技术信号）更明显的阶段性成果，在代码、数学方面的工作。所以我也非常期待，什么时候 OpenAI 能把下一个里程碑也克服了。

**02**

****强化学习不新鲜，****

****「o1」在用强化学习上有创新****

**张鹏：o1 已经能在一些领域展现出复杂推理的能力，其中很重要的原因是，****强化学习****在 o1 系列模型里扮演了非常重要的作用。怎么理解强化学习在新一代模型里起的作用？**

**颜水成：****强化学习是一个存在时间蛮长的方向**，把这个技术用得最好的团队应该是谷歌 DeepMind，他们一开始就是从这个角度出发，去解决真实世界的实际问题。

**我个人觉得强化学习在 o1 里最核心的点，不在于使用强化学习，因为强化学习在 GPT 3.5 里就已经用了**PPO（一种强化学习算法），用一个奖励函数去指导 PPO，进而优化模型参数。

强化学习优化一个描述长期累计 rewards 的目标函数，而原先传统算法只是求解损失函数。相当于，在优化 policy action（策略动作）的时候，需要考虑未来所有奖励的总和。

具体来说，像在围棋博弈中，它会用 self-play（自我博弈）的形式去收集 action-status 序列，这个过程自动生成一个奖励值，而不是说去学一个奖励函数。它是直接自动产生出奖励，或者说人工可以定义奖励，用这些奖励就可以把策略学出来，然后逐步提升策略。它最大的特点是整个过程不需要人类干预，不是像 RLHF（根据人类反馈的强化学习），有很多的步骤需要人去反馈。

我觉得其实 **o1 跟原来的强化学习有一个最本质的差别。**有人认为，o1 的原理可能与斯坦福大学团队 (E Zelikman et al, 2024) 发表的 Quiet-STaR 研究成果最相关。Quiet- STaR 的一个特点是从 CoT（思维链）的角度出发，但是 **CoT 并不是一开始就存在**。

要做推理问题，原本有最初的文本存在，如果在文本里面再插入一些 CoT 的信息，它就能提升推理效果。

但当我们希望去解决通用的、复杂的推理问题时，大部分的情况下 CoT 是不存在的。那么在强化学习的 pipeline（流程管道）里面，如何把这些 CoT 的信息一步一步生成出来是非常困难的。

这就要问 o1 的模型架构是什么？是一个模型它既可以去做规划，又可以根据规划去生成 CoT，又可以做自我反思（self-reflection），又可以做验证，最后做一个总结，这些所有的事情。还是说其实是好几个模型，一个模型根据信息生成 CoT，另外一个模型做反馈，两个模型相互交互，逐步把结果生成。目前 o1 还不是特别清楚，两种可能都能做，单一模型可能会让整个过程更优雅。第二种可能实现起来会更容易一些。

**如何用合适的方式把 CoT（思维链）生成，我觉得这是 o1，跟其他的强化学习区别最大的地方。**这里的细节还不是很清楚，如果清楚的话，o1 的黑盒问题可能就解决了。

**张鹏：怎么把****强化学习****运用到这一代推理模型里？是一个单体的超级智能、还是一个集体决策，这些还没有被公开。**

**颜水成：**上一代的强化学习，可能更像下围棋，通过别人已有的棋局，先学了一些东西以后再接着往前走。我觉得要做通用、复杂推理的话，它就会碰到很多从零开始（zero start），可能一开始根本没有 CoT 的数据，这种情况大概怎么去做学习，有待探索。

![](https://mmbiz.qpic.cn/mmbiz_jpg/8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cWdM2IJVwHLRibZArdw3wXicq7t6hEEmeAxvBoYhMwPjwIPFr6CdqW0ug/640?wx_fmt=jpeg&from=appmsg)2015 年，DeepMind 推出了 AlphaGo，这是第一个击败围棋世界冠军的计算机程序，通过强化学习，其后继者 AlphaZero 和 MuZero 基于自我对弈与强化学习的方式，变得越来越通用，能够解决许多不同的游戏以及复杂的现实世界问题，从压缩视频到发现新的更高效的计算机算法。｜图片来源：DeepMind

**张鹏：为什么把****强化学习****放到模型里，成为接下来发展的共识？这个共识是怎么达成的？核心都是要解决什么样的问题？**

**汪华：**技术上有颜老师在。从商业角度，大家还是在讨论模型的智能上限这样一个问题。

举个例子，哪怕是一个员工的应用场景，小学生能干的工种，跟中学生、大学生能干的工种，差别还是非常大的。所以**模型的幻觉，或者说模型的复杂指令遵循能力、模型的长链路规划和推理能力，已经制约了模型的进一步商业化，哪怕我不是为了实现 AGI（通用人工智能）。**

所以大家早就已经有这个说法了，一开始就有「系统-1」「系统-2」的说法（快思考和慢思考）。基本上预训练相当于知识的压缩，它本身就跟人的直觉一样，没法进行复杂的推理，所以必然要找到一个方法来实现「系统-2」。

在实现「系统-2」的时候，用各种各样的结构化推理，包括用各种各样的强化学习，有一个正好的规划，更稳定的模型输出，更好的指令遵循，包括让模型不光是学会知识本身，包括按什么样的 pipeline（流程管道）去使用知识。比如人类在解决问题 A 时会用思维框架一，解决问题 B 时会用思维框架二。像这些问题怎么来做？大家手里的武器库，其实除了 LLM，就是强化学习。

而且我特别同意颜老师刚刚的说法，**具体实现上用了一个模型还是两个模型，只是一个工程问题，但 CoT 的数据从哪里来？包括怎么来实现一些真实世界的模拟和对抗，这个反而是大家一直在试图攻克的难点。**代码和数学之所以能被很快地解决，是因为它的信号非常明确，对就是对，错就是错，而且它的步骤合成，合成它的推理 CoT 数据其实是相对比较容易的，奖励或者 Critics（评判）也是相对比较明晰。

**颜水成：**就相当于说奖励能直接获得。

**汪华：**更难的就是代码和数学之外，**世界上那种复杂的、复合的，甚至开放结果的，没有明确的、绝对对错的，甚至没有唯一执行路径的这些问题怎么办。**我觉得把这个问题给解了，难度要比一个模型和两个模型其实要难得多。

**颜水成：**o1 这个框架里面我觉得应该还是有一个奖励函数存在的，不然就没办法推演到通用的复杂推理。

**03**

****「o1」发展下去，****

****更接近一个「超级智能体」****

**张鹏：o1 跟跟此前的 GPT 系列相比，是两个技术方向，可以这么理解吗？**

**颜水成：**对，**o****1 表现出来的行为不再是下一个 token 的预测了，而更像是一个超级智能体的样子**，未来可以处理多模态、可以处理工具，可以处理存储记忆，包括短期和长期的语义记忆。

![](https://mmbiz.qpic.cn/mmbiz_jpg/8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cucrqc9VHicicNEV4xp9PELRqPY23fqC3RJUpZgasXhnzslBjKjWYHsrg/640?wx_fmt=jpeg&from=appmsg)《思考，快与慢》，诺贝尔经济学奖得主丹尼尔·卡尼曼经典之作，介绍了大脑的两种思维系统：系统 1 快速直觉、系统 2 缓慢理性｜图片来源：视觉中国

我个人是认为 o1 这个技术方向肯定是对的，从 GPT-4 到 o1 的话，其实就是从「系统-1」到「系统-2」的一个转变。今年 5 月我做过一个演讲，AGI 的终局可能是什么东西，当时提到了两个概念，一个概念叫做 Global Workspace（全局工作空间），一个叫超级智能体。

Global Workspace（全局工作空间）在心理学和神经科学领域里的一个理论，是说大脑里除了专用的子系统，比如视觉、语音，触觉等子系统之外，可能还存在一个区域叫做 Global Workspace。

如果「系统-2」，就是多步和多模型的形式一起来完成的话，现在 CoT（思维链）产生的结果，它非常像 Global Workspace 的工作原理。用一个注意力的模型，把文本的、未来多模态的、工具等信息都拉到这个空间，同时也把你的目标和存储的记忆（memory）都拿到这个空间里进行推理，尝试新的策略、再做验证、尝试新的可能性……不停的往前推理，演绎的结果就是最终得到分析的结果。推理时间越长，就相当于在 Global Workspace 里的推演过程越长，最终得到的结果也会越好。

对于复杂的任务无法用「系统-1」（快思考）一竿子到底，就用「系统-2」（慢思考）的 Global Workspace，把信息逐步分解、推演，同时又动态地去获取工具，动态地去获取存储记忆，最后做总结，得到最后的结果。

**所以我觉得 o1 发展下去，可能就是「系统-2」（慢思考）的 Global Workspace 的 AI 实现方式，如果用 AI 的语言来描述的话，其实它就像是一个超级智能体。也就是说，o1 发展下去，可能就是一个超级智能体。**

**04**

****LLM+RL 的模式，****

****是否可以通向泛化推理？****

**汪华：强化学习相关的共识其实很早就有，但大家一直也没解决好问题。当年强化学习也很火，还被视作 AGI 的一个通路，包括机器人领域也都是用强化学习，但当时就遇到了这个难题：对于非常明确的任务，奖励函数很好建、任务的模拟器也很好建；但一旦扩展到真实世界的泛化任务时，就没法泛化，或者没法建立能完整模拟真实世界各种各样、复杂奇怪的任务模拟器，也没法去建立对它很好的奖励函数。**

**您觉得按现在这条 LLM（大语言模型）加上 Reinforcement Learning（强化学习）的模式，不止是在有明确信号的领域比如代码、数学，如果要往泛化推理走的话，要怎么走？**

**颜水成：**一个最大的差别就是，原来的强化学习，它的泛化性能不好。每次可能是专门针对一个游戏、或者一组类似的游戏去学一个策略。但是**现在它要做通用的复杂推理，面对所有问题都要有能产生 CoT 的能力，这就会变成是一个巨大数据的问题和工程的问题。**

我非常认同汪华的观点，在数学、编程、科学这些问题上，可能比较容易去造一些新的 CoT 数据，但是有一些领域，想要无中生有地生成这些 CoT 数据，难度非常高，或者说还解决得不好。

要解决泛化的问题，数据就要足够多样，但**在通用场景的推理泛化问题上，这种 CoT 的数据到底怎么生成？**

或者也有可能根本就没有必要，因为那个问题可能已经解决得很好了，你再加 CoT 可能也没有意义，比如说在有一些问题上，可能感觉 o1 没有带来本质的效果提升，可能因为那种问题本来就已经解决得非常不错了。

**张鹏：强化学习在下一代的模型里要扮演更重要的作用，会带来什么影响？**

**汪华：**如果大规模采纳这个方案，算力会更短缺，推理会变得更重要。

因为之前说推理成本将来会...