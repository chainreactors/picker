---
title: 全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应
url: https://www.anquanke.com/post/id/287967
source: 安全客-有思想的安全新媒体
date: 2023-03-31
fetch_date: 2025-10-04T11:11:04.257995
---

# 全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应

首页

阅读

* [安全资讯](https://www.anquanke.com/news)
* [安全知识](https://www.anquanke.com/knowledge)
* [安全工具](https://www.anquanke.com/tool)

活动

社区

学院

安全导航

内容精选

* [专栏](/column/index.html)
* [精选专题](https://www.anquanke.com/subject-list)
* [安全KER季刊](https://www.anquanke.com/discovery)
* [360网络安全周报](https://www.anquanke.com/week-list)

# 全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应

阅读量**320837**

发布时间 : 2023-03-30 09:00:40

**x**

##### 译文声明

本文是翻译文章，文章原作者 OpenAI&TheVerge&Techcrunch，文章来源：https://www.odaily.news/post/5185753

译文仅供参考，具体内容表达以及含义原文为准。

GPT-4 可以接受图像和文本输入，而 GPT-3.5 只接受文本。

GPT-4 在各种专业和学术基准上的表现达到 “人类水平”。例如，它通过了模拟的律师考试，分数约为应试者的前 10% 。

OpenAI 花了 6 个月的时间，利用从对抗性测试项目以及 ChatGPT 中获得的经验，反复调整 GPT-4 ，结果在事实性、可引导性和可控制方面取得了 “史上最佳结果”。

在简单的聊天中，GPT-3.5 和 GPT-4 之间的区别可能微不足道，但是当任务的复杂性达到足够的阈值时，区别就出来了，GPT-4 比 GPT-3.5 更可靠，更有创造力，能够处理更细微的指令。

GPT-4 能对相对复杂的图像进行说明和解释，比如说，从插入 iPhone 的图片中识别出一个 Lightning Cable 适配器（下文有图片）。

图像理解能力还没有向所有 OpenAI 的客户开发，OpenAI 正在与合作伙伴 Be My Eyes 进行测试。

OpenAI 承认，GPT-4 并不完美，仍然会对事实验证的问题产生错乱感，也会犯一些推理错误，偶尔过度自信。

开源 OpenAI Evals, 用于创建和运行评估 GPT-4 等模型的基准，同时逐个样本检查其性能。

## 官宣文档

OpenAI 已经正式推出 GPT-4 ，这也是 OpenAI 在扩大深度学习方面的最新里程碑。GPT-4 是大型的多模态模型（能够接受图像和文本类型的输入，给出文本输出），尽管 GPT-4 在许多现实世界的场景中能力不如人类，但它可以在各种专业和学术基准上，表现出近似人类水平的性能。

例如：GPT-4 通过了模拟的律师考试，分数约为全部应试者的前 10% 。而相比之下，GPT-3.5 的分数大约是后 10% 。我们团队花了 6 个月的时间，利用我对抗性测试项目以及基于 ChatGPT 的相关经验，反复对 GPT-4 进行调整。结果是，GPT-4 在事实性（factuality）、可引导性（steerability）和拒绝超范围解答（非合规）问题（refusing to go outside of guardrails.）方面取得了有史以来最好的结果（尽管它还不够完美）

在过去两年里，我们重构了整个深度学习堆栈，并与 Azure 合作，为工作负荷从头开始，共同设计了一台超级计算机。一年前，OpenAI 训练了 GPT-3.5 ，作为整个系统的首次 “试运行”，具体来说，我们发现并修复了一些错误，并改进了之前的理论基础。因此，我们的 GPT-4 训练、运行（自信地说：至少对我们来说是这样！）空前稳定，成为我们首个训练性能可以进行提前准确预测的大模型。随着我们继续专注于可靠扩展，中级目标是磨方法，以帮助 OpenAI 能够持续提前预测未来，并且为未来做好准备，我们认为这一点，对安全至关重要。

我们正在通过 ChatGPT 和 API（您可以加入 WaitList）发布 GPT-4 的文本输入功能，为了能够更大范围地提供图像输入功能，我们正在与合作伙伴紧密合作，以形成一个不错的开端。我们计划开源 OpenAI Evals，也是我们自动评估 AI 模型性能的框架，任何人都可以提出我们模型中的不足之处，以帮助它的进一步的改进。

## 能力

在简单闲聊时，也许不太好发现 GPT-3.5 和 GPT-4 之间的区别。但是，当任务的复杂性达到足够的阈值时，它们的区别就出来了。具体来说，GPT-4 比 GPT-3.5 更可靠，更有创造力，能够处理更细微的指令。

为了理解这两个模型之间的差异，我们在各种不同的基准上进行了测试，包括模拟最开始那些为人类设计的考试。通过使用最新的公开测试（就奥数和 AP 等等考试）还包括购买 2022-2023 年版的练习考试来进行，我们没有为这类考试给模型做专门的培训，当然，考试中存在很少的问题是模型在训练过程中存在的，但我们认为下列结果是有代表性的。

![全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应]()![全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应]()

我们还在为机器学习模型设计的传统基准上，对 GPT-4 进行了评估。GPT-4 大大超过现有的大语言模型，与多数最先进的（SOTA）模型并驾齐驱，这些模型包括针对基准的制作或额外的训练协议。

![全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应]()

由于现有的大多数 ML 基准是用英语编写的，为了初步了解其他语言的能力，我们使用 Azure Translate 将 MMLU 基准：一套涵盖 57 个主题的 14000 个选择题，翻译成了各种语言。在测试的 26 种语言中的 24 种语言中，GPT-4 的表现优于 GPT-3.5 和其他大模型（Chinchilla，PaLM）的英语表现，这种优秀表现还包括类似拉脱维亚语、威尔士语和斯瓦希里语等等。

![全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应]()

我们一直在内部使用 GPT-4 ，发现它对支持、销售、内容审核和编程等功能会产生很大影响，我们还在用它来协助人类评估 AI 的输出，这就是我们调整战略的第二阶段的开始。

## 视觉输入

GPT-4 可以接受文本和图像的提示语（prompt），这与纯文本设置平行。比如说，可以让用户指定任何视觉或语言任务，它可以生成文本输出（自然语言、代码等），给定的输入包括带有文字和照片的文件、图表或屏幕截图，GPT-4 表现出与纯文本输入类似的能力。此外，还可以应用在为纯文本语言模型开发的测试时间技术，包括少数几个镜头和 CoT 的 Prompting，不过目前图像输入仍然属于研究方面预览，没有像 C 端公开产品。

下列图片显示了一个 “Lightning Cable “适配器的包装，有三个面板。

![全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应]()![全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应]()

面板 1 ：一个带有 VGA 接口（通常用于电脑显示器的大型蓝色 15 针接口）的智能手机插在其充电端口。

面板 2 ：”Lightning Cable “适配器的包装上有一张 VGA 接口的图片。

面板 3 ：VGA 连接器的特写，末端是一个小的 Lightning 连接器（用于为 iPhone 和其他苹果设备充电）。

这张图片的搞笑性质来自于将一个大的、过时的 VGA 连接器插入一个小的、现代的智能手机充电端口.. 因此看起来很荒谬

通过在一套狭窄的标准学术视觉基准上，对 GPT-4 的性能进行评估，并且对它进行预览。然而，这些数字并不能代表其的能力范围，因为我们发现，这个模型能够处理很多的新的和令人兴奋的任务，OpenAI 计划很快发布进一步的分析和评估数字，以及对测试时间技术效果的彻底调查结果。

## 可控制的 AI

我们一直在努力实现关于定义 AI 行为那篇文章中，所概述的计划的每个方面，包括 AI 的可控制性。与经典的 ChatGPT 个性的固定言语、语气和风格不同，开发者（很快就是所有的 ChatGPT 用户）现在可以通过在 “系统 “消息中描述这些方向，来规定自己的 AI 的风格和任务。系统消息允许 API 用户在范围内，大幅对用户体验进行定制，我们将持续改进。

## 局限性

尽管能力惊人，不过，GPT-4 仍存在与早期 GPT 模型类似的限制。最重要的是，它仍然不是完全可靠的（比如说，它会对事实产生 “幻觉”，并出现推理错误）。在使用语言模型的输出时，特别是在高风险的情况下，应该非常小心谨慎，比如说：需要人类审查，完全避免高风险的使用）以及需要与特定的使用案例的需求相匹配。

尽管各类情况仍然存在，但相较于以前的模型（这些模型本身也在不断改进），GPT-4 大大减少了 hallucinations（意思是网络错觉，这里指的是一本正经的胡说八道）。在我们内部的对抗性事实性评估中，GPT-4 的得分比我们最新推出的 GPT-3.5 高 40% 。

![全面解读OpenAI推出的多模态GPT-4：准确性提高，支持微软新款必应]()

### 可控制的 AI

GPT-4 的基础模型在这项任务中只比 GPT-3.5 略胜一筹；然而，在经过 RLHF 的后期训练后（应用我们对 GPT-3.5 使用的相同过程），却有很大差距。该模型在其输出中会有各种偏差，我们在这些方面已经取得了进展，但仍有更多工作要做。根据我们最近的博文，我们的目标是使我们建立的人工智能系统具有合理的默认行为，以反映广泛的用户价值观，允许这些系统在广泛的范围内被定制，并获得公众对这些范围的意见。

GPT-4 通常缺乏对其绝大部分数据截止后（2021 年 9 月）发生的事件的了解，也不会从其经验中学习。它有时会犯一些简单的推理错误，这似乎与这么多领域的能力不相符，或者过于轻信用户的明显虚假陈述。有时它也会像人类一样在困难的问题上失败，例如在它产生的代码中引入安全漏洞。GPT-4 也可能在预测中自信地犯错。

## 风险和缓解措施

我们一直在对 GPT-4 进行迭代，使其从训练开始就更加安全，保持一致性，我们所做的努力包括预训练数据的选择和过滤、评估，邀请专家参与，对模型安全改进、监测，以及执行。

GPT-4 与过去的模型会存在类似风险，如生产有害的建议、错误代码或不准确的信息。然而，GPT-4 的额外能力还导致了新的风险面。为了明确这些风险的具体情况，我们聘请了 50 多位来自人工智能对接风险、网络安全、生物风险、信任和安全以及国际安全等领域的专家对该模型进行对抗性测试。他们的参与，使我们能够测试模型在高风险领域的行为，这些领域需要专业知识来评估。来自这些领域专家的反馈和数据，为我们缓解和改进模型提供了依据。比如说，我们已经收集了额外的数据，以提高 GPT-4 拒绝有关如何合成危险化学品的请求的能力。

GPT-4 在 RLHF 训练中加入了一个额外的安全奖励信号，通过训练模型来拒绝对此类内容的请求，从而减少有害产出（由我们的使用指南定义）。奖励是由 GPT-4 的分类器提供的，它能够判断安全边界和安全相关提示的完成方式。为了防止模型拒绝有效的请求，我们从不同的来源（例如，标记的生产数据，人类的红队，模型生成的提示）收集多样化的数据集，并在允许和不允许的类别上应用安全奖励信号（存在正值或负值）。

与 GPT-3.5 相比，我们的缓解措施大大改善了 GPT-4 的许多安全性能。与 GPT-3.5 相比，我们将模型对非法内容的请求的响应倾向，降低了 82% ，而 GPT-4 对敏感请求（如医疗建议和自我伤害）的响应符合我们的政策的频率提高了 29%

总的来说，我们的模型级干预措施增加了诱发不良行为的难度，但仍然存在 “越狱 “的情况，以产生违反我们使用指南的内容。随着人工智能系统的 风险的增加，在这些干预措施中实现极高的可靠性将变得至关重要。目前重要的是，用部署时间的安全技术来补充这些限制，如想办法监测。

GPT-4 和后续模型，很有可能对社会产生正面或者负面的影响，我们正在与外部研究人员合作，以改善我们对潜在影响的理解和评估，以及建立对未来系统中可能出现的危险能力的评估。我们将很快分享我们对 GPT-4 和其他人工智能系统的潜在社会和经济影响的更多思考。

## 训练过程

和之前的 GPT 模型一样，GPT-4 基础模型的训练是为了预测文档中的下一个单词，并使用公开的数据（如互联网数据）以及我们授权的数据进行训练。这些数据是来自于极大规模的语料库，包括数学问题的正确和错误的解决方案，弱的和强的推理，自相矛盾的和一致的声明，以及种类繁多的意识形态和想法。

因此，当被提示有一个问题时，基础模型可以以各种各样的方式作出反应，而这些反应可能与用户的意图相去甚远。为了使其与用户的意图保持一致，我们使用人类反馈的强化学习（RLHF）对模型的行为进行微调。

注意，模型的能力似乎主要来自于预训练过程，RLHF 并不能提高考试成绩（如果不主动努力，它实际上会降低考试成绩）。但是对模型的引导来自于训练后的过程–基础模型需要及时的工程，甚至知道它应该回答问题。

## 可预测的扩展

GPT-4 项目的一大重点是建立一个可预测扩展的深度学习栈。主要原因是，对于像 GPT-4 这样非常大的训练运行，做大量的特定模型调整是不可行的。我们对基础设施进行了开发和优化，在多种规模下都有非常可预测的行为。为了验证这种可扩展性，我们提前准确地预测了 GPT-4 在我们内部代码库（不属于训练集）中的最终损失，方法是通过使用相同的方法训练的模型进行推断，但使用的计算量要少 10000 倍。

我们认为，准确预测未来的机器学习能力是安全的一个重要部分，相对于其潜在的影响，它没有得到足够的重视（尽管我们已经被几个机构的努力所鼓舞）。我们正在扩大我们的努力，开发一些方法，为社会提供更好的指导，让人们了解对未来系统的期望，我们希望这成为该领域的一个共同目标。

## 开放式人工智能评估

我们正在开源 OpenAI Evals，这是我们的软件框架，用于创建和运行评估 GPT-4 等模型的基准，同时逐个样本检查其性能。我们使用 Evals 来指导我们模型的开发（包括识别缺点和防止退步），我们的用户可以应用它来跟踪不同模型版本（现在将定期推出）和不断发展的产品集成的性能。例如，Stripe 已经使用 Evals 来补充他们的人工评估，以衡量他们的 GPT 驱动的文档工具的准确性。

因为代码都是开源的，Evals 支持编写新的类来实现自定义的评估逻辑。然而，根据我们自己的经验，许多基准都遵循一些 “模板 “中的一个，所以我们也包括了内部最有用的模板（包括一个 “模型分级 Evals “的模板–我们发现 GPT-4 有令人惊讶的能力来检查自己的工作）。一般来说，建立一个新的评估的最有效方法是将这些模板中的一个实例化，并提供数据。我们很高兴看到其他人能用这些模板和 Evals 更广泛地建立什么。

我们希望 Evals 成为一个分享和众包基准的工具，最大限度地代表广泛的故障模式和困难任务。作为后续的例子，我们已经创建了一个逻辑谜题评估，其中包含 GPT-4 失败的十个提示。Evals 也与实现现有的基准兼容；我们已经包括了几个实现学术基准的笔记本和一些整合 CoQA（小的子集）的变化作为例子。

我们邀请大家使用 Evals 来测试我们的模型，并提交最有趣的例子。我们相信 Evals 将成为使用和建立在我们的模型之上的过程中不可或缺的一部分，我们欢迎直接贡献、问题和反馈。

## ChatGPT Plus

ChatGPT Plus 用户将在 chat.openai.com 上获得有使用上限的 GPT-4 权限。我们将根据实际需求和系统性能调整确切的使用上限，但我们预计容量将受到严重限制（尽管我们将在接下来的几个月里扩大和优化）。

根据我们看到的流量模式，我们可能会为更高的 GPT-4 使用量引入一个新的订阅级别，我们也希望在某个时候提供一定数量的免费 GPT-4 查询，这样那些没有订阅的用户也可以尝试。

### API

要获得 GPT-4 的 API（使用与 gpt-3.5-turbo 相同的 ChatCompletions API），请可以去 OpenAI 的官方 Waitlist 上注册。

## 结论

我们期待着 GPT-4 成为一个有价值的工具，通过为许多应用提供动力来改善人们的生活。还有很多工作要做，我们期待着通过社区的集体努力，在这个模型的基础上进行建设、探索和贡献，共同对模型进行改进。

参考文献：

1.https://openai.com/research/gpt-4

2.https://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-is-state-of-the-art/

3.https://www.theverge.com/2023/3/14/23638033/openai-gpt-4-chatgpt-multimodal-deep-learning

本文翻译自https://www.odaily.news/post/5185753 原文链接。如若转载请注明出处。

商务合作，文章发布请联系 anquanke@360.cn

本文由**安全客**原创发布

转载，请参考[转载声明](https://www.anquanke.com/note/repost)，注明出处： [https://www.anquanke.com/post/id/287967](/post/id/287967)

安全KER - 有思想的安全新媒体

本文转载自: https://www.odaily.news/post/5185753

如若转载,请注明出处：

安全KER - 有思想的安全新媒体

分享到：![微信](https://p0.ssl.qhimg.com/sdm/28_28_100/t01e29062a5dcd13c10.png)

* [GPT-4](/tag/GPT-4)

**+1**23赞

收藏

![](https://p0.ssl.qhimg.com/t01546a096e83e700fe.jpg)安全客

分享到：![微信](https://p0.ssl.qhimg.com/sdm/28_28_100/t01e29062a5dcd13c10.png)

## 发表评论

您还未登录，请先登录。

[登录](/login/index.html)

![](https://p1.ssl.qhimg.com/t01a1ab830955b940c...