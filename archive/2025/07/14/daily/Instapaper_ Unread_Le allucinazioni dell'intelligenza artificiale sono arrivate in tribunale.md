---
title: Le allucinazioni dell'intelligenza artificiale sono arrivate in tribunale
url: https://www.wired.it/article/allucinazioni-intelligenza-artificiale-errori-giustizia-tribunali/
source: Instapaper: Unread
date: 2025-07-14
fetch_date: 2025-10-06T23:26:34.587940
---

# Le allucinazioni dell'intelligenza artificiale sono arrivate in tribunale

[Skip to main content](#main-content)

Apri il menu di navigazione

Menu

[![Wired Italia](/verso/static/wired-us/assets/logo-header.svg)](/)

Le allucinazioni dell'intelligenza artificiale sono arrivate in tribunale

* [Scienza](/scienza/)
* [Economia](/economia/)
* [Cultura](/cultura/)
* [Gadget](/gadget/)
* [Security](/security/)
* [Diritti](/diritti/)
* [Idee](/idee/)
* [Video](/video/)
* [Podcast](/podcast-wired/)
* [Wired Consiglia](/wired-consiglia/)

More*Chevron*

[Cerca

Cerca](/search/)

* [Scienza](/scienza/)
* [Economia](/economia/)
* [Cultura](/cultura/)
* [Gadget](/gadget/)
* [Security](/security/)
* [Diritti](/diritti/)
* [Idee](/idee/)
* [Video](/video/)
* [Podcast](/podcast-wired/)
* [Wired Consiglia](/wired-consiglia/)

[Laura Carrer](/author/lcarrer/)

[Security](/security/)

10.07.2025

# Le allucinazioni dell'intelligenza artificiale sono arrivate in tribunale

Perché sempre più legali fanno ricorso all'AI a sostegno del loro lavoro. Ma chi non controlla si ritrova con precedenti inventati e leggi strampalate. Un ricercatore sta raccogliendo i casi emersi nei tribunali internazionali. I paesi più colpiti? Stati Uniti e Israele

![L'immagine di una personificazione della giustizia](https://media-assets.wired.it/photos/65c503177aba588db3e7703a/16:9/w_2560%2Cc_limit/1401545074)

L'immagine di una personificazione della giustiziaPaul Campbell/Getty Images

Negli ultimi due anni **i casi di allucinazioni dell'intelligenza artificiale emerse nei tribunali internazionali sono stati quasi 90**. A raccoglierli è Damien Charlotin, studioso di diritto empirico che, tra le altre cose, sviluppa strumenti basati sull’[intelligenza artificiale](https://www.wired.it/article/claude-4-test-ricatto-intelligenza-artificiale-coscienza/) per supportare l’argomentazione giuridica (le basi su cui prende le sue decisioni un giudice, ad esempio). Come spesso accade nella ricerca, si è imbattuto in nel fenomeno attuale ma ancora poco esplorato delle allucinazioni dell'intelligenza artificiale.

“*Non riuscivo a trovare alcun dato, così ho deciso di raccoglierli da solo*”, ha raccontato a *Wired* Charlotin. Dando uno sguardo al database si nota subito che la maggior parte dei casi raccolti riguarda **avvocati o cittadini che scelgono di difendersi da soli in [tribunale](https://www.wired.it/article/app-processo-penale-telematico-non-funziona-problemi-tribunali/)** (*pro se litigants*, ndr).“*Probabilmente i non avvocati cadono più facilmente nelle falsità plausibili fornite dai chatbot,*” osserva Charlotin, “*perché non dispongono delle risorse né dei metodi tipici di uno studio legale strutturato, dove la verifica delle citazioni è di norma parte integrante della prassi argomentativa*”.

[Nel database](https://www.damiencharlotin.com/hallucinations/) Charlotin raccoglie e **classifica i casi per paese** (Stati Uniti e [Israele](https://www.wired.it/article/guerra-cyber-israele-iran-attacchi-informatici/) in testa), **identificando l’autore dell’errore** (che può essere un avvocato, un esperto o una persona che si auto-rappresenta in tribunale) e le conseguenze a cui è andato incontro. Nella maggior parte dei casi si tratta di **semplici avvertimenti**, ma non mancano le sanzioni pecuniarie. In un episodio avvenuto nel [Regno Unito](https://www.wired.it/article/eta-regno-unito-voli-viaggi-app-domanda/), in cui una persona che si auto-rappresentava in tribunale ha portato al giudice almeno 25 precedenti inesistenti, la corte ha chiesto **24.727 sterline di indennità**. Un avvocato israeliano è stato condannato a pagare **1.500 euro di spese personali allo Stato e 3.500 euro alla controparte** per aver portato in una causa familiare almeno cinque decisioni inesistenti, con numeri di fascicoli fittizi e citazioni inventate.

Negli [Stati Uniti](https://www.wired.it/article/stati-uniti-attacco-iran-siti-nucleari/) invece, la corte ha ritenuto che un esperto avesse compromesso in modo irreparabile la propria credibilità e lo ha dunque escluso dal processo. Oltre all’aspetto legato ai precedenti, quindi strutturale nei sistemi di *common law* (anglosassoni perlopiù), secondo Charlotin il motivo per cui molti dei casi si registrino negli Stati Uniti è legato al fatto che “*la quantità di open data è molto maggiore rispetto all’Europa*”. All’inizio di giugno due giudici dell'Alta Corte di Inghilterra e Galles [hanno avvertito gli avvocati](https://www.nytimes.com/2025/06/06/world/europe/england-high-court-ai.html) che potrebbero essere perseguiti penalmente in caso di utilizzo di materiale falso generato dall'intelligenza artificiale. L’avvertimento è arrivato a seguito dell’analisi – da parte dei giudici – di due casi recenti in cui è stato utilizzato materiale falso nelle argomentazioni legali scritte presentate in tribunale.

## Perché le allucinazioni trovano terreno fertile negli studi legali

I casi continuano ad aumentare a ritmo sostenuto: solo nel maggio 2025, Charlotin ne ha registrati 35. E poiché il suo database include esclusivamente episodi che hanno portato a una decisione formale o a un commento da parte di un giudice, è inevitabile pensare che **l’universo reale delle allucinazioni sia molto più vasto**.

Ci si potrebbe poi aspettare che **[ChatGPT](https://www.wired.it/article/chatgpt-cervello-effetti-studio/) sia protagonista della maggior parte dei casi,** vista la sua popolarità presso il grande pubblico e quindi anche tra coloro che scelgono di auto-rappresentarsi in tribunale. Ma nel database compaiono frequentemente anche altri nomi: **[Claude](https://www.wired.it/article/claude-integrazione-google-workspace/), Gemini, Copilot, ChatOn, e così via**. A dimostrazione, se mai ce ne fosse ancora bisogno, che tutti i modelli linguistici, per definizione, possono generare allucinazioni. “*Attenuare questo problema richiede competenze specifiche, e molte persone non si esercitano abbastanza per ottenere il meglio da questi strumenti*”, osserva Charlotin.

“*Non considero il fenomeno del tutto nuovo. Il copia-incolla di citazioni giurisprudenziali, spinte fino al punto in cui perdono rilevanza rispetto al caso originario, è da tempo una prassi consolidata nella professione legale*” dice il ricercatore.

“*Del resto, '**questo precedente non significa ciò che tutti pensano che significhi'*** *è quasi un luogo comune nella dottrina giuridica. La differenza, oggi, è che con i chatbot si è oltrepassato un limite: non c’è più nemmeno una connessione formale con il punto di vista giuridico, perché la fonte normativa citata semplicemente non esiste. Ma questo, più che un problema legato all’[intelligenza artificiale](https://www.wired.it/article/wired-edicola-essere-umani-artificiali/), evidenzia un **limite strutturale**: quello dell’argomentazione basata su una fonte autorevole, quando diventa cieca e automatica*”.

Quando si tratta di precedenti giurisprudenziali infatti, i modelli linguistici di grandi dimensioni sono addestrati su grandi quantità di citazioni che possono essere **replicate fittiziamente con una buona dose di credibilità**: in un paper scientifico in cui lo stesso Charlotin spiega il fenomeno, il ricercatore fa riferimento a un caso fittizio intitolato “*Smith v. United States (1992)*” che non suona meno credibile di un caso reale.

In sostanza, nel contesto giuridico i [chatbot](https://www.wired.it/article/chatbot-personalizzati-gratis-chatgpt/) ottimizzano la coerenza linguistica che, a volte, può sostituire la finzione ai fatti. Il ricercatore spiega poi come questa sia una tecnologia incentivata anche dal modello di business prevalente in molti studi legali. È infatti prassi abbastanza diffusa che una parte significativa della ricerca legale venga affidata ad avvocati junior, spesso sottoposti a forti pressioni in termini di tempo e costi. A questi ultimi è richiesto di **svolgere un lavoro testuale molto lungo e cruciale**, anche se poi la responsabilità del prodotto finale e della reputazione professionale ad esso collegata ...