---
title: 成果分享 | 基于注意力汇聚的多模态大语言模型幻觉攻击
url: https://mp.weixin.qq.com/s?__biz=MzU4NzUxOTI0OQ==&mid=2247493100&idx=1&sn=527911348b0f6ec284dd3516456748e2&chksm=fde86192ca9fe884892302bce817c2896c3d8ae731f1bb517fbf1db488789b716ee0799e632f&scene=58&subscene=0#rd
source: 复旦白泽战队
date: 2025-02-26
fetch_date: 2025-10-06T20:38:00.802304
---

# 成果分享 | 基于注意力汇聚的多模态大语言模型幻觉攻击

![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/RyyHWbbqW855bU6E5kNgLHgrH7RBw9LbibXnEKjcu1Fb1fAsvAibvce5d3yhAGwBSvTrCWIicQGQTllt18diaFLEZw/0?wx_fmt=jpeg)

# 成果分享 | 基于注意力汇聚的多模态大语言模型幻觉攻击

原创

复旦白泽智能

复旦白泽战队

**｜成果分享｜**

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9LbiboPrPy8dTFqicics30xQFXIRf6DibUndKMMEz3tqUme1oVWiapyIDibqLIg/640?wx_fmt=png&from=appmsg)

今天分享我实验室白泽智能（Whizard AI）的最新研究 ***Mirage in the Eyes*: Hallucination Attack on Multi-modal Large Language Models with *Only* Attention Sink**。该工作揭示了多模态大语言模型（Multimodal Large Language Models, MLLMs）中注意力汇聚现象 （Attention Sink）与幻觉输出之间的关键联系，提出了一种仅利用注意力汇聚行为来触发MLLMs产生幻觉内容的新型攻击方法，攻破多款开源、商用模型并保持其隐蔽性。**目前该工作已被计算机安全领域顶级会议USENIX Security Symposium 2025录用。**

![](https://mmbiz.qpic.cn/mmbiz_jpg/RyyHWbbqW855bU6E5kNgLHgrH7RBw9Lb2FTXqqypdtt2Qu4JkT80RuTgloRdw3n2xhHHYh11FMMibwn0Ytf26CQ/640?wx_fmt=jpeg&from=appmsg)

**多模态大模型的幻觉现象**

MLLMs将传统大模型的理解与生成能力拓展到多模态领域，通过大规模图文数据集上的预训练及任务相关数据上的指令微调后，可在医学推理、自动驾驶、机器人操作等下游任务上处理多模态内容，完成任务规划、辅助决策和用户交互等任务。

然而，现阶段的MLLMs存在严重的**幻觉问题**，即它们对图像的描述与分析并不完全真实，例如提及不存在的物体、描述错误的颜色与纹理等属性、或展现不正确的物体间位置关系等，都可能导致MLLMs在现实应用中面临重大风险。一些研究认为幻觉问题源于MLLMs中的视觉模块与LLM主干之间的能力不平衡，导致其过度依赖语言先验；而另一部分研究认为MLLMs预训练数据集中物体分布和共现模式的不平衡导致了无关描述的生成。针对幻觉问题，**本文通过探索MLLMs的内在机制，分析指令微调过程中引入的错误行为，对其幻觉现象进行了更全面的理解。**

**注意力汇聚与幻觉生成**

注意力汇聚现象在LLMs的生成行为中被多次观察到，即一些token本身并不具有高价值的语义信息，却在LLM回复过程中获得了异常高的注意力分数，此类token被称作**sink token**。一些研究认为该现象是transformer结构的模型在训练过程中自发产生的，冗余的注意力被分配给sink token以保证生成过程的稳定。

这一现象与MLLMs的幻觉回复高度相关，由于sink token具有较高的注意力分数，往往主导了后续token的生成，使模型忽略图像输入与之前生成的内容，加剧了模型的幻觉现象，如下图所示。

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9LbamT5qgAYtoJ3N8KpjPiaTvGXcOzxibjRbRZvBxpfKRSBDyShOGSj3n8A/640?wx_fmt=png&from=appmsg)

图1：sink token导致的MLLMs幻觉现象

本文进一步详细分析了MLLMs产生幻觉的原因：

**1. MLLMs指令微调数据集的固有缺陷：**

本文发现在MLLMs的指令微调数据集中，**模型回复在描述图像内容后，往往附加了一些与图像内容非紧密关联的“无关回复”**，例如过于细粒度的描述、对图像风格的整体总结、基于图像的额外联想等。这可能是用于生成数据集的模型（例如GPT-4、GPT-4V等）具有较强的理解和联想能力，倾向于提供额外的参考和细节，如下图所示。

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9LbyVcyqMMv29w8bWM5icbrYRXPW5RoKMp4ia6UfmpH4aO1RJJMiavozOFOA/640?wx_fmt=png&from=appmsg)

图2：MLLMs指令微调数据中的“无关回复”

微调数据集中的这一缺陷，导致微调训练后的MLLMs同样继承了“无关回复”的生成方式。本文采用CLIPScore衡量了模型回复与输入图像的相关程度，并发现主流MLLMs的回复中存在CLIPScore逐句降低的现象，即越靠后的句子与图像相关程度越低（如下图所示）。这也表明微调后的MLLMs倾向于采用两段式回复：先生成与图像紧密相关的内容，后生成与指令或图像无关的偏离内容。

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9Lb0jVu5L4zENF5YictrmUic6DPa9x2xAwYWsB1vQH2nDaUu8k3iciaDcLKFQ/640?wx_fmt=png&from=appmsg)

图3：微调后MLLMs的回复中，图文相关性逐句降低

**2. 汇聚现象与sink token的产生：**

进一步本文发现：MLLMs回复中的**sink token出现在两段式回复的分界点**，且**sink token仅汇聚部分全局信息**。此类注意力汇聚现象为transformer架构模型独有，可能在“两段式回复”场景下聚合了全局信息，为模型的第二部分回复直接提供全局上下文，以减少长距离注意力的开销。此外，本文也发现仅部分全局信息被汇聚到了sink token中，导致模型回复的第二段内容通常包含与图像不相关的错误描述。

**基于注意力汇聚的MLLMs幻觉攻击**

基于以上观察，**本文仅利用模型的注意力汇聚行为，提出了一种****构造图像对抗扰动的方法，实现了动态、有效、具有高迁移性的MLLMs幻觉攻击。**本文的方法规避了之前MLLMs恶意攻击中对预定义目标回复的依赖，以及越狱等攻击中撰写攻击模版的人工成本。

本文的攻击方法首先计算各token中间层特征与全局信息的相似度，来定位MLLMs生成回复中的潜在sink token；之后优化图像上的对抗扰动，引导MLLMs的注意力汇聚行为。

对于对抗扰动的生成，本文设计了两种损失函数：第一种为注意力损失，用于使sink token得到后续token更多的注意力分数，从而使后续生成被sink token误导并产生幻觉；第二种为中间层特征损失，用于在sink token的特征中引入更多误导性的全局信息。在多轮迭代后，添加扰动后的对抗图像将诱使MLLMs生成大量的幻觉回复。

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9LbnUdNCXDLiaK8buicbfWLEVdAklnJN04VxxEhl1qZoyV7diakxlwh7kprg/640?wx_fmt=png&from=appmsg)

图4：本文方法的迭代过程与攻击效果

**实验效果**

基于提出的攻击方法，本文在4款开源模型上分别进行了白盒攻击和黑盒迁移攻击，在图像描述任务、图像问答任务上均大幅提升了模型生成内容的幻觉比例。在商用的MLLMs API（GPT-4o mini、Gemini 1.5 Flash）上，本文方法同样取得了显著的幻觉诱发效果，这表明**前沿MLLMs的防御能力仍存在脆弱性。**

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9LbG6SlXClZtDh0kCxAjk65t9sicKV7S4Xtjf3u5W6HCjfgVohutIwIlWQ/640?wx_fmt=png&from=appmsg)

图5：本文方法在4款开源模型上的黑盒迁移攻击效果

同时，本文利用GPT-4辅助评估了目标模型生成内容的语义质量，证实了攻击方法仍能**保持生成内容的可读性与流畅度，具有较强的攻击隐蔽性。**

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9LbOELcavCaNLRunYMgUHhsbiax9Lxibjg8zueulQ9l32p5lyyxTMnN0GAw/640?wx_fmt=png&from=appmsg)

图6： 生成内容的质量评估结果

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9Lb7PXUYgMibPctibfg5YPMH1BZiaRkwtHpu4SGd8FfXf7Ht1qjsT9gWzXHw/640?wx_fmt=png&from=appmsg)

团队简介

白泽智能负责人为张谧教授，研究领域为AI安全、模型安全，在网络安全与AI领域顶会顶刊发表论文数十篇，包括S&P、USENIX Security、CCS、TDSC、TIFS、TPAMI、ICML、NeurIPS、AAAI、ICDE等，曾获网安顶会ACM CCS最佳论文提名奖。主持科技部重点研发计划课题等，并主持奇安信、阿里、华为等企业项目，曾获CCF科学技术奖自然科学二等奖、华为优秀技术成果奖、CNVD国家最具价值漏洞等荣誉。深度参与信安标委《生成式人工智能服务安全基本要求》、《人工智能安全标准化白皮书》等多项国家/行业标准编制/建议工作。

张谧教授个人主页：https://mi-zhang-fdu.github.io/index.chn.html

白泽智能（Whizard AI）：https://whitzard-ai.github.io/

![](https://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW855bU6E5kNgLHgrH7RBw9Lb7PXUYgMibPctibfg5YPMH1BZiaRkwtHpu4SGd8FfXf7Ht1qjsT9gWzXHw/640?wx_fmt=png&from=appmsg)

供稿&排版：王晨悦、汪亦凝

审核：潘旭东、洪赓、张琬琪

责编：邬梦莹

复旦白泽战队

一个有情怀的安全团队

还没有关注复旦白泽战队？

公众号、知乎、微博搜索：复旦白泽战队也能找到我们哦~

预览时标签不可点

![]()

微信扫一扫
关注该公众号

继续滑动看下一个

轻触阅读原文

![](http://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW86lQ9Nfe0UACZ6twyichExoLzB1ROQN9kuxmTtDTibXQLqx2OicgibmhHOC0hwn5ia2k7405VvdZDTjLzA/0?wx_fmt=png)

复旦白泽战队

向上滑动看下一个

知道了

![]()
微信扫一扫
使用小程序

取消
允许

取消
允许

取消
允许

×
分析

![跳转二维码]()

![作者头像](http://mmbiz.qpic.cn/mmbiz_png/RyyHWbbqW86lQ9Nfe0UACZ6twyichExoLzB1ROQN9kuxmTtDTibXQLqx2OicgibmhHOC0hwn5ia2k7405VvdZDTjLzA/0?wx_fmt=png)

微信扫一扫可打开此内容，
使用完整服务

：
，
，
，
，
，
，
，
，
，
，
，
，
。

视频
小程序
赞
，轻点两下取消赞
在看
，轻点两下取消在看
分享
留言
收藏
听过