---
title: 基于树模型对恶意代码的静态分析方法
url: https://forum.butian.net/share/4304
source: 奇安信攻防社区
date: 2025-05-01
fetch_date: 2025-10-06T22:24:34.951421
---

# 基于树模型对恶意代码的静态分析方法

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 基于树模型对恶意代码的静态分析方法

你将学到什么？​​
✅ ​​决策树的数学基础​​：信息增益 vs. 基尼系数 vs. 增益率，如何影响模型表现？
✅ ​​Bagging vs. Boosting​​：为什么随机森林能并行，而XGBoost必须串行训练？
✅ ​​XGBoost的工程优化​​：二阶泰勒展开、正则化、特征重要性如何让预测更精准？
✅ ​​AST（抽象语法树）实战​​：如何把PHP代码转换成机器学习可用的数值特征？
✅ ​​调参技巧与评估指标​​：如何用网格搜索和Fβ分数平衡准确率与召回率？

树模型
===
一、前言
----
听说XGBoost好用？
这么好用，那就干，开学！！
二、决策树
-----
阐述：将一个集合中的数据，按照不同特征不断划分，形成一颗树；树的叶子节点表示样本所属的类别。如果是回归树，可以把叶子节点的值求平均作为样本的输出结果值。
说白了就是二叉树，每个节点会做一次判断，叶节点即为最终分类的结果
那怎么做判断呢，很简单
常用的方法有信息增益（ID3）、信息增益率（C4.5）、基尼系数（CART），计算方法分别为
信息增益（ID3）：即为熵
![oaRqCmcA9OB84TW.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-0591663389541da5380293b2f28aac7b80bc75c2.png)
信息增益率（C4.5）： 信息增益比率
基尼系数（CART）： p\\*（1-p）
这里来解释一下信息增益（ID3）
```php
举例来说，假设S是一个关于布尔概念的有14个样例的集合，它包括9个正例和5个反例（我们采用记号[9+，5-]来概括这样的数据样例），那么S相对于这个布尔样例的熵为：
Entropy（[9+，5-]）=-（9/14）log2（9/14）-（5/14）log2（5/14）=0.940。
So，根据上述这个公式，我们可以得到：
如果S的所有成员属于同一类，则Entropy(S)=0；
如果S的正反样例数量相等，则Entropy(S)=1；
如果S的正反样例数量不等，则熵介于0，1之间（如下图所示）
```
![B3olLk1i6QT5gzY.jpg](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-30487ba5ece40441e2de9c789446181cba9111c7.jpg)
可以看到数据量越靠近中间的值，熵越大，不确定性越高
三、集成学习(Bagging vs Boosting)
---------------------------
所谓集成学习，即先使用多个弱分类器对数据进行预测（效果一般不好），然后采用某些策略将所有弱分类器得到的结果集成起来，做为最终的预测结果，通俗比喻就是“三个臭皮匠赛过诸葛亮”
### bagging
每个分类器见到不同的样本，样本权重相同，训练出不同效果的分类器，分类器相互独立，然后将这一系列的分类器组合起来使用
\*\*类比一下就是物理学上，弱分类器看成是元器件，所有元器件串联\*\*
### boosting
每个分类器见到相同的样本，但每个分类器样本权重不同，并且分类器之间相互依赖。核心问题： 1） 如何确定样本权重 2） 如何组合各个分类器
\*\*类比一下就是并联\*\*
### bagging vs. boosting
- 训练样本 有放回抽样
- 是否能并行，分类器训练过程是否独立
- boosting 一般要比 bagging的效果好，
- Boosting流派，各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT(Gradient Boosting Decision Tree)、Xgboost
- Bagging流派，各分类器之间没有依赖关系，可各自并行，比如随机森林（Random Forest）
四、Adaboost
----------
这里就不详细讲解，他不是主角,贴上大佬的解释
```php
AdaBoost，是英文"Adaptive Boosting"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。
具体说来，整个Adaboost 迭代算法就3步：
1.初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
2.训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
3.将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。
```
五、GBoost
--------
说到Xgboost，不得不先从GBDT(Gradient Boosting Decision Tree)说起。因为xgboost本质上还是一个GBDT，但是力争把速度和效率发挥到极致，所以叫X (Extreme) GBoosted
与AdaBoost不同，GBoost每一次的计算是都为了减少上一次的残差，进而在残差减少（负梯度）的方向上建立一个新的模型
还是不懂的话，扒了一张图
![E9NrxfbjTCYktpz.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-062cd5637a9804c5e9ab88982b7411a838ece037.png)
后面的树的输入是 总数据和前面树输出之差
看一个大佬举的例子就知道啦
```php
它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；
接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；
接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；
最后在第四课树中用1岁拟合剩下的残差，完美。
```
还有个例子：
现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。
我们会得到如下图所示结果：
![vrHs5yWi6njG4Tt.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-6da04b055c474ee025d3e78cabe9f871e1e3c7fb.png)
在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为左右两拨，每拨用平均年龄作为预测值。
此时计算残差（残差的意思就是：A的实际值 - A的预测值 = A的残差），所以A的残差就是实际值14 - 预测值15 = 残差值-1。
注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值。
残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。“残差”蕴含了有关模型基本假设的重要信息。如果回归模型正确的话， 我们可以将残差看作误差的观测值。
进而得到A,B,C,D的残差分别为-1,1，-1,1。
然后拿它们的残差-1、1、-1、1代替A B C D的原值，到第二棵树去学习，第二棵树只有两个值1和-1，直接分成两个节点，即A和C分在左边，B和D分在右边，经过计算（比如A，实际值-1 - 预测值-1 = 残差0，比如C，实际值-1 - 预测值-1 = 0），此时所有人的残差都是0。
残差值都为0，相当于第二棵树的预测值和它们的实际值相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了，即每个人都得到了真实的预测值。
换句话说，现在A,B,C,D的预测值都和真实年龄一致了。Perfect！
A: 14岁高一学生，购物较少，经常问学长问题，预测年龄A = 15 – 1 = 14
B: 16岁高三学生，购物较少，经常被学弟问问题，预测年龄B = 15 + 1 = 16
C: 24岁应届毕业生，购物较多，经常问师兄问题，预测年龄C = 25 – 1 = 24
D: 26岁工作两年员工，购物较多，经常被师弟问问题，预测年龄D = 25 + 1 = 26
原文链接：[https://blog.csdn.net/v\\_JULY\\_v/article/details/81410574](https://blog.csdn.net/v\_JULY\_v/article/details/81410574)
六、XGBoost
---------
终于来到XGBoost了，由于不是数学科班出生，这里的数学原理就不做过多阐述，以免画蛇添足，之后会贴上大佬的链接，注意，要求数学原理要大致看懂，搞懂算法实现代码部分的底层逻辑
学疏才浅，本来想自己举个例子的，发现还是原作者的例子好用
引用自xgboost原作者陈天奇的讲义PPT中
举个例子，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。
![xiOqkQoFN5bBuSc.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-294362987cff18a7dc7da747892b5fdad03b1aaf.png)
训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。具体如下图所示
![yR36PjMVGXtYSE2.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-ea8a649966412774cad27b72d9fdb4823e50475f.png)
### xgboost目标函数
![6oQpKtDRj5NvJX9.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-226bc63eea5067ce6404700d06d74a52f6134488.png)
误差/损失函数鼓励我们的模型尽量去拟合训练数据，使得最后的模型会有比较少的 bias。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。
这两者是对抗关系，矛盾矛盾
XGBoost和gboost最大的区别就是是用了二阶导（由泰勒公式推算而来）
### 矛：误差函数
误差函数中的i表示第i个样本，l(y^i-yi)表示第`i`个样本的预测误差，我们的目标当然是误差越小越好。
类似之前GBDT的套路，xgboost也是需要将多棵树的得分累加得到最终的预测得分
![5CGoW1ihptS8RUP.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-2fdf7570d88e31be1007f43e0ab59892e653523b.png)
### 盾：正则化项
正则项是为了防止模型过拟合的
这里的正则依据有： 一个是树里面叶子节点的个数
这是因为叶子节点过多，就可以说明树分叉太多了，如果不加以限制的话会伸出巨大的枝叶，可以完全契合数据，导致过拟合，这是模型训练的大忌
七、\*\*分裂节点\*\*
----------
很有意思的一个事是，我们从头到尾了解了xgboost如何优化、如何计算，但树到底长啥样，我们却一直没看到。很显然，一棵树的生成是由一个节点一分为二，然后不断分裂最终形成为整棵树。那么树怎么分裂的就成为了接下来我们要探讨的关键。
看了很多大佬的文章，都是直接用的贪心算法
从树深度0开始，每一节点都遍历所有的特征，比如年龄、性别等等，然后对于某个特征，\*\*先按照该特征里的值进行排序，然后线性扫描该特征进而确定最好的分割点\*\*，最后对所有特征进行分割后，我们选择所谓的增益Gain最高的那个特征
说人话就是指针走到每个点时把所有的可以出现的情况都遍历一遍找出最好的情况，之后往下走，重复这个过程，白话就是，鼠目寸光，只看到眼前的利益（肯定有更好的方法等你发现）
而Gain如何计算呢？
![ReMszDB5W34kUjO.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-1d3c2cd7e1b09c4a0c1f3eefa557fa3f7687abb0.png)
举例：
比如总共五个人，按年龄排好序后，一开始我们总共有如下4种划分方法：
把第一个人和后面四个人划分开
把前两个人和后面三个人划分开
把前三个人和后面两个人划分开
把前面四个人和后面一个人划分开
接下来，把上面4种划分方法全都各自计算一下Gain，看哪种划分方法得到的Gain值最大则选取哪种划分方法，经过计算，发现把第2种划分方法“前面两个人和后面三个人划分开”得到的Gain值最大，意味着在一分为二这个第一层层面上这种划分方法是最合适的。
八、代码实现bagging
-------------
### Bagging策略
- 首先对训练数据集进行多次采样，保证每次得到的采样数据都是不同的
- 分别训练多个模型，例如树模型
- 预测时需得到所有模型结果再进行集成
![6pz8cUQdRwBYuk1.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-940ae3dc1a3d8744bd18dda8e84ce092ad3cd3eb.png)
对比一下bagging集成方法和传统方法
```py
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model\_selection import train\_test\_split
from sklearn.datasets import make\_moons
from sklearn.metrics import accuracy\_score
X,y = make\_moons(n\_samples=500, noise=0.30, random\_state=42)
X\_train, X\_test, y\_train, y\_test = train\_test\_split(X, y, random\_state=42)
bag\_clf = BaggingClassifier(DecisionTreeClassifier(),
n\_estimators = 500,
max\_samples = 100,
bootstrap = True,
n\_jobs = -1,
random\_state = 42
)
bag\_clf.fit(X\_train,y\_train)
y\_pred = bag\_clf.predict(X\_test)
accuracy\_score(y\_test,y\_pred)
print(accuracy\_score(y\_test,y\_pred))
tree\_clf = DecisionTreeClassifier(random\_state = 42)
tree\_clf.fit(X\_train,y\_train)
y\_pred\_tree = tree\_clf.predict(X\_test)
accuracy\_score(y\_test,y\_pred\_tree)
print(accuracy\_score(y\_test,y\_pred\_tree))
```
![VASrWYRb8U6t54z.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-977870fb363ba33eba3287e1c49953fcf9789440.png)
可以看到集成算法正确率更高
在写一个画图函数，更加直观看到
```py
#定义画图函数
def plot\_decision\_boun...