---
title: No, un’intelligenza artificiale militare non ha deciso di uccidere l’operatore che le impediva di completare la propria missione. Però...
url: http://attivissimo.blogspot.com/2023/06/no-unintelligenza-artificiale-militare.html
source: Il Disinformatico
date: 2023-06-04
fetch_date: 2025-10-04T11:46:26.958444
---

# No, un’intelligenza artificiale militare non ha deciso di uccidere l’operatore che le impediva di completare la propria missione. Però...

# [Il Disinformatico](https://attivissimo.blogspot.com/)

Un blog di Paolo Attivissimo, giornalista informatico e cacciatore di bufale

**Informativa privacy e cookie:** Questo blog include cookie di terze parti. Non miei ([dettagli](https://tinyurl.com/2p9apfu5))

[Prossimi eventi pubblici](https://attivissimo.me/disinformaticalendario/prossimi/) – [Donazioni](https://attivissimo.me/donazioni/) – [Sci-Fi Universe](https://scifiuniverse.it)

## Cerca nel blog

|  |  |
| --- | --- |
|  |  |

## 2023/06/03

### No, un’intelligenza artificiale militare non ha deciso di uccidere l’operatore che le impediva di completare la propria missione. Però...

[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnLakumAD2tYRXNSiV89pqiBDl7kJKvkjOkrmR2-yz3bTpMVLkVCjyX5g3VRds52lRx93UbR4UOATX9PGgeTgNhQ3OBU5RBWBRC7ZVoT3FeFe8XW-trycoJJKvXZlLS7_WJ6paJWrEK9oXPc0dEq7n5JkjwfJLQtzKB8z7_GPvBDYrpopBmOQ/s320/Screenshot%202023-06-03%20at%2014.35.20.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnLakumAD2tYRXNSiV89pqiBDl7kJKvkjOkrmR2-yz3bTpMVLkVCjyX5g3VRds52lRx93UbR4UOATX9PGgeTgNhQ3OBU5RBWBRC7ZVoT3FeFe8XW-trycoJJKvXZlLS7_WJ6paJWrEK9oXPc0dEq7n5JkjwfJLQtzKB8z7_GPvBDYrpopBmOQ/s777/Screenshot%202023-06-03%20at%2014.35.20.png)

*Ultimo aggiornamento: 2023/06/08 10:50. Questo articolo è disponibile anche in [versione podcast](https://attivissimo.blogspot.com/2023/06/podcast-rsi-strane-mail-dagli.html).*

Ieri (2 giugno) è partita l’ennesima frenesia mediatica secondo la quale una
intelligenza artificiale, durante un’esercitazione, avrebbe ucciso il proprio
operatore umano perché non le permetteva di completare la propria missione.

Prima di tutto, non è morto nessuno. Secondo, non c’è stata nessuna esercitazione del
genere. Si tratta solo di uno scenario *ipotetico*, che è stato
presentato maldestramente e quindi è stato frainteso perché la storia era
ghiotta. Ma c’è comunque una riflessione molto importante da fare a proposito
di tutte le applicazioni dell’intelligenza artificiale.

Alcuni siti (come
*[Il Post](https://www.ilpost.it/2023/06/02/intelligenza-artificiale-drone-uccide-operatore-simulazione-scenario)*) hanno già fatto un ottimo lavoro di demistificazione, citando anche i
titoli sensazionalisti e irresponsabili di molta stampa internazionale. La
notizia è partita dal sito Aerosociety.com, che ha riportato una sintesi delle
relazioni presentate a una conferenza sulle tecnologie militari prossime
venture *[il Future Combat Air and Space Capabilities Summit]* tenutasi a
Londra a fine maggio scorso e piena di spunti interessantissimi anche
lasciando da parte il clamore di questa notizia specifica.

Aerosociety ha attribuito *[[qui](https://www.aerosociety.com/news/highlights-from-the-raes-future-combat-air-space-capabilities-summit/); [copia permanente](https://archive.is/eQOQM)]* al colonnello
Tucker ‘Cinco’ Hamilton, capo dei collaudi delle operazioni di intelligenza
artificiale *[chief of AI test and operations]* dell’Aeronautica militare
statunitense, una descrizione di un test *simulato* nel quale un drone gestito
tramite intelligenza artificiale avrebbe avuto il compito di identificare e
distruggere delle postazioni di missili terra-aria, aspettando
l’autorizzazione finale da parte di un operatore umano. Ma siccome all’IA era
stata data la direttiva primaria di distruggere quelle postazioni, il software
sarebbe arrivato alla conclusione che l’operatore era un ostacolo al
compimento della propria missione e quindi avrebbe deciso di eliminarlo.
Successivamente sarebbe stato insegnato all’IA che *no*, uccidere l’operatore non
andava bene; e quindi il software avrebbe elaborato una nuova strategia:
distruggere l’impianto di comunicazioni attraverso il quale arrivavano gli
ordini di interrompere le missioni.

Ma se si va a leggere l’articolo originale è chiaro sin da subito che si
tratta di una
*simulazione* *[“simulated test”]*, non di una esercitazione reale.
E se non ci si ferma al paragrafo che tutti hanno citato *[quello evidenziato
qui sotto in grassetto]* ma si legge tutto quanto l’articolo, il contesto è abbastanza
evidente: il colonnello Hamilton stava mettendo in guardia contro l’eccesso di
fiducia nell’IA, che secondo lui è *“facile da ingannare”* e soprattutto
*“crea strategie altamente inattese per raggiungere il proprio obiettivo”*.

Anche il paragrafo finale dell’articolo originale spiega che siamo nel campo delle
ipotesi sviluppate a titolo *preventivo*, visto che cita un altro relatore, il
tenente colonnello Brown, anche lui dell’Aeronautica militare statunitense,
che ha parlato del proprio lavoro, che è consistito nel creare una serie di
scenari
*“per informare i decisori e porre domande sull’uso delle tecnologie”* attraverso una serie di racconti di *fiction* che usciranno sotto forma
di fumetti.

Riporto per intero l’articolo per chiarire bene il contesto:

> As might be expected artificial intelligence (AI) and its exponential growth
> was a major theme at the conference, from secure data clouds, to quantum
> computing and ChatGPT. However, perhaps one of the most fascinating
> presentations came from Col Tucker ‘Cinco’ Hamilton, the Chief of AI Test
> and Operations, USAF, who provided an insight into the benefits and hazards
> in more autonomous weapon systems.  Having been involved in the
> development of the life-saving Auto-GCAS system for F-16s (which, he noted,
> was resisted by pilots as it took over control of the aircraft) Hamilton is
> now involved in cutting-edge flight test of autonomous systems, including
> robot F-16s that are able to dogfight. However, he cautioned against relying
> too much on AI noting how easy it is to trick and deceive. It also creates
> highly unexpected strategies to achieve its goal.
>
> **He notes that one simulated test saw an AI-enabled drone tasked with a
> SEAD mission to identify and destroy SAM sites, with the final go/no go
> given by the human. However, having been ‘reinforced’ in training that
> destruction of the SAM was the preferred option, the AI then decided that
> ‘no-go’ decisions from the human were interfering with its higher mission
> – killing SAMs – and then attacked the operator in the simulation. Said
> Hamilton: “We were training it in simulation to identify and target a SAM
> threat. And then the operator would say yes, kill that threat. The system
> started realising that while they did identify the threat at times the
> human operator would tell it not to kill that threat, but it got its
> points by killing that threat. So what did it do? It killed the operator.
> It killed the operator because that person was keeping it from
> accomplishing its objective.”**
>
> **He went on: “We trained the system – ‘Hey don’t kill the operator –
> that’s bad. You’re gonna lose points if you do that’. So what does it
> start doing? It starts destroying the communication tower that the
> operator uses to communicate with the drone to stop it from killing the
> target.”**
>
> This example, seemingly plucked from a science fiction thriller, mean
> *[sic]* that: “You can't have a conversation about artificial
> intelligence, intelligence, machine learning, autonomy if you're not going
> to talk about ethics and AI” said Hamilton.
>
> On a similar note, science fiction’s  – or ‘speculative fiction’
> *[sic]* was also the subject of a presentation by Lt Col Matthew Brown,
> USAF, an exchange officer in the RAF CAS Air Staff Strategy who has been
> working on a series of vignettes using stories of future operational scenarios
> to inform decisionmakers and raise questions about the use of technology. The
> series ‘Stories from the Future’ uses fiction to highlight air and space power
> concepts that need consideration, whether they are AI, drones or human machine
> teaming. A graphic novel is set to be released this summer.

Dopo il clamore mediatico e i dubbi espressi dagli esperti sulla plausibilità
dell’intera descrizione, un portavoce dell’Aeronautica militare statunitense ha...