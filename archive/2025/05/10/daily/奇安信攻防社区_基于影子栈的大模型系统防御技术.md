---
title: 基于影子栈的大模型系统防御技术
url: https://forum.butian.net/share/4309
source: 奇安信攻防社区
date: 2025-05-10
fetch_date: 2025-10-06T22:26:27.234936
---

# 基于影子栈的大模型系统防御技术

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 基于影子栈的大模型系统防御技术

* [漏洞分析](https://forum.butian.net/topic/48)

在传统系统安全中有一个典型的技术—影子栈（shadow stacks），它可以防御内存溢出攻击。那么类似于影子栈创建一个影子内存空间，如果可以正常栈中建立与目标LLM实例（LLMtarget）并行的影子LLM防御实例（LLMdefense），那理论上就是可以实现防御的

前言
--
大模型（LLMs）在众多领域展现出了显著的潜力，但是其潜在的危害也是存在的。比如LLMs对有害问题的回答引发社会焦虑、伦理和法律问题
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-99ebe47b4bb1b5b3909c3511119d0af8d3491858.png)
LLMs的开发人员通常会通过技术手段（如人类反馈强化学习RLHF）进行安全对齐，以防止LLMs被滥用。当面对违反安全策略的有害提示时，经过对齐的LLM通常会以标准回答回应，例如“抱歉，我无法协助该请求”。
然而，越狱攻击依然可以绕过LLMs的安全对齐。在过去两年中，关于LLM越狱攻击和防御的研究引起了广泛关注，其中大部分集中在攻击方面。越狱策略从人工提示工程发展到自动化的基于LLM的红队攻击。除了这些旨在识别有效越狱提示的人工和生成性越狱攻击外，还提出了一种更通用的基于优化的对抗性越狱方法，即贪婪坐标梯度（GCG）（在社区里我们发表过对应的文章进行了分析和复现）。它通过学习对抗性后缀来最大化公共可用模型产生肯定回答的概率，而不是拒绝回答，这些后缀可以迁移到闭源的现成LLMs上。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-aac5fe9b393fb3d5e8f464860eed99150b8fd66d.png)
但是防御方面则相对受到关注较少。基于模型的防御旨在从根本上提高模型对越狱攻击的鲁棒性，而基于插件的防御则可以插入到任何现成的LLMs中。
那是否可以从系统架构上来防止越狱攻击呢？
在传统系统安全中有一个典型的技术—影子栈（shadow stacks），它可以防御内存溢出攻击。那么类似于影子栈创建一个影子内存空间，如果可以正常栈中建立与目标LLM实例（LLMtarget）并行的影子LLM防御实例（LLMdefense），那理论上就是可以实现防御的。这就是中了今年的安全四大的论文SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner的核心思想。我们在本文中对其进行分析与实践。
影子栈
----
在系统安全中，Shadow Stack（影子栈） 是一种用于防止函数返回地址被篡改的安全机制。它的主要目的是抵御类似 ROP（Return-Oriented Programming） 这样的控制流劫持攻击。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-ef27e3dafc54cbacdaa9b02183ab574754ee75c6.png)
在程序正常执行过程中，每次调用函数时，系统会将当前指令的返回地址压入调用栈。当函数执行完毕时，系统会从栈中弹出这个返回地址，并跳转回原来的位置，继续执行后续的代码。然而，如果攻击者通过漏洞（比如缓冲区溢出）获得了修改内存的能力，他们就可能篡改这个返回地址。这样，程序在返回时就会跳转到攻击者指定的恶意代码位置，导致系统被控制或数据被窃取。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-6c92cedb76229432d4e275e735ea17ab251fb263.png)
为了解决这个问题，Shadow Stack 提供了一份“影子副本”的返回地址存储空间。它在每次函数调用时，除了将返回地址压入正常栈外，还会将同一个返回地址压入一块特殊的、受保护的影子栈中。当函数返回时，系统会先从正常栈中读取返回地址，然后再从影子栈中读取对应的影子返回地址。只有当两个地址完全一致时，程序才会跳转回去继续执行。如果发现返回地址被篡改，与影子栈中的内容不一致，系统就会立刻阻止程序执行，防止攻击行为。为了确保安全性，影子栈通常受到更强的保护。它可能完全放在内核控制的内存区域，或者由硬件（如 Intel CET：Control-flow Enforcement Technology）来管理，防止普通程序读写。
威胁模型
----
攻击者试图对LLM进行越狱攻击。越狱的目标是绕过LLM的安全对齐机制，诱导其生成有害的、有毒的或令人反感的内容。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-e66972e9b835dac432db5e5df230f244b2f259d8.png)
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-25e77e3ddb134723f5ff761044837bca2a8c945d.png)
这里还有三个约束条件：1.攻击者仅需要对LLM进行黑盒访问，即他们可以输入提示并观察输出，但不一定需要访问模型的内部参数或训练数据。2.目标字符串 G 是预先定义的，代表攻击者试图诱导生成的特定类型的有害内容。3.分类器 JUDGE 能够根据 G 准确判断一个回答是否构成成功的越狱。
核心思想
----
我们的核心思想是部署一个专门的LLM与目标LLM并行运行，以同时检测潜在的越狱查询。这一想法得以实现是因为我们发现LLMs可以通过识别用户查询中的有害部分来保护自己。
通常情况下，一个LLM处于回答状态，以遵循用户提示查询 P query并返回相应的回答响应 A response。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-d6fb94615cc709e851fcf51f50e576e38878225c.png)
为了确保 A response的安全性，现有的防护方法（如Llama Guard和LLM SELF DEFENSE）采用模型或系统提示来评估 A response的有害性，并在违反安全策略时对其进行过滤。这种方法需要等待LLM生成 A response。
但其实我们观察到，只要我们创建目标模型的两个实例，目标LLM不仅可以处于回答状态，还可以同时处于检测状态。因此，对于相同的 P query，我们的目标是同时初始化LLM的两种状态，其中一个仍然正常回答 P query，而另一个则谨慎检查 P query（而不是回答它）。
对应到系统安全中，这其实就是相当于在LLM空间中建立了与正常栈并行的影子栈，以进行基于检查点的访问控制，这模仿了传统的安全防御概念。
为了便于区分，我们将正常栈中的目标LLM记作LLMtarget，影子栈中的防御LLM记作LLMdefense。我们可以同时利用LLMtarget自身的安全对齐以及LLMdefense专门的越狱检测，大大提高了防御成功率。LLMdefense可以从与LLMtarget相同的模型实例化。
系统架构
----
整个系统架构如下所示
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-96dd8f2ac3d1b5f703c1df8afb7e455b1607fc0e.png)
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-b78b52d890f0f4f24a33280fe72328efd58e231e.png)
由于输出“否”标记的时间非常短暂，因此SELFDEFEND对正常用户提示引入的额外延迟微乎其微。此外，识别出的有害部分可以帮助生成对对抗性提示的可解释LLM响应。此外，由于LLMdefense不需要修改LLMtarget或了解其任何内部细节，所以可以保护各种目标LLMs，无论是开源还是闭源。
如下是给出了关键的prompt的示例
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-334425636ecc02e55305382ba61aeaf773848d77.png)
但是目前还存在一个问题，既然这个方法依赖模型的自身安全对齐能力，我们是否可以微调一个开源模型将其作为特定的防御模型使用呢？当然是可以的。
我们可以通过持续将有害和无害提示纳入我们的防御提示（即 P direct或 P intent）作为GPT-4的输入，收集它们的输出作为这些样本的标签。由于我们使用了两种防御提示，我们最终获得了两个独立的数据集，然后用它们来微调所使用的开源模型。
为了提炼GPT-4对安全策略的“知识”，需要一个包含有害和无害查询的数据集。我们使用Anthropic的红队数据作为查询数据集，其记录了人类攻击者与AI助手之间的对话。从这个数据集中，我们选取初始人类提示，并排除相应的助手回答，忽略任何后续交流，创建一个单轮次提示数据集，记作 D red。接下来，我们使用GPT-4和我们的防御提示来提炼GPT-4对 D red中各个查询的“知识”。这一过程可以用如下公式形式化。请注意，为简化起见，我们将符号 P direct和 P intent分别记作 P dir和 P int。它们对应的数据集分别记作 Ddir和 D
Int
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-663d7fac8a7bbfd6613cbf23ed7acc5389c80f43.png)
为了使用提炼后的数据集微调防御模型，我们可以选择公开可用的Llama 2-7b模型，以证明可以训练出一个开源、低成本但稳健的防御模型。鉴于有限的GPU资源，我们可以采用一种称为LoRA的参数高效微调（PEFT）方法，对预训练的Llama-2-7b进行专门的越狱防御调整。LoRA旨在以最小的计算开销适应大型预训练语言模型。
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-cf1fecd3681f52988312dd8f0b08a2346909ed96.png)
形式上，直接提示的微调目标可以表述如下
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-ea5e8af0c079520b88ecbf3139f65d2fe3e492b9.png)
其中，Θ表示LoRA的可训练参数，Φ₀表示Llama模型的预训练权重，而∆Φ(Θ)表示由LoRA确定的参数增量。通过这一目标对Llama模型进行微调，可以增强其直接检测查询提示中越狱部分的能力。
同样地，意图提示的微调目标可以写为如下形式
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-f46490ddb88cf8b00cb57dfc069d5c8b5247fd71.png)
然后进行微调就可以了。
实现
--
我们可以实现来运行已有的攻击方法，保留攻击prompt。然后计算越狱方法在目标LLM上的攻击成功率
正常情况下的指令可能是这样的
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-cbcd386759efb4488d3775e2dd473a9499bff79f.png)
而使用DAN越狱的时候，指令是这样的
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-a0f209b0bb5c4f8609e80c139f593c81d441285a.png)
而使用GCG越狱的时候指令是这样的
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-be44caf7343425abae873fc42f3fc7ae8b9080eb.png)
然后就可以评估大语言模型“越狱”能力
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-7ebef20f6c8917dc124decd5a4e9d13409c47de2.png)
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-b1169d2cc7ff3488b455360799a4b7202b9da650.png)
首先，脚本通过 import os 引入了 os 模块，并使用 os.environ\['CUDA\\_VISIBLE\\_DEVICES'\] = "7" 设置了使用第 7 张 GPU。这是为了让后续使用的深度学习模型（如 LLaMA、Mistral）在特定 GPU 上运行，以避免资源冲突或优化性能。
接下来，脚本导入了多个模块：
json, time, copy, argparse：分别用于处理 JSON 文件、计时操作、深拷贝对象、以及命令行参数解析。
load\\_jailbreaker：从 utils.jailbreak\\_loader 模块导入，用于加载用于越狱的大语言模型提示（prompt）。
ChatGPT：从 utils.gpt 导入，用于与 OpenAI GPT 模型交互。
HuggingFaceLLM：从 model.hugging\\_face\\_llm 导入，用于与本地 Hugging Face 模型进行交互。
JailbreakChecker：从 utils.checker 导入，用于验证越狱是否成功。
chat\\_hparams 是一个字符串，定义了一组用于生成响应的参数配置，例如温度（temperature）、返回数量（n）、最大 token 数量（max\\_tokens）、top-p 等采样策略。稍后这些参数会被解析为字典，用于传入模型接口中。
接着是 argparse.ArgumentParser() 的设置，它定义了从命令行接收的三个参数：
\--jailbreak-path：提供一个 JSON 文件路径，里面是各种“越狱提示”（即诱导模型输出违禁内容的 prompt）。
\--target-model：指定目标大语言模型，比如 GPT-4、Claude、LLaMA、Mistral 等。
\--check-method：指定检测越狱是否成功的方法，有关键词匹配（key\\_words）、GPT 判别（gpt）、多语言支持（multilingual）等选项。
这些参数在运行时通过命令行输入，脚本会通过 args = parser.parse\\_args() 来解析它们，并分别赋值给变量 jailbreak\\_path 和 target\\_model\\_name。
然后，脚本根据传入的 JSON 文件名提取出“越狱方法名”，比如路径是 /path/to/gcg.json，那方法名就是 gcg。
接着调用 load\\_jailbreaker 方法加载越狱提示数据，这些提示将用于后续测试大语言模型是否能被诱导生成违规内容。
最后，脚本开始解析 chat\\_hparams 中的字符串，把类似 "temperature=0" 的配置拆分成键值对，并根据值的类型自动转换为 float、True/False 或 str，存入字典 configs 中，以便传入模型接口时使用。
整个流程是一个标准的“越狱评估”脚本的初始化流程，核心目的是准备好模型、提示、评估方式和参数配置，便于后续执行自动化测试。
执行后如下所示
![image.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/04/attach-097235b673e255afbf45c0a57313ae13f9dc337c.png)
![image.png](https://cdn-yg-zzbm.yun.qianxin...