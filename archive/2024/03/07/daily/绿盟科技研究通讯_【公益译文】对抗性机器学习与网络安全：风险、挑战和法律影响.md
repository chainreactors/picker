---
title: 【公益译文】对抗性机器学习与网络安全：风险、挑战和法律影响
url: https://mp.weixin.qq.com/s?__biz=MzIyODYzNTU2OA==&mid=2247496762&idx=1&sn=07a64109e6575e749a6fcd041eab1f1d&chksm=e84c52e5df3bdbf3a0ca920a46a9f0e0e56e980dbfc3f6b8bb879104935438961e577ae99b5f&scene=58&subscene=0#rd
source: 绿盟科技研究通讯
date: 2024-03-07
fetch_date: 2025-10-06T17:11:08.753790
---

# 【公益译文】对抗性机器学习与网络安全：风险、挑战和法律影响

![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/hiayDdhDbxUamTncICIaee0PQoiaKicU4heRe5OGdKGtmicN00rX45hmDbYpiaRYAeTcMtqNwAkkazb5988htxB1bVQ/0?wx_fmt=jpeg)

# 【公益译文】对抗性机器学习与网络安全：风险、挑战和法律影响

小蜜蜂翻译组

绿盟科技研究通讯

![](https://mmbiz.qpic.cn/mmbiz_gif/hiayDdhDbxUamTncICIaee0PQoiaKicU4heJabc7BuAS9ZQxtC4NytkU6uVDFk0CAUB5rVWtVicNNLjrKtz1w33KuQ/640?wx_fmt=gif&from=appmsg)

全文共**10889**字，阅读大约需**20**分钟。

执行摘要

乔治城大学安全与新兴技术中心（CSET）和斯坦福网络政策中心地缘政治、技术和治理项目曾召集过一次专家研讨会，探究人工智能（AI）系统中的漏洞与传统类型的软件漏洞之间的关系。讨论的主题包括在标准网络安全流程下可以在多大程度上解决AI漏洞、目前阻碍准确共享AI漏洞信息的障碍、针对AI系统的对抗性攻击相关的法律问题，以及政府支持改进AI漏洞管理和缓解的领域。

与会者包括网络安全和人工智能红队角色的行业代表、具有对抗性机器学习研究经验的学者、网络安全监管、人工智能责任和计算机相关刑法方面的法律专家，以及具有重大人工智能监督责任的政府代表。

本篇报告的目的有两点。首先，本报告对AI系统漏洞进行了深入讨论，其中包括此类漏洞与其他类型漏洞的不同之处，以及AI漏洞的信息共享和法律监督的现状。其次，本报告将针对相关问题给出建议，建议均得到了大多数与会者的广泛赞同。建议内容主要分为四个方面，下文将逐一阐述。

一

前言

AI技术，特别是机器学习，现已广泛应用于商业和政府场景中。AI技术容易受到操作的影响，触发错误、从训练数据集推断私有数据、降低性能或泄露模型参数。研究人员证实了许多AI模型中的主要漏洞，包括许多在面向公众的环境中部署的AI模型。正如安德鲁·摩尔（Andrew Moore）2022年5月在美国参议院军事委员会作证时所说，保护AI系统免受对抗性攻击“绝对是目前最主要的问题之一”。

然而，AI漏洞可能不会直接反应到修复网络安全漏洞的补丁的传统定义上（请参阅“针对AI漏洞扩展传统网络安全领域”一节）。AI漏洞与可用补丁修复的传统漏洞之间的差异导致AI漏洞和AI攻击状态的模糊性。这将引发一系列企业责任和公共政策问题：是否可以通过传统的网络风险补救措施或缓解方法修复AI漏洞？开发和使用机器学习产品的公司是否具备充分的防御能力？AI系统的开发者或破坏AI系统的攻击者负有什么法律责任？政策制定者如何支持创建更安全的AI生态系统？

2022年7月，乔治城大学安全与新兴技术中心（CSET）和斯坦福网络政策中心地缘政治、技术和治理项目召集了一次专家研讨会，主要探究以上问题。与会者包括网络安全和人工智能红队角色的行业代表、具有对抗性机器学习研究经验的学者、网络安全监管、AI责任和计算机相关刑法方面的法律专家，以及具有重大AI监督责任的政府代表。本篇报告旨在总结研讨会的主要结论，同时向研究人员、相关行业的专业人员和政策制定者提出建议。

本报告将总结研讨会上针对对抗性机器学习问题达成的一致结论，同时提出改进未来应对措施的建议。建议分为以下四个部分：

* 关于在现有网络安全流程下可以在多大程度上处理AI漏洞的建议。
* 为积极参与构建AI模型、在商业产品中集成模型和使用AI系统的组织和个人提供关于组织文化和信息共享的变化的建议。
* 关于AI漏洞的法律问题的建议。
* 关于未来研究领域和政府支持的建议，促进AI系统领域更安全发展。

我们强调，对抗性机器学习是具有高技术性工具的复杂领域，AI漏洞可能带来社会性和社会性问题。我们提出的建议将强调工业和政府决策者通过投资技术研究促进智能安全的机会。但大多数建议还是集中在流程、机构文化和AI开发人员和用户的意识的变化上。我们虽然希望这些建议可以促使使用AI的组织主动思考AI系统安全相关问题，但我们也强调，个人保持自己的观点，并非必须支持某项建议。

二

扩展针对AI漏洞的传统网络安全领域

从很多意义上讲，针对AI系统的攻击并非是新出现的攻击类型。几十年来，攻击者一直试图绕过算法垃圾邮件过滤器、操纵推荐算法。在过去十年中，机器学习模型的流行程度急剧上升，同时越来越多地部署在高风险环境中。与此同时，研究人员不断证明，机器学习算法和训练过程中的漏洞是普遍存在的，而且难以解决。机器学习图像和语音识别系统会因人类难以察觉的扰动所影响，数据扭曲系统输出、被污染，敏感数据也被重建。

尽管有证据表明，现实中的黑客会利用深度学习系统中的漏洞，但目前为止，攻击大多还是发生在研究环境中。此外，随着AI模型被广泛应用，针对深度学习技术的攻击的频率将会增加。研讨会上，有人认为，这些攻击可能十分常见，通过攻击机器学习模型可在一定程度上获利（主要针对独立黑客），或通过攻击获得战略优势（主要针对国家支持的黑客）。

我们强调，所有AI系统都可能带有某些类型的AI漏洞，这些漏洞在高风险环境下十分令人担忧。发现这一趋势并非意味着支持在这种情况下使用AI。大多数与会者至少在某些高风险情况下对使用AI表示质疑，但本报告没有也无法提供一般规则，用于评估实际应用相关的伦理问题。

对抗性机器学习领域主要关注针对深度学习系统的攻击。深度学习目前是机器学习研究的主要焦点，但要强调的是，从某种程度上来说，针对深度学习模型的攻击是建立在针对其他传统机器学习方法的攻击的基础上。我们认为，深度学习模型中的“AI漏洞”与其他类型的漏洞并没有不同，其中包括传统机器学习模型中的漏洞。然而，深度学习模型的社会影响和研究关注度日益提高，所以应该特别关注此类漏洞。本报告还通过使用“深度学习”这一概念将相对较新的模型（以及对应的攻击）与所有机器学习模型以及其中的漏洞进行区分。

应对AI漏洞面临挑战。一方面，现有的网络安全框架较为通用，足以覆盖新类别的漏洞，例如，由深度学习方法产生的漏洞。实际上，可在标准的风险或漏洞管理框架下分析AI开发的风险。另一方面，AI漏洞在某些重要方面与传统的软件漏洞不同，相关人员可能需要对现有的网络安全风险管控框架进行扩展或调整。从抽象角度来看，AI和传统软件漏洞在以下方面有所不同：

* AI漏洞通常来自于训练数据和训练算法之间复杂的交互过程。某些类型的漏洞主要源于一个或多个可能用于训练AI模型的特定数据集，这类漏洞在模型训练完成之前很难被预测或缓解，而且很难测试潜在用户输入的全部空间，以理解系统如何对这些输入进行响应。
* “修复”AI模型中的漏洞可能需要重新训练，可能需要花费相当大的费用，或者根本无法实现。减少安全漏洞的模型再训练也能降低非恶意系统输入的整体性能。
* 在许多情况下，AI系统中的漏洞可能是临时的，比如在使用连续培训通道的组织中，模型经常用新数据进行更新。在其他环境中，漏洞可能较依赖于环境，例如，跨设备部署中心模型的本地微调版本的组织。在任何情况下，攻击和缓解措施似乎都无法通用于模型的所有版本。
* 在AI系统中，什么“算是”漏洞，往往存在着较大的不确定性。例如，对抗性示例是AI系统输入，它以某种方式被干扰，故意降低系统的性能。但是很难区分“攻击”和其他情况，甚至是预期的用户操纵，比如戴太阳镜让面部识别系统更难识别某人。虽然这个问题并不一定是AI特有的，但确实使定义单个AI漏洞的问题更加复杂。

这些差异很可能会改变AI系统中漏洞的处理方式。例如，如果不可能完全“修复”某个漏洞，AI开发人员和部署人员可能会让存在已知漏洞的系统保持运行状态。

应对措施可能会相对较多地关注风险缓解，而相对较少地关注风险补救，其中潜在的漏洞已完全消除。

**建议**

**构建或部署AI模型的组织应使用风险管理框架，解决AI系统生命周期的安全问题。**

风险管理框架是所有组织网络安全政策的关键要素，我们鼓励组织将其用于管理人工智能安全。与其他类型的风险管理框架相同，组织十分有必要将其纳入整个产品开发流程中。然而，针对AI漏洞，应用风险管理框架时还应多考虑一些其他因素。例如，如果机器学习模型中的漏洞不能像许多传统软件漏洞那样容易修复，那么组织可能会选择暂时缓解漏洞，而非修复或停用模型，尤其是当AI模型包含在复杂系统中时，删除个别组件可能会导致整个系统发生变化，难以控制。

使用风险管理框架的重要问题包括：如何确保模型可靠且安全？在什么情况下可安全部署AI模型？有哪些补偿措施？组织应如何做出决定，是将存在漏洞的系统或功能完全离线，还是将其保留在适当的位置并采取缓解措施？如何权衡某些重要问题？例如，针对模型应用实施防御措施和确保总体性能保持较高水平之间的权衡。哪些决策能保障机器学习产品终端用户的最大利益，同时符合最终受其影响的个人的最大利益？如何让这些群体参与整个系统的开发？在AI系统存在高风险的情况下，组织应考虑如何对少数群体有利，同时又要仔细评估这些群体是否因使用AI系统而受到伤害。

**对抗性机器学习研究人员、网络安全从业者和AI组织应积极试验扩展现有的网络安全流程，覆盖AI漏洞。**

网络安全领域人员开发了许多工具用于追踪和缓解漏洞，提出事件的应对方法。在漏洞管理方面，包括用于通用漏洞披露（CVE）系统、用于评估已知漏洞相关的潜在风险的通用漏洞评分系统（CVSS），以及用于安全研究人员和软件供应商之间进行协调的漏洞协同披露指南（CVD）。这些流程的设计没有考虑到AI系统，但研讨人员一致认为，这些机制的应用可能足够广泛，可以用于管理许多类型的AI漏洞。然而，网络安全从业者、机器学习工程师和对抗性机器学习研究人员之间需要更多地进行合作，将其适当地应用于AI漏洞。评估AI漏洞需要的技术可能与网络安全从业者的技术专长不是完全匹配，提醒各组织不要在没有培训和资源的情况下重新调整现有的安全团队。

AI漏洞和传统软件漏洞之间的差异可能会使这些流程更加复杂。与此同时，研讨会参与者认为，这些差异并不能证明创建一套单独的流程处理AI漏洞就是合理的。因此，我们鼓励研究人员和组织将AI漏洞纳入既定的风险管理实践。

**对抗性机器学习领域的研究人员和从业者应听取相关人员的建议，如：负责处理AI偏见和鲁棒性的人员以及具有相关专业知识的组织。**

多位研讨会参与者指出，在某些重要方面，与传统软件漏洞相比，AI漏洞可能更类似于算法偏见等问题。AI公平性方向研究人员广泛研究了数据、设计选择和风险决策如何导致模型失败，从而对现实世界造成伤害的问题。AI安全界应在开发自己的风险评估框架和评估AI在后续应用中的使用时，更好地理解这些经验教训。总而言之，研讨会参与者认为，提高对抗性机器学习研究人员、AI偏见领域、网络安全从业者、其他相关专家组和受影响群体的参与度十分重要。

三

改变应对信息共享和组织安全的心态

某些结构特征使得很难准确评估AI系统受到攻击的威胁有多大。首先，关于现有AI漏洞的大多数信息来自理论或学术研究环境、网络安全公司或内部研究人员对其组织的AI系统进行红团队研究。其次，由于缺乏系统化和标准化的手段追踪AI资产（如：数据集和模型）及对应的漏洞，很难了解到受影响的系统是否广泛。再次，针对AI系统的某些类型的攻击，可能需要机器学习或数据科学专业技能才能实现攻击检测，或者至少需要熟悉AI攻击相关的行为模式。由于许多网络安全团队可能不具备检测此类攻击的相关专业知识，企业可能缺乏发现和披露实际发生的AI攻击及其原因的能力。

组织即使发现了漏洞或恶意攻击，也很少将这些信息传输给他人，如：同行组织、供应链中的其他公司、终端用户、政府或其他社会人员。尽管存在某些信息传播机制，但缺乏专门用于共享事件信息且受保护的可信平台。几位来自行业和政府组织与会者指出，定期的信息交流使他们受益匪浅，但目前还不存在信息共享网络，官僚主义、政策和文化等方面因素阻碍了信息的共享。

这些情况意味着，根据当前的情况，攻击者成功利用漏洞之后，该问题可能始终会被忽视。为了避免出现这种情况，我们建议开发AI模型的组织采取重大措施，正式建立或利用信息共享网络，监控针对AI系统的攻击，提高透明度。

**建议**

**部署AI系统的组织应进行信息共享，提高对威胁的了解程度。**

目前，很少有可靠的机制可以让发现AI系统受到攻击的组织与他人共享信息，导致相关人员很难理解问题的范围和性质。AI事件数据库等现有的人工智能系统风险信息共享方式依赖于公开报告，主要关注机器学习故障或滥用，而不关注故意操控问题。这些方式虽然十分重要，但只适合用于汇总公开的AI故障问题，而无法起到让组织在更可信的环境中共享新安全威胁信息的作用。用于鼓励更开放地共享信息方式的机制可以采取多种形式，例如，关键行业利益相关者的定期非正式会议或较为正式的结构、存储库或组织。

**AI部署人员应强调AI开发的安全文化贯穿产品生命周期的每个阶段的重要性。**

许多机器学习库提供的功能默认优先处理针对速度和性能的细微改进，而非安全性。在构建模型后，只考虑安全问题的产品团队可能会在模型的开发流水线中引入不安全因素，一旦模型经过充分训练，可能很难或无法消除不安全因素。与所有软件相同，组织应该将安全性问题作为AI流水线中每个阶段的优先事项。需要为对抗性机器学习团队提供强大的支持，使团队参与到产品开发的每个阶段，避免将安全性问题“外包”给单独的团队的问题。

**高风险AI系统的开发人员和部署人员必须优先考虑透明度问题。**

AI模型本身就具有漏洞，更不用说所有统计模型固有的其他类型的故障模式了，这些漏洞很难修复。在高风险环境中，假定有些漏洞是存在的，这对社会福利和隐私问题具有重大影响。鉴于此，与会者认为，机器学习模型的安全特性的透明度具有一定影响。按照以上原则，最低标准的透明度可能需要达到一定要求，即消费者和其他普通公民在高风险环境中使用人工智能模型时，应被告知实际情况。此外，模型的设计者就相关需要权衡的问题做出重要决定时，如安全性、性能、稳健性和公平性之间的权衡问题，许多与会者认为应将做出的决定进行公开，起到对受模型决策影响的最终用户或公民的保护作用，有助于其在面对不利或歧视性结果时能够诉诸法律。与会者对透明度原则的发展问题存在分歧，许多人强调，该原则不应延伸到简单地向公众披露每个漏洞这一问题上，但本节讨论的最低透明度标准得到了广泛支持。

例如，大多数机器学习库默认提供图像处理功能，这些功能容易受到对抗性规避的影响。虽然可以很容易地使用替代的预处理函数，但默认情况下，大多数库使用的方法并不安全，而且是在没有大量再训练的情况下，在以一种方式预处理的图像上训练的模型无法很容易地转换为以另一种方式进行预处理的图片。在模型训练后，难以更改的其他安全相关决策包括影响训练数据的安全性和完整性的决策、其他类型的输入过滤的使用、用于微调的上游模型的选择等等。

四

明确AI漏洞的法律地位

美国没有全面的人工智能相关的法律（也不太可能很快出台）。然而，刑法、消费者保护法规、隐私法、民权法、政府采购要求以及合同、过失和产品责任的普通法规则等法律领域，甚至美国证券交易委员会关于上市公司披露义务的规则，都与人工智能息息相关。人工智能符合传统的网络安全风险框架，与此相同的是现有法律也涵盖了人工智能领域，但方式和程度尚未得到法院和监管机构的充分说明。迄今为止，人工智能领域相关的政策重点都集中在偏见和歧视问题上。

联邦贸易委员会（FTC）、美国平等就业机会委员会（U.S.Equal Employment Opportunity Commission）和消费者金融保护局（Consumer Financial Protection Bureau）等机构都发布了指导意见，针对可能违反联邦民权法、反歧视法和消费者保护法的情况下使用人工智能模型的情况进行了说明。

纽约州金融服务部警告保险公司，其使用的数据、算法以及应用的预测模型可能存在歧视形式，为州法律所禁止。在加利福尼亚州，隐私立法要求加州隐私保护机构通过法规“管理企业使用自动决策技术的访问权和选择退出权，其中包括分析和要求企业对访问请求的响应包含决策相关过程中涉及的逻辑信息。”

我们的观点是，应尽可能在现有的网络安全程序下处理AI漏洞，我们建议最好通过扩展和调整网络安全法处理漏洞，而不应将AI安全问题作为独立的问题进行管控。然而网络安全法仍需不断发展和改进，许多问题尚未解决或即将出现。各部门的要求各不相同，联邦和州的规定相互重叠。在此情况下，受保护的医疗数据、金融信息、联邦政府收购的软件和信息系统、关键基础设施以及其他系统或数据，都具有相关的法规或行政命令规定的网络安全要求。但没有一部全面的网络安全法对绝大多数公司规定明确的法定义务。虽然普通法中关于过失、产品责任和合同的理论确实适用于AI产品和系统，但这些领域的法律原则（包括关于是否存在注意义务、过失案件中的经济损失规则和免责声明的问题）意味着，几乎没有对安全故障的责任作出明确裁决的案例。在联邦法院，立场学说导致更加难以达到诉讼的公平性。几乎没有根据实际情况决定的网络安全案例，阻碍了对传统类型的漏洞制定明确标准的进程，更不用说AI相关的漏洞了。

与此同时，联邦贸易委员会表示，监管的不公平和欺骗性商业行为的权力的影响逐渐扩展大，影响未通过合理网络安全措施保护客户信息的企业。联邦贸易委员会对未保护消费者数据的公司提起了多项诉讼。虽然迄今为止没有任何案件直接提出这一诉讼，但很容易想象，部署存在漏洞的AI系统可能会引发类似形式的监管监督，尤其是当这些漏洞存在于公司可合理预测的所有机器学习模型的通用功能时。

此外，如果AI公司的模型存在可预见的漏洞，其声称的模型稳健性和性能会受到质疑，这些公司可能会被指控存在欺诈行为。联邦贸易委员会通过研讨会和非约束性声明明确表示，他们会关注AI的影响。

尽管联邦贸易委员会已经对未保护消费者数据的公司提起了数十起诉讼，但其权威问题仍存在不确定性，尤其是其声称未能提供合理的安全保障属于不公平和欺骗性行为管辖权。除非采取立法行动，否则联邦监管机构对AI漏洞的监管同样需要从模糊的权威地位开始。

针对阻止对AI系统的攻击的问题，联邦出台了《计算机欺诈与滥用法》。此项法律规定，在未经授权或超出授权访问范围的情况下访问计算机信息，以及通过故意传输“程序、信息、代码或命令”“损坏”计算机的行为均为违法行为。这一直存在争议，尤其是关于该法律是否适用于非恶意网络安全研究人员调查系统漏洞的活动的问题。对于许多实体来说，可以通过漏洞披露计划减轻法律风险，披露计划授权或邀请独立的安全研究人员对系统或产品进行探究。就《计算机欺诈与滥用法》对AI领域的适用情况而言，许多条款关注的是攻击者是否首先获得了“未经授权的权限”或超过了对受保护计算机的授权权限范围，但这一步骤对于许多对抗性AI攻击来说可能不是必要的。然而《计算机欺诈与滥用法》规定，在未经授权的情况下对受保护的计算机造成损害是非法行为，其中“损害”这一概念被广泛定义为对数...