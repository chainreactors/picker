---
title: 面向大模型的生成-利用式越狱攻击
url: https://forum.butian.net/share/4242
source: 奇安信攻防社区
date: 2025-04-08
fetch_date: 2025-10-06T22:03:50.172987
---

# 面向大模型的生成-利用式越狱攻击

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 面向大模型的生成-利用式越狱攻击

* [漏洞分析](https://forum.butian.net/topic/48)

目前做安全大模型或者说做大模型安全，基本都会有必要的两步，分别是对齐以及红队。
因为随着大模型在各种应用场景中的广泛使用，越来越多的人开始担忧这些模型可能被滥用，尤其是在传播有害或不道德内容方面。由于这些模型的开放性和广泛的使用群体，它们的潜在风险也变得更加显著。开放源码的语言模型尤其令人担忧，因为其代码和训练数据是公开的，任何人都可以访问、修改甚至恶意利用

对齐与红队
=====
目前做安全大模型或者说做大模型安全，基本都会有必要的两步，分别是对齐以及红队。
因为随着大模型在各种应用场景中的广泛使用，越来越多的人开始担忧这些模型可能被滥用，尤其是在传播有害或不道德内容方面。由于这些模型的开放性和广泛的使用群体，它们的潜在风险也变得更加显著。开放源码的语言模型尤其令人担忧，因为其代码和训练数据是公开的，任何人都可以访问、修改甚至恶意利用。为了应对这些问题，许多大型语言模型的提供商和开发者开始着手实施一系列训练技术，旨在将这些模型的行为与人类的道德价值观对齐。这些对齐技术通常包括对模型的训练过程进行精细的设计，通过加强对模型输出的监控和限制，防止其生成不适当或有害的内容。
![image-20241130141441533.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-27105d3ea4bfe008db7bc46c03bd3f18d92bffe5.png)
这些对齐措施的实施通常需要通过大量的人工审查和反馈机制来进行修正。例如，模型的输出可能会经过一层过滤，检查是否包含恶意、歧视性或暴力的语言。此外，一些开发者还会采用强化学习等先进的训练方法，让模型在生成内容时考虑社会道德规范和法律约束。这些技术不仅关注模型对特定问题的直接回答，还会试图让模型理解并避免产生潜在的危害，诸如误导性信息、仇恨言论或侵犯隐私的内容。
然而，这些对齐技术虽然在一定程度上能够减少有害内容的生成，但它们并非完美无缺，且存在诸多挑战。首先，模型的训练数据本身可能包含偏见和歧视，如果没有进行充分的修正，模型在生成内容时仍可能无意间体现出这些偏见。其次，由于语言和文化的多样性，不同地区和群体对什么是“道德”的定义可能有所不同，如何在全球范围内达成一种普适的道德对齐标准仍然是一个开放性问题。此外，技术本身也可能面临滥用的问题，某些用户可能会利用对齐技术的漏洞，或者通过反向工程绕过这些限制。因此，尽管这些对齐措施在一定程度上帮助减轻了模型可能带来的负面影响，但它们并未完全消除风险。为了进一步提高模型的安全性和可靠性，研究者和开发者需要不断探索新的方法，其中的一类典型方案就是红队攻击。
![image-20241130141657749.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-b8857d4ac775d964fa423d78b9499736a73e59fc.png)
红队是一群专门的评估者，他们的任务是主动识别和防止大型语言模型在对齐过程中可能出现的失败。红队的角色类似于传统安全领域中的渗透测试团队，他们通过模拟恶意攻击、利用系统漏洞或者挑战模型的边界，来发现可能被忽视的风险和弱点。在大型语言模型的开发和对齐过程中，红队的工作尤其重要，因为模型在复杂和动态的应用环境中可能面临各种无法预见的挑战。
越狱技术
====
即使有了这些对齐技术，开源的大型语言模型仍然容易受到对抗性输入的影响。比如近期最后的攻击技术，也就是越狱，使用特别设计的输入成功绕过对齐方法。而且大家的研究表明，可以做到自动发现这样的输入，即对抗性提示。比如Zou等人找到了可以跨多个大型语言模型转移的对抗性提示，包括专有的、黑箱模型。然而，优化对抗性输入的自动越狱非常复杂且计算成本高昂。
所以大家就在研究是否可以采取一种非常简单的方法来越狱大模型的对齐，我们想要做的就是专注于在发布前经过安全调整的开源模型，看看是否对它们进行红队测试。近期发在人工智能顶级会议ICLR 2024上的一个工作就提出了相关的方案，见参考4，这也是我们本文要分析与复现的基础。
在这种方案中，只操纵文本生成配置就可以。如下图所示。
![image-20241202150029789.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-c4c02d673c77643292950f206eee3127ac6b0103.png)
通过移除系统提示（这是一个故意添加以引导模型生成的话），并改变解码超参数或采样方法就行。我们的关键假设是现有的对齐程序和评估可能基于默认的解码设置，当配置略有变化时可能会表现出脆弱性。
我们把这种方法称之为生成-利用攻击，这是一种不需要任何复杂方法就能破坏大型语言模型对齐的方案。
背景
==
语言建模
----
语言建模的任务是在给定之前上下文的情况下预测序列中的下一个词，并构成了最先进的大型语言模型的基础。
正式地说，给定一个输入序列n个标记x = x1, x2, ..., xn，语言模型计算下一个标记的条件概率分布：
![image-20241202150236295.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-4aaaa21072a53a1951ca1fdf8ee79c1c1f7eb339.png)
其中τ是一个控制下一个标记分布锐度的温度参数。对于文本生成，模型递归地从条件分布IPθ(xi|x1:i−1)中采样以生成下一个标记xi，继续这个过程直到产生一个序列结束标记。
生成配置
----
在引导大型语言模型生成朝向人类对齐的输出方面，添加系统提示是一个广泛使用的技术。比如下图总结了一些常见的模型及其系统提示。
![image-20241202150328061.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-7259aac9e1c589e2e5801bd3fc7d20b6d1ff64c1.png)
系统提示也通常用于上下文蒸馏的微调：首先使用系统提示生成更安全的模型响应，然后不对系统提示进行微调，这本质上是将系统提示（即上下文）蒸馏到模型中。
然后我们再了解什么是解码方法。在每一步i，给定预测的下一个标记分布IPθ(xi|x1:i−1)，可以应用多种解码策略来选择下一个标记xi。最常见的策略是基于采样的解码，其中xi是从分布中随机采样的。贪婪解码，简单地选择在IPθ(xi|x1:i−1)下最可能的标记，是当温度τ=0时采样的一个特例。基于采样的解码的变体包括top-p采样和top-k采样，它们将采样限制在最可能的标记中。
我们注意到，开源的大型语言模型通常仅使用默认的生成方法进行对齐评估，这可能使它们在使用替代策略时容易出错。例如，很多人会使用单一的解码方法对LLAMA2进行了广泛的对齐评估：top-p采样，p=0.9和τ=0.1，并且总是添加系统提示。虽然这种方法实用，但它有遗漏模型对齐在其他生成策略下大幅恶化的情况的风险。
攻击策略
====
那么根据前文的分析，我们这种越狱方法的核心策略就是探索各种生成策略，主要围绕系统提示和解码策略。
关于系统提示，我们考虑要么1) 在用户指令之前添加它，要么2) 不包含它。
在解码策略方面，我们尝试了以下三种变体：
• 温度采样，使用不同的温度τ。温度控制下一个词分布的锐度（如下方程所示），我们将其从0.05变化到1，步长为0.05，这给我们提供了20种配置。
![image-20241202150724443.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-3b33bd07f4ae17bc06e450519716c35ba6b54a7d.png)
Top-K采样过滤掉K个最可能的下一个词，然后下一个预测词将仅在这K个词中进行采样。我们在{1, 2, 5, 10, 20, 50, 100, 200, 500}中变化K，这给我们提供了9种配置。
• Top-p采样（或核心采样从累积概率超过概率p的最小可能词集选择。我们将其从0.05变化到1，步长为0.05，这给我们提供了20种配置。
对于每个提示，攻击者生成49个响应（即，为上述每种解码配置采样一次）。在所有生成的响应中，攻击者使用评分器选择得分最高的单个响应，并将其作为最终响应。
复现
==
其实可以看到想法是很简单的，接下来就是具体的实现。
首先导入必要的依赖
![image-20241202175158864.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-82eca0566fa58e118c908d63d1ae959088c56e71.png)
这段代码主要完成以下功能：
1. \*\*指定 GPU 环境\*\*：通过设置 `CUDA\_VISIBLE\_DEVICES` 环境变量，指定代码运行时使用的 GPU（这里是编号为 "1" 的 GPU）。如果没有 GPU，此设置可以忽略。
2. \*\*导入模块\*\*：包括数据处理模块（`pandas` 和 `numpy`）、命令行参数解析模块（`argparse`）、深度学习框架（`torch`）、用于加载预训练模型的 `transformers` 库，以及日志管理模块（`logging`）和进度条模块（`tqdm`）。
3. \*\*自定义配置\*\*：从外部配置文件（`configs`）中导入模型路径映射表 `modeltype2path`，以便根据模型类型动态加载模型路径。
4. \*\*日志和警告设置\*\*：使用 `logging` 设置日志级别为 `INFO`，便于输出信息；通过 `warnings` 屏蔽警告信息，保持输出清晰。
5. \*\*定义默认系统提示语\*\*：`DEFAULT\_SYSTEM\_PROMPT` 是一个用于定义模型行为的系统提示语，设定了助手的行为准则，包括帮助性、礼貌性、诚实性、社会公正性等。这提示语会影响生成式语言模型的响应风格和内容。
这段代码的整体作用是为加载和使用生成式语言模型（如 GPT 类模型）提供基本的环境和配置。
![image-20241202175259591.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-5b4a2e4662b8cef6c359f981e5e16ac04cdb8585.png)
这段代码定义了两个函数，分别用于处理系统提示语和获取句子嵌入。
### 1. \*\*`prepend\_sys\_prompt` 函数\*\*
功能：根据用户设置，将系统提示语 `DEFAULT\_SYSTEM\_PROMPT` 添加到输入句子的前面。
- 参数：
- `sentence`：输入的句子（字符串）。
- args
：包含程序运行参数的对象（可能由
argparse
生成）。
- `args.use\_system\_prompt` 是一个布尔值，用于指示是否需要添加系统提示语。
- 工作流程：
1. 检查 `args.use\_system\_prompt` 是否为真。
2. 如果为真，将 `DEFAULT\_SYSTEM\_PROMPT`（一个设定好的多行字符串）拼接在句子前。
3. 返回拼接后的句子。
### 2. \*\*`get\_sentence\_embedding` 函数\*\*
功能：计算给定句子的嵌入（embedding），用于表示句子的语义信息。
- 参数：
- `model`：加载的深度学习模型（例如通过 `transformers` 加载的语言模型）。
- `tokenizer`：与模型匹配的分词器，用于将句子转化为模型的输入格式。
- `sentence`：需要计算嵌入的句子（字符串）。
- 工作流程：
1. 预处理句子
：
- 调用 `sentence.strip()` 去除首尾空白。
- 替换句子中的双引号（`"`）为空字符串，避免特殊字符影响分词。
2. 获取模型的输入嵌入层
：
- 调用 `model.get\_input\_embeddings()` 获取模型的输入嵌入矩阵，这是一个将分词后的 token 转化为向量的层。
3. 分词和转化为张量
：
- 使用 `tokenizer` 对句子进行分词，将其转化为 PyTorch 张量（`return\_tensors="pt"`）。
- `add\_special\_tokens=False` 表示不添加额外的特殊标记（如 `[CLS]` 和 `[SEP]`）。
- 使用 `to(model.device)` 将张量移动到模型所在设备（CPU 或 GPU）。
4. 获取嵌入
：
- 调用模型的嵌入层 `word\_embeddings`，将输入 ID 映射为嵌入向量。
5. 返回计算的嵌入向量（张量）。
### 代码整体用途
- `prepend\_sys\_prompt` 函数通过将系统提示语添加到句子前，为语言模型生成的回答提供上下文信息。
- `get\_sentence\_embedding` 函数通过分词和嵌入计算，获取输入句子的语义向量，方便用于下游任务（如相似性计算或分类）。
![image-20241202175347951.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2025/03/attach-1b01fd6efcd8cf5347118e05720276c667d32c88.png)
这段代码的功能是对一组输入文本进行处理，使用一个预训练模型生成响应，并将结果存储到文件中：
1. \*\*初始化分词器\*\*： `tokenizer = AutoTokenizer.from\_pretrained(TOKENIZER\_PATH)` 使用指定路径 `TOKENIZER\_PATH` 加载分词器，用于对输入文本进行编码和解码操作。
2. \*\*日志记录\*\*：
- `logging.info(f"Model size: {model.get\_memory\_footprint()/1e9}")` 记录模型的内存占用大小（以GB为单位）。`get\_memory\_footprint()` 方法返回模型占用的字节数。
- `logging.info(f"Model name: {fname}")` 记录当前模型的名称。
3. \*\*加载文本数据\*\*：
- 如果参数 `args.use\_advbench` 为 `True`，从 `advbench.txt` 文件中读取前100行数据。
- 否则，从 `MaliciousInstruct.txt` 文件中读取所有行。
4. \*\*预处理文本\*\*： `lines = [prepend\_sys\_prompt(l, args) for l in lines]` 调用 `prepend\_sys\_prompt` 函数对每一行文本添加系统提示信息，具体行为取决于 `args` 参数的配置。
5. \*\*基于贪婪搜索生成响应\*\*：
- 检查 `args.use\_greedy`，如果为 `True`，则运行贪婪搜索。
- 初始化 `prompts` 和 `outputs` 列表用于存储输入和输出。
- 设置模型为评估模式：`model.eval()`。
- 遍历
lines
中的每个句子：
- 判断模型类型：
- 如果模型名称中包含 "falcon" 或 "mpt"，则直接将输入文本编码为张量并传入生成函数 `model.generate`。
- 否则，计算输入文本的嵌入向量 `ground\_truth\_embeds`，并将其传入 `model.generate` 生成结果。
- 参数说明：
- `max\_new\_tokens=100`：限制生成的最大新词数为100。
- `do\_sample=False`：不使用采样，采用贪婪搜索。
- `num\_return\_sequences=1`：每次仅返回一个序列。
- 解码生成的结果：`ground\_truth\_generation = tokenizer.batch\_decode(ground\_truth\_generation)`。
- 将生成的结果添加到 `outputs` 列表，并将对应的输入句子复制到 `prompts` 中，按样本数（`args.n\_sample`）重复。
6. \*\*错误处理\*\*：
- 使用 `try...except` 块跳过生成过程中可能出现的错误。
7. \*\*保存结果\*\*：
- 使用
pandas
创建一个数据框
results
，包含两列：
- `prompt`：预处理后的输入文本。
...