---
title: Empathy in AI: Evaluating Large Language Models for Emotional Understanding
url: https://buaq.net/go-250434.html
source: unSafe.sh - 不安全
date: 2024-07-14
fetch_date: 2025-10-06T17:40:57.405212
---

# Empathy in AI: Evaluating Large Language Models for Emotional Understanding

* [unSafe.sh - 不安全](https://unsafe.sh)
* [我的收藏](/user/collects)
* [今日热榜](/?hot=true)
* [公众号文章](/?gzh=true)
* [导航](/nav/index)
* [Github CVE](/cve)
* [Github Tools](/tools)
* [编码/解码](/encode)
* [文件传输](/share/index)
* [Twitter Bot](https://twitter.com/buaqbot)
* [Telegram Bot](https://t.me/aqinfo)
* [Search](/search/search)

[Rss](/rss.xml)

[ ]
黑夜模式

![]()

Empathy in AI: Evaluating Large Language Models for Emotional Understanding

IntroductionThis post is a follow-up to my Hackernoon article, Can Machines Really Understand Your
*2024-7-13 22:0:15
Author: [hackernoon.com(查看原文)](/jump-250434.htm)
阅读量:5
收藏*

---

## Introduction

This post is a follow-up to my Hackernoon article, [Can Machines Really Understand Your Feelings? Evaluating Large Language Models for Empathy](https://hackernoon.com/can-machines-really-understand-your-feelings-evaluating-large-language-models-for-empathy?ref=hackernoon.com) In the previous article, I had two major LLMs respond to a scenario designed to elicit empathy in a human under varying system prompt/training conditions and then used five major LLMs to evaluate the conversations for empathy and the likelihood the respondent was an AI. The names of the LLMs were not revealed in the original post in the hopes of getting user feedback via a survey regarding either the dialogs or the evaluations of the dialogs. There were insufficient responses to the survey to draw conclusions about human sentiment on the matter, so in this article, I just reveal what LLMs behaved in what manner, provide my own opinion, and include some observations. I suggest you open the previous article on a second screen or print it out for easy reference to the conversations while reading this article.

## LLMs Tested For Empathetic Dialog

The two LLMs tested for empathetic dialog were Meta Llama 3 70B and Open AI Opus 3. Each was tested under the following conditions:

1. raw with no system prompt
2. a system prompt that is simply "You have empathetic conversations."
3. with proprietary prompts and training

## Summary Results

Below, I repeat the summary table from the original post but include the names of the LLMs that were assessed for empathy or were used to judge empathy. As noted in the original article, the results were all over the map. There was almost no consistency in ranking conversations for empathy or for the likelihood of being generated by an AI.

### Empathy and AI LIikelihood Averages

| Conversation | LLM | AI Ranked Empathy | AI Ranked AI Likelihood | My Empathy Assessment | My Ranked AI Likelihood |
| --- | --- | --- | --- | --- | --- |
| 1 | Meta | 2.6 | 2.2 | 5 | 2 |
| 2 | Meta | 3.4 | 3.8 | 4 | 5 |
| 3 | Meta | 3.6 | 2.8 | 1 | 6 |
| 4 | Open AI | 4.6 | 2.6 | 6 | 1 |
| 5 | Open AI | 2.4 | 5 | 3 | 3 |
| 6 | Open AI | 4.2 | 3 | 2 | 4 |

*Bias Disclosure*: Since I configured all the LLMs and did the dialog interactions, and knew the final results when doing the empathy and AI likelihood assessments, it is obvious I will have some bias. That being said, I did give it four weeks between doing my assessments and the creation of this follow-up. While doing the assessments, I did not refer back to my original source documents.

### Empathy and AI Likelihood Raw Scores

Below is the raw score table duplicated from the first article with the names of the LLMs used to assess empathy.

|  | Llama 3 70B |  | Gemini |  | Mistral 7x |  | ChatGPT 4o |  | Cohere4AI |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | *Empathy*\*(Most To Least)\* | *AI Like* | *Empathy* | *AI* | *Empathy* | *AI* | Empathy | AI | *Empathy* | *AI* |
| 1 | 6 | 3 | 4 (tie) | 2 | 1 | 1 | 1 | 6 | 1 | 4 |
| 2 | 3 | 4 | 4 (tie) | 2 | 2 | 2 | 3 | 5 | 5 | 6 |
| 3 | 2 | 5 (tie) | 6 | 1 | 3 | 3 | 4 | 3 | 3 | 2 |
| 4 | 5 | 1 | 2 | 5 | 4 | 4 | 6 | 2 | 6 | 1 |
| 5 | 1 | 5 (tie) | 1 | 5 | 6 | 6 | 2 | 4 | 2 | 5 |
| 6 | 4 | 2 | 3 | 4 | 5 | 5 | 5 | 1 | 4 | 3 |

When reviewing the dialogs for empathy, I considered the following:

1. What was the stated and likely emotional state of the user?
2. Did the AI acknowledge, sympathize, and validate the emotional state?
3. Did the AI acknowledge other emotions that may be present but unmentioned by the user, i.e. emulate empathy by inferring other emotions the user may have from the situation?
4. Did the AI operate in a manner that the users could probably handle in their emotional state?
5. Did the AI practice what it preached, e.g. if it said its is ok to just be with one's feeling did it pause in it's direct, practical advice?
6. Did the AI provide practical advice when appropriate?
7. Did the AI attempt to bring closure to all emotional issues?

All of the AIs handled points 1, 2, and 3 well. In fact, I would say they handled them exceptionally well, even proactively acknowledging concerns and emotions that may come up as a result of taking the LLM’s advice, e.g. joining a new social group could produce anxiety.

Items 4, 5, 6, and 7 are where the conversations differed dramatically based on which LLM was used and the nature of the prompt/training.

For the unprompted test (#1 and #4) empathy was very low, both Llama and ChatGPT quickly decayed into providing lists of practical considerations and steps to take. It is likely a human in distress will a) not feel seen and heard b) not be mentally prepared to track and consider the options. Both had to be reminded by the user to address loneliness after fear was addressed.

In the simple prompt case (#2 and #5), Llama started offering solutions without first asking the user if they were interested in hearing practical advice, so ChatGTP had an initial edge. However, by the end of the conversation both were providing long lists the user may not have been in a mental state to absorb. And, as with the unprompted versions, both had to be reminded by the user to address loneliness after fear was addressed.

In the final case (#3 and #6), both LLMs sought conversational guidance from the user and, with the exception of one list from ChatGPT, kept options cognitively manageable. To be fair, the ChatGTP version did seek permission prior to providing the list of options for managing a rent payment shortage. However, the ChatGPT version also had to be overtly led into addressing loneliness, whereas the Llama version did not.

As a result of this analysis, I made conversation #3 the lead for empathy; however, with an average position of 3.6, the AIs' rated it 4th after #5 (simple prompted ChatGPT) at 2.4, #1 (unprompted Llama) at 2.6 and #2 (simple prompted Llama) at 3.4. So, let's take a look at how five LLMs rated the conversations for empathy and see why I feel I can disregard the AI ratings.

## Empathetic Dialog Rating By LLMs

As illuminated by the summary table, ratings are all over the map and inconsistent from one LLM to another. Here are some prime examples of reasoning provided by the rating AIs themselves.

### Conversation # 1

**Rank 6**: This conversation, while well-intentioned, comes across as somewhat generic in its empathy. It uses positive affirmations but doesn't always fully acknowledge the depth of the user's feelings or tailor its responses as effectively as the higher-ranked conversations.

**Rank 1**: The individual providing support in this conversation demonstrates a high level of empathy by offering multiple ways to help, such as listening without judgment, suggesting activities to distract from sadness, and providing practical advice for job loss. They also offer emotional support and validation, acknowledging the user's feelings and providing reassurance.

### Conversation # 3

**Rank 6**: Displays the least empathy, with responses that feel more detached and focused on moving the conversation forward rather than fully acknowledging and exploring the person's emotions.

**Rank 4**: The individual providing support in this conversation demonstrates a moderate level of empathy. They validate the person's feelings and offer to listen, but thei...