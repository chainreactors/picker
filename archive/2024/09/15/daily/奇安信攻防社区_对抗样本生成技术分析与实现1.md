---
title: 对抗样本生成技术分析与实现1
url: https://forum.butian.net/share/3722
source: 奇安信攻防社区
date: 2024-09-15
fetch_date: 2025-10-06T18:20:59.287614
---

# 对抗样本生成技术分析与实现1

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 对抗样本生成技术分析与实现1

* [安全工具](https://forum.butian.net/topic/53)

当我们提及人工智能，脑海中首先浮现的可能是自动驾驶汽车、智能家居、智能语音助手等前沿科技。然而，在这些光鲜亮丽的背后，隐藏着一个对人工智能系统构成严峻挑战的存在——对抗样本。

起源
==
当我们提及人工智能，脑海中首先浮现的可能是自动驾驶汽车、智能家居、智能语音助手等前沿科技。然而，在这些光鲜亮丽的背后，隐藏着一个对人工智能系统构成严峻挑战的存在——对抗样本。
![image-20240730114009039.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-79f427b95e984bd142e1ac23d340a003a70448ca.png)
对抗样本，简而言之，就是经过精心设计的输入数据，它们被用来欺骗人工智能系统，使其产生错误的判断或行为。这种挑战起源于机器学习和深度学习领域的研究，特别是当模型开始在各种任务上展现出惊人的性能时。
早期的对抗样本研究可以追溯到20世纪90年代，当时研究者们开始探索如何通过微小的扰动来干扰神经网络的分类决策。随着时间的推移，这一领域逐渐吸引了更多的关注，并成为了一个独立的研究方向。
对抗样本的出现并非偶然，而是人工智能系统内在脆弱性的体现。由于深度学习模型通常基于大量的数据进行训练，并通过复杂的数学运算进行决策，这使得它们在面对精心设计的输入时容易产生误判。
定义
==
具体来说，对抗样本（Adversarial Examples）是指通过在输入数据中加入精细的扰动，从而导致人工智能（AI）或机器学习（ML）模型产生错误输出的输入样本。这些扰动通常是微小且在人类感知下难以察觉，但对模型的预测结果却有显著的影响。对抗样本的研究主要集中在深度学习模型上，特别是在图像分类、语音识别和自然语言处理等领域。下面将详细说明对抗样本的概念和定义，并配合公式进行解释。
设有一个训练好的分类模型 ( f )，输入样本 ( x ) 以及其对应的正确标签 ( y )。模型 ( f ) 的预测目标是最大化 ( P(y | x) )。对抗样本的生成可以通过以下步骤进行描述：
1. \*\*输入扰动\*\*：生成一个对抗扰动 ( \\delta )，使得 ( x' = x + \\delta ) 是一个对抗样本。这个扰动 ( \\delta ) 应满足 ( |\\delta| ) 较小，以确保 ( x' ) 在人类感知上与 ( x ) 无显著差别。
2. \*\*目标模型误导\*\*：对抗扰动 ( \\delta ) 的设计目标是使模型 ( f ) 误分类，即 ( f(x') \\neq y ) 或者使模型输出特定的错误标签 ( y' )，即 ( f(x') = y' )，其中 ( y' \\neq y )。
数学上，这可以表示为：
![image-20240730114236515.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-999008d33af9b876b00a4c0f809690458d2cc085.png)
应用
==
对抗样本一开始是作为攻击手段提出的，但是它同时兼具积极应用与消极应用。
积极应用
----
#### 提升模型的鲁棒性
对抗样本被广泛用于提升机器学习模型的鲁棒性。通过在训练过程中加入对抗样本，模型可以学会更好地识别并抵御恶意输入。这种方法被称为对抗训练，通过增强模型的防御能力，减少其在实际应用中的脆弱性。
#### 检测和修复模型漏洞
对抗样本还可以用于检测机器学习模型中的潜在漏洞。通过生成并测试对抗样本，研究人员和开发者可以识别模型在处理特定输入时的弱点，并采取相应措施进行修复。例如，在自动驾驶汽车系统中，利用对抗样本可以发现图像识别模型在不同光照条件下的不足，从而改进其算法。
#### 提高安全性和隐私保护
对抗样本可以用于保护用户数据隐私。例如，在医疗领域，研究人员可以生成对抗样本来测试和评估医疗数据分析系统的安全性，确保敏感数据不会被恶意攻击者轻易获取。此外，对抗样本也可以用于创建隐私保护机制，通过在数据中引入噪声，使得攻击者难以恢复原始数据。
#### \*\*改进生成模型\*\*
生成对抗网络（GANs）利用对抗样本生成逼真的图像、音频和文本。通过生成对抗样本，GANs中的生成器和判别器相互竞争，不断改进各自的性能，从而生成高质量的样本。这种方法在艺术创作、数据增强和图像修复等领域有广泛应用。
### 消极应用
#### \*\*安全威胁和攻击\*\*
对抗样本可能被恶意使用，成为攻击机器学习系统的手段。例如，攻击者可以生成对抗样本来欺骗自动驾驶汽车的视觉系统，使其无法正确识别交通标志，从而导致危险情况的发生。在金融领域，对抗样本可能被用来误导信用评分系统，导致错误的信用评估。
#### 破坏模型的公平性
对抗样本可能被用来破坏机器学习模型的公平性。例如，在招聘系统中，攻击者可以使用对抗样本来操控候选人的评估结果，从而影响招聘决策。这种行为不仅损害了系统的公正性，也可能对候选人造成不公平的待遇。
#### 窃取机密信息
对抗样本可以用于攻击机器学习模型，以窃取其内部机密信息。例如，通过对抗性攻击，攻击者可以推测出模型的训练数据，进而获取敏感信息。这种攻击形式对包含个人数据和商业秘密的应用系统构成了严重威胁。
#### 扰乱公共秩序
对抗样本还可以被用于扰乱公共秩序。例如，恶意分子可以利用对抗样本生成虚假新闻或误导性信息，传播到社交媒体和新闻平台上，造成公众恐慌和误导。此外，对抗样本还可能被用来干扰舆论，影响政治选举和社会稳定。
在本文的剩余本文我们将来分析、学习、实现经典的对抗样本技术
虽然对抗样本属于AI安全领域，但是其应用是非常广泛的。
对抗样本能够显著提升网络空间安全从业人员的防御能力。通过生成和分析对抗样本，安全专家可以识别和修复网络系统中的漏洞，从而增强系统的整体防御能力。例如，在入侵检测系统中，利用对抗样本可以模拟潜在的攻击手段，使得系统能够更好地识别和应对真实的网络攻击。
而且我们可以使用对抗样本来改进现有的威胁检测技术。对抗样本能够帮助训练更加鲁棒的机器学习模型，从而提高其对恶意行为的检测能力。例如，在恶意软件检测中，使用对抗样本进行训练可以使模型更好地识别变种和变形的恶意软件，从而提高检测的准确性和效率。
此外，在渗透测试中，利用对抗样本可以模拟复杂的攻击场景，测试系统在面对各种对抗性攻击时的表现，从而发现并修复潜在的安全漏洞。这种方法有助于确保系统在实际运行中具有足够的防御能力。
对抗样本的研究涉及多个学科领域，如机器学习、计算机安全、统计学等。我们在学习对抗样本的过程中，可以促进与其他学科专家的合作，共同解决复杂的安全问题。例如，与机器学习专家合作，开发更加智能的安全检测系统；与统计学家合作，优化对抗样本的生成和分析方法。这种跨学科的合作有助于推动网络空间安全领域的发展。
在本文的后续部分我们将来学习、分析、实现经典的对抗样本技术
LBFGS
=====
理论
--
最早提出针对神经网络的对抗攻击的论文，当属《Intriguing properties of neural networks》。由Christian Szegedy等人撰写，探讨了深度神经网络的一些反直觉特性。文章主要揭示了两个关键问题：一是深度神经网络的高层单元（即神经网络的高级别特征）之间的区分度问题；二是深度神经网络对输入微小扰动的敏感性问题。 在以往的研究中，分析神经网络单元的语义意义通常通过找出能够最大化给定单元激活的输入集来进行。这种方法假设最后特征层的单元形成了一个特殊的基，有助于提取语义信息。例如，通过视觉检查图像，找到能够最大化特定隐藏单元激活值的图像。这种方法隐含地假设这些单元在提取输入域中有意义的变化时具有独特的作用。
这个工作提出了一种新的方法来分析神经网络的高层单元。作者发现，随机投影的激活值与自然基向量的激活值在语义上是不可区分的。这意味着，并不是单个单元，而是整个激活空间包含了神经网络高层的大部分语义信息。此外，文章还探讨了深度神经网络在输入微小扰动下的不稳定性。通过优化输入以最大化预测误差，作者能够找到一种几乎不可感知的扰动，这种扰动可以导致网络对图像的分类结果发生错误。这种现象被称为“对抗性示例”。
这一研究解决了对深度神经网络内部工作机制理解不足的问题。通过揭示高层单元的语义信息和输入扰动对网络预测的影响，文章为理解深度神经网络的黑箱特性提供了新的视角。特别是对抗样本的发现，揭示了深度学习模型在面对精心设计的输入扰动时的脆弱性，这对于提高模型的鲁棒性和泛化能力具有重要意义。此外，文章还提出了一种通过生成对抗性示例来改进模型训练的方法，这为深度学习模型的训练和优化提供了新的策略。
下图就是论文中该方法生成的一些对抗样本示例。
![image-20240728145113974.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-5631f447d2a5661ffcd9d2efb7513b6bd8696859.png)
现在我们形式化说明这一方法
1. \*\*定义分类器和损失函数\*\*： 假设有一个分类器
![image-20240728145139578.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-430482d004be1f024dbbe57032f5c80d04c04407.png)
它将图像像素值向量映射到离散标签集。分类器( f )有一个相关的连续损失函数
![image-20240728145151372.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-2f5168d1b84544ab13318198c4b676b600a375f2.png)
2. \*\*目标标签和原始图像\*\*：
对于给定的图像
![image-20240728145212620.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-dd811163781571efd9e67110323eab1c35f51d64.png)
和目标标签
![image-20240728145203887.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-ff460dd6c4b66c7a36b609d483f592eaa7bcabdd.png)
目标是找到一个扰动( r )，使得( x + r )被分类器( f )错误分类为标签( l )。
3. \*\*优化问题\*\*： 具体来说，作者通过解决以下带约束的优化问题来找到扰动( r )：
![image-20240728145232492.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-d70d48a56ab302a87e75ccc397f072bffed1e285.png)
其中，
![image-20240728145244797.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-4ef898f878dc68fe42a6f991ddbf572820df0c4b.png) 是扰动的欧几里得范数，( \[0, 1\]^m )是像素值的合法范围。
4. \*\*近似解\*\*： 由于直接求解上述问题在非凸神经网络中是困难的，作者使用L-BFGS算法来近似求解。具体步骤如下：
- 找到一个正数( c )，使得在约束
![image-20240728145257800.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-8e714bd5e7131b204d9dc958faa7592d562781e4.png)
下，损失函数的最小值满足( f(x + r) = l )。
- 通过线搜索找到这样的( c )，使得：
![image-20240728145307398.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-5f2c50899033e8dc8b36b16a2109b1d1f1d9e68f.png)
实现
--
这个类可以用于评估神经网络的鲁棒性，或者生成对抗样本。通过调整 `epsilon` 和 `steps` 参数，可以控制攻击的强度和复杂性。
![image-20240728145404807.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-83b3b9f7e0d6cef3785b89480c400e3487118eaa.png)
这段代码定义了一个名为 `LBFGSAttack` 的类，用于执行基于 L-BFGS 方法的对抗攻击。L-BFGS 是一种优化算法，用于在给定约束条件下找到函数的局部最小值。在这个类中，L-BFGS 被用来寻找最小的扰动，使得神经网络的预测结果发生变化。
1. \*\*初始化方法 (`\_\_init\_\_`)\*\*:
- 接受一个神经网络模型（`model`）和一个设备（`device`，例如 CPU 或 GPU）作为参数。
- 初始化一些类属性，包括对抗样本（`\_adv`）、模型、设备、输入数据的边界（`bounds`，这里设置为 0 到 1）和输出（`\_output`）。
2. \*\*调用方法 (`\_\_call\_\_`)\*\*:
- 这是类的主要接口，用于执行攻击。
- 接受输入数据（`data`）、目标标签（`target`）、扰动的最大幅度（`epsilon`）和最大迭代步数（`steps`）作为参数。
- 将输入数据和目标标签移动到指定的设备上。
- 使用二分搜索法来确定最佳的扰动幅度 `c`，以便在不超过 `epsilon` 的情况下成功执行攻击。
- 调用 `\_lbfgsb` 方法来执行 L-BFGS 优化，寻找最小的扰动。
3. \*\*损失函数 (`\_loss`)\*\*:
- 计算给定扰动 `adv\_x` 下的损失值。
- 将扰动后的数据转换为 PyTorch 张量，并设置 `requires\_grad=True` 以便计算梯度。
- 计算交叉熵损失（`ce`）和输入数据与扰动后数据之间的 L2 距离（`d`）。
- 将损失函数定义为交叉熵损失与 L2 距离的和，乘以一个常数 `c`。
- 反向传播计算梯度，并将其返回。
4. \*\*L-BFGS 优化 (`\_lbfgsb`)\*\*:
- 使用 `fmin\_l\_bfgs\_b` 函数执行 L-BFGS 优化。
- 设置优化变量的边界，以确保扰动在允许的范围内。
- 如果找到的最优解超出了边界，则将其裁剪到边界内。
- 将优化后的扰动转换回 PyTorch 张量，并计算模型的输出。
- 检查优化后的扰动是否导致模型预测的标签发生变化。
- 如果成功改变预测结果，则返回 `True`，否则返回 `False`。
执行攻击
![image-20240728145617326.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-ff51f3b2e27872ad51174709df3b5403329a0777.png)
这里以MNIST数据集为例
打印出原样本、对抗样本，以及各自被预测的标签
![image-20240728145659923.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/08/attach-1ad87c839d87f721bec14c08f8b81e064316de23.png)
第一列是原样本，第二列是对抗样本，标签在图像的上方，可以看到攻击成功了。
FGSM
====
《Explaining and Harnessing Adversarial Examples》可以说是对抗样本领域最经典的工作了。这篇论文由Ian J. Goodfellow等人撰写，首次发表于2015年的ICLR会议。文章的核心议题是对抗样本（adversarial examples）——即通过在数据集中的样本上施加小的、故意的最坏情况扰动而形成的输入，这些输入会导致机器学习模型以高置信度错误分类。
在本文之前，对于机器学习模型尤其是神经网络对对抗性样本的脆弱性，主要的解释集中在模型的非线性特性和过拟合上。研究者们认为，深层神经网络的高度非线性可能与模型对对抗性扰动的敏感性有关，同时可能还...