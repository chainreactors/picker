---
title: Dark LLM e cybercrime: la minaccia dei modelli linguistici malevoli in cybersecurity
url: https://www.ictsecuritymagazine.com/notizie/dark-llm/
source: ICT Security Magazine
date: 2025-08-22
fetch_date: 2025-10-07T00:48:28.252051
---

# Dark LLM e cybercrime: la minaccia dei modelli linguistici malevoli in cybersecurity

[Salta al contenuto](#main)

[![ICT Security Magazine](https://www.ictsecuritymagazine.com/wp-content/uploads/2016/01/logo-ict-security.jpg)](https://www.ictsecuritymagazine.com/)

* [Home](https://www.ictsecuritymagazine.com/)
* [Articoli](https://www.ictsecuritymagazine.com/argomenti/articoli/)
* RubricheEspandi
  + [Cyber Security](https://www.ictsecuritymagazine.com/argomenti/cyber-security/)
  + [Cyber Crime](https://www.ictsecuritymagazine.com/argomenti/cyber-crime/)
  + [Cyber Risk](https://www.ictsecuritymagazine.com/argomenti/cyber-risk/)
  + [Cyber Law](https://www.ictsecuritymagazine.com/argomenti/cyber-law/)
  + [Digital Forensic](https://www.ictsecuritymagazine.com/argomenti/digital-forensic/)
  + [Digital ID Security](https://www.ictsecuritymagazine.com/argomenti/digital-id-security/)
  + [Business Continuity](https://www.ictsecuritymagazine.com/argomenti/business-continuity/)
  + [Digital Transformation](https://www.ictsecuritymagazine.com/argomenti/digital-transformation/)
  + [Cyber Warfare](https://www.ictsecuritymagazine.com/argomenti/cyber-warfare/)
  + [Ethical Hacking](https://www.ictsecuritymagazine.com/argomenti/ethical-hacking/)
  + [GDPR e Privacy](https://www.ictsecuritymagazine.com/argomenti/gdpr-e-privacy/)
  + [IoT Security](https://www.ictsecuritymagazine.com/argomenti/iot-security/)
  + [Industrial Cyber Security](https://www.ictsecuritymagazine.com/argomenti/industrial-cyber-security/)
  + [Blockchain e Criptovalute](https://www.ictsecuritymagazine.com/argomenti/blockchain-e-criptovalute/)
  + [Intelligenza Artificiale](https://www.ictsecuritymagazine.com/argomenti/intelligenza-artificiale/)
  + [Geopolitica e Cyberspazio](https://www.ictsecuritymagazine.com/argomenti/geopolitica-cyberspazio/)
  + [Interviste](https://www.ictsecuritymagazine.com/argomenti/interviste/)
* [Notizie](https://www.ictsecuritymagazine.com/argomenti/notizie/)
* [Pubblicazioni](https://www.ictsecuritymagazine.com/pubblicazioni/)
* [Cybersecurity Video](https://www.ictsecuritymagazine.com/argomenti/cybersecurity-video/)
* [Eventi](https://www.ictsecuritymagazine.com/eventi/)
* [Newsletter](https://www.ictsecuritymagazine.com/newsletter/)

[Linkedin](https://www.linkedin.com/company/ict-security-magazine/) [YouTube](https://www.youtube.com/%40ictsecuritymagazine1403) [RSS](https://www.ictsecuritymagazine.com/feed/)

[![ICT Security Magazine](https://www.ictsecuritymagazine.com/wp-content/uploads/2016/01/logo-ict-security.jpg)](https://www.ictsecuritymagazine.com/)

Attiva/disattiva menu

[![Forum ICT Security 2025](https://www.ictsecuritymagazine.com/wp-content/uploads/banner-header-2025.jpg)](https://www.ictsecuritymagazine.com/eventi/forumictsecurity2025)

![Dark LLM cybersecurity: rappresentazione visiva delle minacce AI malevole, phishing automatizzato e strategie di difesa digitale](https://www.ictsecuritymagazine.com/wp-content/uploads/freepik__the-style-is-candid-image-photography-with-natural__63996.jpeg)

# Dark LLM e cybercrime: la minaccia dei modelli linguistici malevoli in cybersecurity

A cura di:[Redazione](#molongui-disabled-link)  Ore 21 Agosto 202517 Luglio 2025

I modelli linguistici di grandi dimensioni (*Large Language Models*, LLM) hanno rivoluzionato molti ambiti grazie alla loro capacità di generare testo coerente e contestuale.

Tuttavia, questa tecnologia presenta un **dualismo intrinseco**: gli stessi modelli che portano benefici possono essere sfruttati in modo dannoso. Di recente, è emersa la categoria dei cosiddetti **“Dark LLM”**, ovvero LLM *malevoli* concepiti o impiegati appositamente per scopi cybercriminali. Questi modelli – spesso privi di filtri di sicurezza – vengono *armati* da attori ostili per automatizzare e potenziare attacchi informatici su larga scala. In questo articolo analizzeremo il concetto di LLM malevoli con esempi concreti, valuteremo le implicazioni per la sicurezza (dal phishing avanzato alla disinformazione automatizzata, fino a scenari di attacchi zero-day generati da AI) e discuteremo strategie di difesa e linee guida etiche per contrastare l’uso maligno di tali modelli.

## **LLM malevoli: il lato oscuro dei modelli linguistici**

Un **LLM malevolo** è un modello di linguaggio intenzionalmente addestrato o configurato per generare *output* nocivi, oppure un [LLM](https://www.ictsecuritymagazine.com/articoli/llm/) legittimo manipolato (tramite *jailbreaking* o tecniche di *fine-tuning* su dati dannosi) per aggirare i filtri di sicurezza. Negli ultimi tempi sono apparsi sul dark web servizi di *AI-as-a-service* rivolti al cybercrime: ad esempio **WormGPT**, **FraudGPT** e **DarkBERT** (alternativa basata su modelli open source) sono LLM senza le restrizioni etiche di ChatGPT, offerti in abbonamento per scopi illeciti.

Tali strumenti promettono funzioni come la generazione di phishing altamente convincente, scrittura di malware personalizzati e persino creazione di *deepfake* testuali o audio. Ad esempio, WormGPT (basato sul modello open-source GPT-J) è stato addestrato su dati di malware e utilizzato per attacchi di **Business Email Compromise (BEC)**, producendo email di phishing in perfetto stile aziendale prive di qualunque filtro morale. Allo stesso modo, FraudGPT è pubblicizzato come *“soluzione all-in-one per cybercriminali”* in grado di scrivere codice malevolo, generare pagine di phishing e persino scoprire vulnerabilità. Il proliferare di questi **LLM oscuri** dimostra come i cybercriminali stiano sfruttando l’intelligenza artificiale generativa per abbassare la barriera d’ingresso al crimine informatico e aumentarne l’efficacia.

Oltre agli LLM deliberatamente creati per il crimine, va considerato il rischio di modelli compromessi. Un esempio è **PoisonGPT**, un modello sperimentale sviluppato da ricercatori di sicurezza per dimostrare come si possa *avvelenare* un LLM open-source inserendo informazioni false nelle sue conoscenze. Usando tecniche di *model editing*, PoisonGPT fu istruito a fornire disinformazione (es. dichiarare che la Tour Eiffel si trovi a Roma) restando al contempo all’apparenza affidabile e superando i normali benchmark di verifica.

Questo esperimento evidenzia la possibilità che attori malevoli distribuiscano modelli apparentemente legittimi ma con *backdoor* o bias intenzionali, mettendo a rischio chi li utilizza. Un ulteriore vettore di minaccia riguarda i repository di modelli pubblici: recentemente sono stati scoperti modelli condivisi su piattaforme open source contenenti **backdoor silenti**, capaci di eseguire codice arbitrario sui sistemi dei data scientist che li caricano. In sintesi, la categoria dei “Dark LLM” include sia modelli progettati per attività illecite, sia l’uso improprio di LLM esistenti tramite bypass dei controlli o alterazioni malevole.

## Implicazioni e scenari di attacco avanzati con Dark LLM

L’avvento di LLM malevoli amplia il ventaglio di minacce in **cybersecurity**, introducendo scenari di attacco prima teorici e ora concretamente fattibili. Di seguito analizziamo i principali vettori di abuso abilitati da questi modelli:

* **Phishing potenziato e social engineering automatizzato:** Gli LLM eccellono nel produrre testo persuasivo e contestuale, qualità ideale per campagne di *phishing* mirato. Studi recenti hanno dimostrato che modelli come GPT-4 possono generare email di **spear phishing** altamente realistiche e personalizzate su larga scala, a costi irrisori (frazioni di centesimo per email). Un ricercatore è riuscito a creare messaggi indirizzati a centinaia di parlamentari con pochi prompt e informazioni pubbliche, ottenendo testi credibili e su misura. Inoltre, tramite semplici stratagemmi di *prompt engineering* è possibile indurre anche LLM dotati di filtri a fornire istruzioni offensive – ad esempio come scrivere malware o manipolare le vittime – aggirando le salvaguardie integrate. Ciò significa che anche cybercriminali con scarsa padronanza linguistica o limitate abilità di scrittura possono orchestrare camp...