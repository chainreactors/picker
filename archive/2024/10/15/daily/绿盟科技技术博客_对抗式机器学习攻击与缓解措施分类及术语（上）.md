---
title: 对抗式机器学习攻击与缓解措施分类及术语（上）
url: https://blog.nsfocus.net/nist-ai-100-2e2023/
source: 绿盟科技技术博客
date: 2024-10-15
fetch_date: 2025-10-06T18:51:11.671298
---

# 对抗式机器学习攻击与缓解措施分类及术语（上）

* [登录](http://blog.nsfocus.net/wp-login.php)
* [注册](http://blog.nsfocus.net/wp-login.php?action=register)

[![Logo](http://blog.nsfocus.net/wp-content/uploads/2020/07/blog-logo.png)](https://blog.nsfocus.net/)

* [技术产品](https://blog.nsfocus.net/category/technology-product/)
* [数智安全](https://blog.nsfocus.net/category/digital-intelligence-secuirty/)
* [威胁通告](https://blog.nsfocus.net/category/threat-alert/)
* [研究调研](https://blog.nsfocus.net/category/security-research/)
* [洞见RSA](https://blog.nsfocus.net/category/rsac/)
* [公益译文](https://blog.nsfocus.net/category/translation/)
* [安全分享](https://blog.nsfocus.net/category/security-sharing/)

[![Logo](http://blog.nsfocus.net/wp-content/uploads/2020/07/blog-logo.png)](https://blog.nsfocus.net/)

* [技术产品](https://blog.nsfocus.net/category/technology-product/)
* [数智安全](https://blog.nsfocus.net/category/digital-intelligence-secuirty/)
* [威胁通告](https://blog.nsfocus.net/category/threat-alert/)
* [研究调研](https://blog.nsfocus.net/category/security-research/)
* [洞见RSA](https://blog.nsfocus.net/category/rsac/)
* [公益译文](https://blog.nsfocus.net/category/translation/)
* [安全分享](https://blog.nsfocus.net/category/security-sharing/)

# 对抗式机器学习攻击与缓解措施分类及术语（上）

### 对抗式机器学习攻击与缓解措施分类及术语（上）

[2024-10-14](https://blog.nsfocus.net/nist-ai-100-2e2023/ "对抗式机器学习攻击与缓解措施分类及术语（上）")[绿盟科技](https://blog.nsfocus.net/author/nsfocuser/ "View all posts by 绿盟科技")

阅读： 2,596

## **执行摘要**

NIST的可信及负责任人工智能报告旨在制定对抗性机器学习（AML）分类和术语，用以保护AI应用，防止被敌方操控。广义上讲，AI系统分为两类：预测式和生成式。AI系统的基本组成部分包括用于训练、测试和部署机器学习模型的数据、模型和流程，以及使用这些数据、模型和流程所需的基础设施。生成式AI系统在适配特定领域和用例后，可以链接到公司文档和数据库。除了大多数操作系统所面临的传统安全和隐私威胁之外，机器学习的数据驱动方法还在机器学习运行的各个阶段引入了其他安全和隐私挑战，包括训练数据可能被攻击者操控、模型漏洞可能被攻击者利用进而影响AI系统的性能，甚至恶意操控、篡改，或通过与模型交互获取人员、模型本身或企业私有数据等敏感信息。此类攻击已成为现实，其复杂性和潜在影响与日俱增。AML主要关注的是攻击能力及目标，以及在机器学习生命周期的开发、训练和部署阶段利用机器学习漏洞的攻击方法。AML还研究如何设计机器学习算法，以对抗这些安全和隐私挑战。就恶意攻击而言，机器学习的鲁棒性是指控制此类攻击后果的缓解措施。

本报告采用了NIST人工智能风险管理框架中机器学习系统的安全性、韧性和鲁棒性概念。安全性、韧性和鲁棒性由风险衡量，风险是衡量实体（例如系统）遭遇潜在情况或事件（例如攻击）威胁的可能性以及此类事件结果严重性的指标。但是，本报告并未就风险承受能力（组织或社会可接受的风险水平）提出建议，因为风险承受能力高度依赖于环境和应用/用例。这种广义的风险概念可用于评估和管理AI系统组件的安全性、韧性和鲁棒性。如何量化这些可能性非本文档关注内容。AML分类围绕AML风险评估的以下五个维度定义：（1）AI系统类型（预测式或生成式），（2）发起攻击时的机器学习生命周期阶段和学习方法，（3）攻击目的，（4）攻击能力，（5）攻击者对学习过程及其他方面的了解。

针对机器学习的有效攻击范围广，变化快，涵盖了机器学习生命周期的各个阶段——从设计和实施到训练、测试，最后到实际部署。这类攻击的性质和威力各不相同，有的利用机器学习模型中的漏洞，有的利用部署AI系统的基础设施中的漏洞。AI系统组件还可能受到各种非计划因素的不利影响，例如设计和实施缺陷以及数据或算法偏差，但这些因素不属于蓄意攻击。这些因素虽然也可能被攻击者利用，但不在AML文献或本报告讨论范围内。

本文件对AML领域的攻击进行了分类，并定义了相关术语。通过研究AML文献，我们对攻击进行了概念上的分层，其中包括主要的机器学习方法和攻击的生命周期阶段、攻击目的以及攻击者的能力和对学习过程的了解。报告还提出了相应的方法来缓解和控制攻击影响，指出了AI系统生命周期中需要考虑的相关挑战。报告中使用的术语与AML文献一致，并提供了词汇表，定义了与AI系统安全相关的关键术语，以方便普通用户理解。总而言之，本分类和术语为快速发展的AML提供了通用语言，统一了认识，其他的AI系统安全评估和管理标准及未来的实践指南可以此为参考。与分类一样，术语和定义并非详尽无遗，仅用以促进对AML文献中关键概念的理解。

## **概述**

人工智能（AI）系统在过去几年间在全球范围内快速发展。许多国家着手开发了AI系统并将其广泛部署在各个经济领域，从而催生出基于AI的各种服务，供人们在现实和虚拟世界使用。根据功能，AI系统分为两大类：预测式AI（PredAI）和生成式AI（GenAI）。这些系统逐步渗透到数字经济并成为日常生活中不可或缺的一部分，对其安全性、稳健性和韧性的要求也越来越高。这些运行属性是NIST AI风险管理框架和AI可信度分类中可信AI的关键要素。

然而，随着AI和机器学习（ML）在多个应用领域的重大进展，各种攻击也随之而来，可能导致重大故障，带来严重后果。

例如，在用于对象检测和分类的PredAI计算机视觉应用中，对输入图像进行对抗性扰动是广为人知的手段，曾导致自动驾驶汽车突然转向，进入逆行车道。有的自动驾驶汽车将停车标志错误分类为限速标志，导致关键物体从图像中消失，更有甚者，有的汽车在高级别安全环境中无法正确识别戴眼镜的人。同样，在医疗领域部署了越来越多的机器学习模型用以协助医生，这些机器学习模型可能会泄露医疗记录，暴露极其敏感的个人信息。

在GenAI中，大语言模型（LLM）也成为了互联网基础设施和软件应用程序不可或缺的一部分。大语言模型用于提升在线搜索能力，帮助软件开发人员编写代码，甚至可为客户服务机器人提供支持。大语言模型适配特定领域和用例后，可与公司数据库和文档集成，支持强大的检索增强生成（RAG）场景。然而，这些场景扩大了攻击面，可能造成企业的机密和私有数据泄露。

除BLOOM和LLaMA（元宇宙平台公司Meta公开发布的产品）外，大多数开发此类模型的公司都不会公布用于构建其语言模型的数据集的详细信息，但这些数据集不可避免地包含一些敏感的个人信息，如地址、电话号码和电子邮件地址，这给用户的在线隐私带来了严重风险。一条信息在数据集中出现的频率越高，模型在响应随机或精心设计的查询或提示时泄露该信息的可能性就越大。这些信息会与其他信息恶意关联，给相关人员带来破坏性后果，引入其他安全隐患。

攻击者还能操控PredAI和GenAI系统的训练数据，导致使用这些训练数据的AI系统受到攻击。黑客可能会从互联网上抓取训练数据，对数据进行大规模投毒，产生漏洞，进而破坏安全。

机器学习模型的规模不断扩大，许多组织开始依赖预训练模型，这些模型可以直接使用，也可以使用新数据集进行微调以执行不同的任务。这为恶意修改预训练模型创造了机会，攻击者可以通过插入木马来破坏模型的可用性，强制进行错误处理，或根据指示泄露数据。

从过往经验看，PredAI和GenAI系统中的各种输入模态（如文本、图像、语音、表格数据）都有特定的AI技术，每种技术都容易受到特定攻击。例如，针对图像分类任务的攻击不会直接转化为针对自然语言处理（NLP）模型的攻击。最近，在NLP中广泛使用的Transformer架构开始在计算机视觉领域中应用。此外，多模态机器学习在许多任务（包括生成式和分类任务）中取得了令人振奋的进展，有人开始尝试使用多模态学习来缓解单模态攻击。然而，同时针对多模态模型中所有模态的强大攻击也开始出现。

本质上，现代AI系统中使用的机器学习方法容易受到攻击，通过暴露模型的公共API就能攻击这些方法以及部署平台。本报告重点关注前者，将后者视为传统网络安全分类范畴。针对模型，攻击者只需要使用模型的公共接口并提供允许范围内的数据输入，即可突破数据和模型的机密性和隐私保护措施。从这个意义上讲，AML面临的挑战与密码学面临的挑战类似。现代密码学依赖于信息论意义上安全的算法。因此，人们只需关注如何可靠安全地实施这些算法——当然，这并非易事。与密码学不同的是，广泛使用的机器学习算法没有信息论意义上的安全证明。此外，开始有文献提及信息论上的*不可能*结果，这些结果限制了通用缓解技术的有效性。因此，针对不同种类攻击的缓解措施的诸多进展往往是经验性的，本质上效果有限。

本报告为下述各方面提供指导：

* 对机器学习和网络安全社区使用的AML术语进行标准化定义；
* 对AML中研究最广泛、最有效的攻击进行分类，包括：

（1）PredAI系统中的逃逸、投毒和隐私攻击；

（2）GenAI系统中的逃逸、投毒、隐私和滥用/误用攻击；

（3）针对多种数据模态的所有可行学习方法（例如有监督、无监督、半监督、联邦学习（又译“联合学习”、“联盟学习”）、强化学习）的攻击。

* 讨论AML中的潜在缓解措施以及部分现有缓解技术的局限性。

机器学习是一个快速发展的领域，我们预期会根据攻击和缓解方面的新发展而定期更新报告。

本报告的目标不是详尽调研AML的所有相关文献，实际上，这是一项几乎不可能完成的任务，因为在arXiv上搜索2021年和2022年的AML文章会得到 5000余篇参考文献。本报告的目标是对PredAI和GenAI系统的攻击及其缓解措施进行分类，从主要攻击类型着手：1）逃逸攻击，2）数据和模型投毒，3）数据和模型隐私，以及4）滥用/误用（仅限于GenAI）。

本报告包含三部分。第2部分围绕PredAI系统展开。2.1节介绍了PredAI系统的攻击分类。首先，对攻击目的进行了分类。在此基础上，定义了攻击者为实现相应目标须具备哪些能力。然后，介绍了各种能力所对应的攻击类型。2.2、2.3和2.4节分别讨论了主要的攻击类型：逃逸、投毒和隐私。攻击类型章节提供了每种攻击的对应缓解措施。第3部分围绕GenAI系统展开。3.1节介绍了GenAI系统的攻击分类。与PredAI情形类似，基于这种攻击分类，接下来定义了攻击者为实现对GenAI系统的相应目标须具备哪些能力。然后，介绍了各种能力所对应的攻击类型。第4部分探讨了该领域的其他挑战。

## **预测式****AI****分类**

### **攻击分类**

PredAI系统对抗性机器学习中的攻击分类如图1所示。攻击目的用三个圆圈表示，显示在各圆圈中心：破坏**可用性**、破坏**完整性、隐私**泄露。攻击者为实现这些目标必须具有的能力在圆圈外环显示。攻击类型用引线标注，与所需的攻击能力相连。多种攻击类型为实现相同目标所需要的能力如果相同，仅使用一个引线标注。不同攻击类型为实现同一目标若需要使用不同能力，则用虚线相连。

![](https://blog.nsfocus.net/wp-content/uploads/2024/10/WeChatc8dda832f42eefa11834dbc2c37c2a27-199x300.png)

**图****1** **预测式****AI****系统攻击分类**

这些攻击根据以下维度进行分类：（1）发起攻击时的学习方法和学习过程阶段，（2）攻击目的，（3）攻击能力，（4）攻击者对学习过程的了解。之前有几篇文章介绍了若干对抗性攻击分类框架，本次目标是创建对抗性机器学习攻击标准术语库，在相关文章中统一使用。

**1）学习阶段**

机器学习涉及训练阶段和部署阶段。在训练阶段进行模型学习，在部署阶段将模型部署在未标注的新数据样本上以生成预测。对于有监督学习，训练数据在标注后作为训练阶段训练算法的输入，同时，优化机器学习模型，最大程度地减小特定损失函数。通常，在实际部署模型之前，先进行机器学习模型的验证和测试。常见的有监督学习技术包括分类（涉及不连续的预测标签或类别）和回归（涉及连续的预测标签或响应变量）。

机器学习模型可分为生成式模型（即学习训练数据的分布并生成类似样本，如生成式对抗网络[GAN]和大语言模型[LLM]；详见第3部分）和判别式模型（即仅学习决策边界，如逻辑回归、支持向量机和卷积神经网络）。大多数的PredAI模型属于后者。

机器学习文献中的其他学习范式还包括无监督学习（使用未标注数据训练模型）、半监督学习（一小部分样本带标签，大多数样本未标注）、强化学习（智能体与环境交互并学习最佳策略以最大化奖励）、联邦学习（一组客户端通过与聚合模型更新的服务器通信来联合训练机器学习模型）和集成学习（通过组合多个模型的预测结果来提升预测性能）。

对抗性机器学习方面的文献主要考虑在训练阶段或机器学习部署阶段可能发生的针对AI系统的对抗性攻击。在机器学习训练阶段，攻击者会控制部分训练数据及其标签、模型参数或机器学习算法代码，发动各种投毒攻击。在机器学习部署阶段，机器学习模型已经过训练，攻击者可能发起逃逸攻击以破坏完整性并篡改机器学习模型的预测，还可能发起隐私攻击来推测有关训练数据或机器学习模型的敏感信息。

**训练阶段攻击。**机器学习训练阶段的攻击称为投毒攻击。在数据投毒攻击中，攻击者通过插入或篡改训练样本来控制训练数据子集。在模型投毒攻击中，攻击者会控制模型及其参数。数据投毒攻击适用于所有学习范式，而模型投毒攻击在联邦学习中最为普遍，攻击时客户端会将本地模型更新发送到聚合服务器。此外，在供应链攻击中，模型技术供应商可能会将恶意代码添加到模型中。

部署阶段攻击。推理或部署阶段的攻击有两种。第一种是逃逸攻击。攻击者篡改测试样本，创建与原始样本类似的（根据特定距离度量）对抗样本，但会根据需要更改模型预测。第二种是隐私攻击（包括成员推理攻击和数据重构攻击），通常由具有机器学习模型查询权限的攻击者发起。隐私攻击可以进一步分为数据隐私攻击和模型隐私攻击。

**2）攻击目的**

根据系统安全三要素（可用性、完整性、机密性），攻击目的从这三个维度可分为：破坏可用性、破坏完整性**、**隐私泄露。相应地，攻击成功则表示实现了其中一个或多个目标。图1根据攻击目的使用三个圆圈划分攻击类型，圆圈中心为攻击目的。

**破坏可用性。**可用性攻击是一种针对机器学习的无差别攻击，目的是在部署阶段破坏模型性能。这种攻击可通过数据投毒（攻击者控制一小部分训练集）或模型投毒（攻击者控制模型参数）实现，也可以通过查询权限发起能耗-时延（Energy-Latency）攻击。有人提出，数据投毒可用性攻击可针对支持向量机（Support Vector Machine）、线性回归甚至神经网络，而模型投毒攻击则专门针对神经网络和联邦学习。最近，针对神经网络出现了能耗-时延攻击，这种攻击只需要对模型具有黑盒访问权限，就可以影响计算机视觉和NLP中的各种任务。

**破坏完整性。**完整性攻击旨在破坏机器学习模型输出的完整性，导致机器学习模型输出错误预测。攻击者可通过部署阶段的逃逸攻击或训练阶段的投毒攻击来破坏完整性。逃逸攻击需要修改测试样本，创建对抗样本，诱导模型进行人类难以察觉的错误分类。通过投毒发起的完整性攻击可分为定向投毒、后门投毒和模型投毒。定向投毒中，攻击者试图破坏少数目标样本的完整性。为此，攻击者需要具有训练数据的控制权，以插入中毒样本。后门投毒攻击需要生成后门模式，添加到中毒样本和测试样本中，导致错误分类。在相关文献中，后门攻击是唯一需要同时控制训练和测试数据的攻击。模型投毒可导致定向攻击或后门攻击，攻击者通过修改模型参数来破坏完整性。这类攻击专门针对集中式学习和联邦学习。

**隐私泄露。**有的攻击者企图获取训练数据方面的信息（数据隐私攻击），有的企图获取机器学习模型方面的信息（模型隐私攻击）。攻击者之所以入侵训练数据隐私，其目的各有不同，包括数据重构（推测训练数据的内容或特征）、成员推理（推测训练集中是否存在某数据）、数据提取（从生成式模型中提取训练数据）和属性推理（推测训练数据分布的属性）。模型提取是一种模型隐私攻击，攻击者旨在提取模型相关信息。

**3）攻击能力**

攻击者可利用六种能力来实现其目标，如图1中的外环所示：

* 训练数据控制：攻击者可通过插入或篡改训练样本来控制训练数据的子集。此能力用于数据投毒攻击（如可用性投毒、定向投毒或后门投毒）。
* 模型控制：攻击者生成木马触发器并将其插入模型中或在联邦学习中发送恶意本地模型更新，以此控制模型参数。
* 测试数据控制：攻击者在模型部署时向测试样本添加扰动，比如，在逃逸攻击中生成对抗样本或发动后门投毒攻击
* 标签限制：此能力主要限制有监督学习中对训练样本标签的对抗性控制。干净标签投毒攻击中，攻击者不控制中毒样本的标签（现实投毒场景），而常规投毒攻击会控制中毒样本的标签。
* 源代码控制：攻击者篡改机器学习算法的源代码，例如随机数生成器或通常开源的第三方库。
* 查询权限：机器学习模型由云服务商管理（机器学习即服务（MLaaS））时，攻击者向模型提交查询，获取预测结果（标签或模型置信度）。黑盒逃逸攻击、能耗-时延攻击和所有隐私攻击都需要该能力。

请注意，即使攻击者无法篡改训练/测试数据、源代码或模型参数，也可以通过访问这些内容发起白盒攻击。有关攻击者知识的更多信息，详见2.1.4节。

图1将每种攻击与发起攻击所...