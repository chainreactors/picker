---
title: Inside DEF CON: Michael Brown on how AI/ML is revolutionizing cybersecurity
url: https://blog.trailofbits.com/2024/09/17/inside-def-con-michael-brown-on-how-ai-ml-is-revolutionizing-cybersecurity/
source: Trail of Bits Blog
date: 2024-09-18
fetch_date: 2025-10-06T18:25:18.376007
---

# Inside DEF CON: Michael Brown on how AI/ML is revolutionizing cybersecurity

[The Trail of Bits Blog](/ "The Trail of Bits Blog")

[![Trail of Bits Logo](/img/tob.png)](https://trailofbits.com "Trail of Bits")

# Inside DEF CON: Michael Brown on how AI/ML is revolutionizing cybersecurity

Trail of Bits

September 17, 2024

[aixcc](/categories/aixcc/), [machine-learning](/categories/machine-learning/)

At DEF CON, Michael Brown, Principal Security Engineer at Trail of Bits, sat down with Michael Novinson from [Information Security Media Group (ISMG)](https://www.bankinfosecurity.com/aimls-role-in-cybersecurity-balancing-innovation-safety-a-26029) to discuss four critical areas where AI/ML is revolutionizing security. Here’s what they covered:

**AI/ML techniques surpass the limits of traditional software analysis**
As Moore’s law slows down after 20 years of increasing computational power, traditional methods for finding, analyzing, and patching bugs yield diminishing returns. However, cloud computing and GPUs enable a new class of AI/ML systems that aren’t as constrained as conventional methods. By pivoting to AI/ML or a combination of AI/ML and traditional approaches, we can make new breakthroughs.

**Leverage AI/ML to solve complex security problems**
When solving computing problems using conventional methods, we use a prescriptive approach—we feed the system an algorithm that then produces a solution. In contrast, AI/ML systems are descriptive; we feed them numerous examples of what is right and wrong, and they learn to solve problems through their own modeling algorithms. This is beneficial In areas where we rely on highly specialized security engineers to solve complex, ‘fuzzy’ problems, because now AI/ML can step in. This is crucial as more complex problems are on the rise, yet there isn’t enough specialized expertise to address them all, and traditional methods fall short.

**Securing AI/ML systems is different than securing traditional systems**
Engineers at Trail of Bits have been researching ML vulnerabilities, both data- and deployment-born, and have discovered that the vulnerabilities affecting AI/ML systems differ significantly from those in traditional software. So to secure AI/ML, we need distinct methods to avoid missing large parts of the attack surface. Therefore, it’s crucial to acknowledge these differences, treat them as such, and harden AI/ML systems early in their development to prevent costly, persistent flaws—avoiding the unnecessary mistakes that plagued early iterations of Web 2.0, mobile apps, and blockchain.

**DARPA-funded projects, like AIxCC, apply AI/ML to traditional cyber issues**
DARPA’s AI Grand Cyber Challenge (AIxCC) challenges teams to develop AI/ML systems that address conventional security problems. Our team’s submission, Buttercup, is one of seven finalists advancing to next year’s [AIxCC finals](https://blog.trailofbits.com/2024/08/12/trail-of-bits-advances-to-aixcc-finals/), where it will compete on its ability to autonomously detect and patch vulnerabilities in real-world software.

**That’s a wrap! Watch the full video [here](https://www.youtube.com/watch?v=r8b95VESzRA)!**

Trail of Bits is at the forefront of integrating AI and ML into cybersecurity practices. Through our involvement in initiatives like the AI Cyber Challenge, we are addressing today’s security challenges while shaping the future of cybersecurity.
Reach out to us to learn more: [www.trailofbits.com/contact](http://www.trailofbits.com/contact)

Explore our AI/ML resources:

* Vulnerabilities and audits
  + [Hugging Face](https://github.com/trailofbits/publications/blob/master/reviews/2023-03-eleutherai-huggingface-safetensors-securityreview.pdf)
  + [LeftoverLocals](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)
  + [YOLOv7](https://blog.trailofbits.com/2023/11/15/assessing-the-security-posture-of-a-widely-used-vision-model-yolov7/)
* Research and blog
  + [Exploiting ML models with pickle file attacks: Part 1](https://blog.trailofbits.com/2024/06/11/exploiting-ml-models-with-pickle-file-attacks-part-1/)
  + [Auditing the Ask Astro LLM Q&A app](https://blog.trailofbits.com/2024/07/05/auditing-the-ask-astro-llm-qa-app/)
  + [PCC: Bold step forward, not without flaws](https://blog.trailofbits.com/2024/06/14/pcc-bold-step-forward-not-without-flaws/)
* Tools
  + [Fickling](https://blog.trailofbits.com/2024/03/04/relishing-new-fickling-features-for-securing-ml-systems/)
  + [Privacy Raven](https://github.com/trailofbits/PrivacyRaven)
* [AI safety and security training](https://22554992.fs1.hubspotusercontent-na1.net/hubfs/22554992/Resources/AI_ML_Training.pdf)

#### If you enjoyed this post, share it:

[X](https://x.com/trailofbits "X")

[LinkedIn](https://linkedin.com/company/trail-of-bits "LinkedIn")

[GitHub](https://github.com/trailofbits "GitHub")

[Mastodon](https://infosec.exchange/%40trailofbits "Mastodon")

[Hacker News](https://news.ycombinator.com/from?site=trailofbits.com "Hacker News")

#### Page content

#### Recent Posts

* [Taming 2,500 compiler warnings with CodeQL, an OpenVPN2 case study](/2025/09/25/taming-2500-compiler-warnings-with-codeql-an-openvpn2-case-study/)
* [Supply chain attacks are exploiting our assumptions](/2025/09/24/supply-chain-attacks-are-exploiting-our-assumptions/)
* [Use mutation testing to find the bugs your tests don't catch](/2025/09/18/use-mutation-testing-to-find-the-bugs-your-tests-dont-catch/)
* [Fickling’s new AI/ML pickle file scanner](/2025/09/16/ficklings-new-ai/ml-pickle-file-scanner/)
* [How Sui Move rethinks flash loan security](/2025/09/10/how-sui-move-rethinks-flash-loan-security/)

© 2025 Trail of Bits.
Generated with [Hugo](https://gohugo.io/) and [Mainroad](https://github.com/Vimux/Mainroad/) theme.