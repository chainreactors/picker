---
title: Apple “clearly underreporting” child sex abuse, watchdogs say
url: https://arstechnica.com/tech-policy/2024/07/apple-clearly-underreporting-child-sex-abuse-watchdogs-say/
source: Instapaper: Unread
date: 2024-07-27
fetch_date: 2025-10-06T17:44:37.080658
---

# Apple “clearly underreporting” child sex abuse, watchdogs say

[Skip to content](#main)
[Ars Technica home](https://arstechnica.com/)

Sections

[Forum](/civis/)[Subscribe](/subscribe/)[Search](/search/)

* [AI](https://arstechnica.com/ai/)
* [Biz & IT](https://arstechnica.com/information-technology/)
* [Cars](https://arstechnica.com/cars/)
* [Culture](https://arstechnica.com/culture/)
* [Gaming](https://arstechnica.com/gaming/)
* [Health](https://arstechnica.com/health/)
* [Policy](https://arstechnica.com/tech-policy/)
* [Science](https://arstechnica.com/science/)
* [Security](https://arstechnica.com/security/)
* [Space](https://arstechnica.com/space/)
* [Tech](https://arstechnica.com/gadgets/)

* [Feature](/features/)
* [Reviews](/reviews/)

* [AI](https://arstechnica.com/ai/)
* [Biz & IT](https://arstechnica.com/information-technology/)
* [Cars](https://arstechnica.com/cars/)
* [Culture](https://arstechnica.com/culture/)
* [Gaming](https://arstechnica.com/gaming/)
* [Health](https://arstechnica.com/health/)
* [Policy](https://arstechnica.com/tech-policy/)
* [Science](https://arstechnica.com/science/)
* [Security](https://arstechnica.com/security/)
* [Space](https://arstechnica.com/space/)
* [Tech](https://arstechnica.com/gadgets/)

[Forum](/civis/)[Subscribe](/subscribe/)

Story text

Size

Small
Standard
Large
Width
\*

Standard
Wide
Links

Standard
Orange

\* Subscribers only
  [Learn more](/store/product/subscriptions/)

Pin to story

Theme

* HyperLight
* Day & Night
* Dark
* System

Search dialog...

Sign In

Sign in dialog...

Sign in

CSAM "black hole"

# Apple “clearly underreporting” child sex abuse, watchdogs say

Report: Apple vastly undercounts child sex abuse materials on iCloud and iMessage.

[Ashley Belanger](https://arstechnica.com/author/ashleybelanger/)
–

Jul 22, 2024 12:46 pm
| [223](https://arstechnica.com/tech-policy/2024/07/apple-clearly-underreporting-child-sex-abuse-watchdogs-say/#comments "223 comments")

[![](https://cdn.arstechnica.net/wp-content/uploads/2024/07/GettyImages-2158085120.jpg)](https://cdn.arstechnica.net/wp-content/uploads/2024/07/GettyImages-2158085120.jpg)

Credit:
[Bloomberg / Contributor | Bloomberg](https://www.gettyimages.com/detail/news-photo/signage-outside-the-apple-inc-store-ahead-of-its-opening-at-news-photo/2158085120?adppopup=true)

Credit:
[Bloomberg / Contributor | Bloomberg](https://www.gettyimages.com/detail/news-photo/signage-outside-the-apple-inc-store-ahead-of-its-opening-at-news-photo/2158085120?adppopup=true)

Text
settings

Story text

Size

Small
Standard
Large
Width
\*

Standard
Wide
Links

Standard
Orange

\* Subscribers only
  [Learn more](/store/product/subscriptions/)

Minimize to nav

After years of [controversies over plans to scan iCloud](https://arstechnica.com/tech-policy/2021/08/apple-photo-scanning-plan-faces-global-backlash-from-90-rights-groups/) to find more child sexual abuse materials (CSAM), Apple [abandoned those plans](https://arstechnica.com/tech-policy/2023/09/apple-details-reasons-to-abandon-csam-scanning-tool-more-controversy-ensues/) last year. Now, child safety experts have accused the tech giant of not only failing to flag CSAM exchanged and stored on its services—including iCloud, iMessage, and FaceTime—but also allegedly failing to report all the CSAM that is flagged.

The United Kingdom’s National Society for the Prevention of Cruelty to Children (NSPCC) shared UK police data [with The Guardian](https://www.theguardian.com/technology/article/2024/jul/22/apple-security-child-sexual-images-accusation) showing that Apple is "vastly undercounting how often" CSAM is found globally on its services.

According to the NSPCC, police investigated more CSAM cases in just the UK alone in 2023 than Apple reported globally for the entire year. Between April 2022 and March 2023 in England and Wales, the NSPCC found, "Apple was implicated in 337 recorded offenses of child abuse images." But in 2023, Apple only [reported](https://www.missingkids.org/content/dam/missingkids/pdfs/2023-reports-by-esp.pdf) 267 instances of CSAM to the National Center for Missing & Exploited Children (NCMEC), supposedly representing all the CSAM on its platforms worldwide, The Guardian reported.

Large tech companies in the US must report CSAM to NCMEC when it's found, but while Apple reports a couple hundred CSAM cases annually, its big tech peers like Meta and Google report millions, NCMEC's report showed. Experts told The Guardian that there's ongoing concern that Apple "clearly" undercounts CSAM on its platforms.

Richard Collard, the NSPCC's head of child safety online policy, told The Guardian that he believes Apple's child safety efforts need major improvements.

“There is a concerning discrepancy between the number of UK child abuse image crimes taking place on Apple’s services and the almost negligible number of global reports of abuse content they make to authorities,” Collard told The Guardian. “Apple is clearly behind many of their peers in tackling child sexual abuse when all tech firms should be investing in safety and preparing for the rollout of the Online Safety Act in the UK.”

Outside the UK, other child safety experts shared Collard's concerns. Sarah Gardner, the CEO of a Los Angeles-based child protection organization called the Heat Initiative, told The Guardian that she considers Apple's platforms a "black hole" obscuring CSAM. And she expects that Apple's efforts to bring AI to its platforms will intensify the problem, potentially making it easier to spread AI-generated CSAM in an environment where sexual predators may expect less enforcement.

"Apple does not detect CSAM in the majority of its environments at scale, at all," Gardner told The Guardian.

Gardner agreed with Collard that Apple is "clearly underreporting" and has "not invested in trust and safety teams to be able to handle this" as it rushes to bring sophisticated AI features to its platforms. Last month, Apple [integrated ChatGPT into Siri, iOS and Mac OS](https://arstechnica.com/gadgets/2024/06/apple-integrates-chatgpt-into-siri-ios-and-mac-os/), perhaps setting expectations for continually enhanced generative AI features to be touted in future Apple gear.

“The company is moving ahead to a territory that we know could be incredibly detrimental and dangerous to children without the track record of being able to handle it,” Gardner told The Guardian.

So far, Apple has not commented on the NSPCC's report. Last September, [Apple did respond to the Heat Initiative's demands to detect more CSAM](https://arstechnica.com/tech-policy/2023/09/apple-details-reasons-to-abandon-csam-scanning-tool-more-controversy-ensues/), saying that rather than focusing on scanning for illegal content, its focus is on connecting vulnerable or victimized users directly with local resources and law enforcement that can assist them in their communities.

## Spiking sextortion, surge in AI-generated CSAM

Last fall, as Apple shifted its focus from detecting CSAM to supporting victims, every state attorney general in the US [signed a letter to Congress](https://arstechnica.com/information-technology/2023/09/ai-generated-child-sex-imagery-has-every-us-attorney-general-calling-for-action/) urging lawmakers to get serious about studying how children could be harmed by AI-generated CSAM.

Lawmakers entertained legislation but largely have not acted quickly enough to get ahead of the problem, child safety experts worry. By January, US law enforcement had sounded alarms, warning that a ["flood" of AI-generated CSAM](https://arstechnica.com/tech-policy/2024/01/surge-of-fake-ai-child-sex-images-thwarts-investigations-into-real-child-abuse/) was making it harder to investigate real-world child abuse. Adding to concerns, Human Rights Watch (HRW) researchers discovered that popular [AI models were being trained on real photos of kids](https://arstechnica.com/tech-policy/2024/06/ai-trained-on-photos-from-kids-entire-childhood-without-their-consent/), even when parents [use stricter privacy settings on so...