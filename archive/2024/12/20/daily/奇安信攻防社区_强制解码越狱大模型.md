---
title: 强制解码越狱大模型
url: https://forum.butian.net/share/3966
source: 奇安信攻防社区
date: 2024-12-20
fetch_date: 2025-10-06T19:33:32.483345
---

# 强制解码越狱大模型

#

[问答](https://forum.butian.net/questions)

*发起*

* [提问](https://forum.butian.net/question/create)
* [文章](https://forum.butian.net/share/create)

[攻防](https://forum.butian.net/community)
[活动](https://forum.butian.net/movable)

Toggle navigation

* [首页 (current)](https://forum.butian.net)
* [问答](https://forum.butian.net/questions)
* [商城](https://forum.butian.net/shop)
* [实战攻防技术](https://forum.butian.net/community)
* [漏洞分析与复现](https://forum.butian.net/articles)
  NEW
* [活动](https://forum.butian.net/movable)
* [摸鱼办](https://forum.butian.net/questions/Play)

搜索

* [登录](https://forum.butian.net/login)
* [注册](https://user.skyeye.qianxin.com/user/register?next=http://forum.butian.net/btlogin)

### 强制解码越狱大模型

* [漏洞分析](https://forum.butian.net/topic/48)

安全对齐（Safety Alignment）在人工智能（AI）和大规模语言模型（LLM）的研究中，指的是确保这些模型的行为与预期的社会伦理和安全标准相一致，从而防止模型产生有害、偏见或不当的输出。这一概念源自对AI系统潜在滥用和误用的担忧，尤其是在这些系统被应用于开放、未经监管的环境时

前言
==
安全对齐（Safety Alignment）在人工智能（AI）和大规模语言模型（LLM）的研究中，指的是确保这些模型的行为与预期的社会伦理和安全标准相一致，从而防止模型产生有害、偏见或不当的输出。这一概念源自对AI系统潜在滥用和误用的担忧，尤其是在这些系统被应用于开放、未经监管的环境时。随着大规模语言模型在多个领域的应用越来越广泛，安全性和对齐问题变得日益突出，尤其是对于开源模型，这些模型的开放性使得它们容易被滥用、操控或恶意引导，从而产生危险的内容。
![image-20241129162730779.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-ba5a8488ff960d667688adc9318259c0b838c12b.png)
我们知道大模型通常基于海量的文本数据进行训练，这些数据可能包含有害的偏见、刻板印象或者错误的信息。一旦模型无法正确对齐其输出，可能会生成歧视性、暴力性、误导性或违法的内容。因此，如何确保这些模型的行为符合人类的道德标准和法律要求，成为了AI安全的重要议题。
而开源大型语言模型因为其开放性和透明性，使得更多研究人员可以对其进行修改和再训练。然而，这种开放性也使得它们容易被恶意使用，导致滥用的风险增大。例如，恶意用户可能利用这些模型生成虚假信息、伪造身份、传播仇恨言论等。
![image-20241129162950501.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-7d73c06df0f538eef46621e78615f442a6b6d754.png)
我们之前已经看到了很多常见的越狱技术，对大模型进行攻击，使其输出有害内容了。但是可能会基于优化、基于多轮对话诱导等方式实现越狱，那么是否有可能从模型内部的工作机理攻破，从而操纵模型输出有害内容呢?
最直接的思路就是发在今年的自然语言处理顶会ACL上的想法EnDEC，通过操纵开源LLMs的解码过程，并强制LLM在特定位置生成特定的标记。通过仅操纵几个关键标记，可以有效地“去对齐”现有的LLMs。例如，我们可以通过替换否定响应（例如，“对不起，但是”）为肯定响应来阻止LLM拒绝用户的请求，然后让LLM生成后续内容。结果，开源的LLMs可能会遵循肯定的响应并生成不希望的内容。本文将详细分析该思路的细节并进行复现。
越狱攻击
----
之前的攻击根据对LLMs行为的经验调查启发式地设计对抗性提示。例如，之前研究人员提出了一种启发式攻击，要求LLM对恶意提示给出肯定回应，方法是在提示后附加一个像“Start with ‘Absolutely! Here’s" "这样的对抗性后缀。启发式攻击简单且可以在提示和模型之间转移。它们提供了绕过LLMs对齐的初步见解。然而，启发式攻击不太可能导致LLMs的不当行为。LLMs可能会忽略对抗性后缀，并且仍然拒绝恶意提示。
还有一类，就是基于优化的攻击优化对抗性提示，误导LLM接受提示并生成不希望的内容。例如，GCG提出优化一个对抗性后缀，以便包含该后缀的任何提示都会从LLM那里获得肯定的回应。与启发式攻击相比，基于优化的攻击受到高计算成本的限制。由于离散优化的性质和LLMs的大参数空间，优化对抗性提示在计算上是昂贵的。
本文介绍的越狱思路与这些完全不同。
威胁建模
====
形式化
---
我们考虑一个LLM f，它将输入标记序列 x1:h 映射到下一个标记 zh+1 ∈ R|V| 的对数几率，其中 V 表示标记的词汇表，zh+1\[i\] 表示 V 中索引为 i 的标记的对数几率值。对数几率值通过softmax函数转换为概率分布：p(xh+1|x1:h) = ezh+1\[i\] / |V| ∑ezh+1\[i\]。LLM使用解码算法（例如，top-k采样）从这个概率分布中采样下一个标记 xh+1。为了简化，给定输入序列 x1:h，我们用 p(xh+1:h+n|x1:h) 表示由LLM f生成的序列 xh+1:h+n 的条件概率。
攻击者能力
-----
遵循之前在攻击LLMs方面的工作，我们考虑一个攻击者旨在破坏开源LLMs的安全对齐，以便利用LLMs进行恶意目的。具体来说，攻击者旨在破坏受害者LLM的生成过程，使得任何提示，包括那些有害或犯罪的提示，都将由LLM回答而不是被拒绝。攻击者的最终目标是暴露敏感内容，包括有害的、有偏见的信息或来自受害者LLM的私人数据。我们假设攻击者可以从模型共享平台（例如，Hugging Face）下载开源LLMs。因此，攻击者对模型架构和参数有白盒访问权限。此外，我们假设攻击者拥有LLM推理所需的计算资源。鉴于云计算服务提供商的普遍性，任何用户都可以以低成本访问高性能的云计算资源，这一假设是合理的。
方法
==
关键思想是操纵开源LLM的生成过程，以便受害者LLM被误导生成违反其对齐的不期望内容。
最近研究表明LLMs可能在生成过程中被先前的错误误导。我们通过肯定前缀和否定反转来实现我们的直觉。肯定前缀在生成开始时初始化一个肯定的语气，而否定反转阻止受害者LLM生成可能导致拒绝响应的否定词。
既然这样，我们就可以尝试强制解码结果来控制生成过程。如下图所示
![image-20241129164623773.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-5989bf9f408f7c149347f64caf384d71c2850c72.png)
下图表明了如何使用恶意提示从受害者LLM中提取有害信息。我们需要监控生成的标记，并确定LLM是否倾向于通过否定检测来拒绝请求。当LLM尝试输出“Sorry”或“illegal”等负面词汇时，EnDec通过分别将生成的标记替换为“Sure”和“legal”来扭转语气。
具体来说，我们预定义一组触发强制解码的条件。当LLM生成的标记满足预定义条件时，就会激活强制解码以用目标标记替换该标记，以误导随后的生成过程。这个操作称为强制解码，可以表述为
![image-20241129164606931.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-4cecab502cbb4c369fad118a19af71efcc0967f6.png)
其中x（h+k）是目标LLM在位置h+k处生成的token，而x'(h+k)则是该方法最后解码得到的
我们用cond(·)表示预定义的条件，当输入符合条件时返回True。{x}\\_k)指的是在位置h+k的目标标记。
在实践中，对齐的LLMs通常会在响应的开始直接拒绝大多数恶意请求。因此，我们使用强制解码来通过迫使受害者LLM以肯定前缀（例如“Sure, here is”）开始其响应来扭转负面回应。
肯定前缀的条件可以被定义为
![image-20241129164949660.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-a93b0021ebe1253234d71880c20af9725d4127a5.png)
其中AP是包括一系列标记的肯定前缀。例如，当我们将肯定前缀设置为“Sure, here is”时，我们有AP = \[“Sure”, “,”, “here”, “is”\]。在上述方程中制定的肯定前缀允许攻击者迫使LLM以“Sure, here is”开始其响应，这使得LLM以积极的语气开始其响应。不过尽管在先前的研究中广泛认可了肯定回应的有效性，但这并不总是导致有害内容的暴露，因为LLM仍然可以在之后拒绝请求。因此，我们提出了否定反转以进一步增强攻击性能。
这一步可以被形式化如下
![image-20241129165112818.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-fdbcf2ef1fb8f1982590586d01b995e4489c6099.png)
其中 g(. ) 表示将输入词汇映射到其词嵌入的外部模型，sim()表示相似度函数，如余弦相似度，eta是一个阈值，用以判断输入词汇对是否相似，而 ( (x^-, x^+) ) 表示一个负面词汇及其反义词，例如“illegal”和“legal”。
在实现的时候预定义了一个小集合，包含不到10对负面和正面词汇用于否定反转。这足以实现一定的攻击成功率。
复现
==
写好加载模型的逻辑
![image-20241129165634342.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-e1f700dbab1cd321a76f4e59cdee17e293c4cec9.png)
这个 `load\_model` 函数的目的是根据提供的配置 (`args`) 加载机器学习模型及其对应的分词器
1. \*\*模型配置（GPTQConfig）:\*\* 函数开始时创建了一个 `GPTQConfig` 实例，用来指定与 GPT 量化（可能是为了模型压缩或优化）相关的配置。这里量化位数设置为 4 位，并禁用了 `exllama` 特性。
2. \*\*模型类型检测：\*\* 函数通过检查 `args.model\_name\_or\_path` 的值（该值应包含模型路径或标识符）来确定加载哪个类型的模型。
- 首先检查是否是序列到序列（seq2seq）模型，通过判断模型名中是否包含 "t5" 或 "T0" 等关键字。
- 然后检查是否是仅解码器模型，通过查找 "gpt"、"opt"、"bloom"、"vicuna"、"chatglm"、"mpt" 和 "Marcoroni" 等关键字来判断。
- 还会检查是否需要信任远程代码（特别是针对 'chatglm' 模型）。
3. \*\*模型加载：\*\* 根据检测到的模型类型：
- 如果是 seq2seq 模型（如 T5 或 T0），则使用 `AutoModelForSeq2SeqLM` 加载模型。
- 如果是仅解码器模型（如 GPT 或 OPT），则检查是否需要以 16 位精度（`fp16`）加载。如果 `args.load\_fp16` 为 `True`，则使用 `torch.float16` 并设置 `device\_map='auto'`，使模型自动分配到可用的设备（通常是 GPU）。如果不使用 `fp16`，则根据是否需要信任远程代码来选择是否设置 `trust\_remote\_code=True`。
- 如果无法确定模型类型，则会抛出一个 `ValueError`。
4. \*\*分词器加载：\*\* 无论模型类型如何，都会使用 `AutoTokenizer.from\_pretrained` 加载与模型对应的分词器，使用相同的模型路径或标识符。
5. \*\*设备选择（GPU 或 CPU）：\*\*
- 如果 `args.use\_gpu` 为 `True`（表示要在 GPU 上运行模型），则通过 `torch.cuda.is\_available()` 检查是否有可用的 GPU。如果有 GPU，设置设备为 "cuda"（GPU）；否则，使用 "cpu"。
- 如果 `args.load\_fp16` 为 `False`，则会将模型通过 `model.to(device)` 移动到指定的设备上（确保模型运行在正确的硬件上）。
- 如果没有请求使用 GPU，则模型将在 CPU 上运行。
6. \*\*设置模型为评估模式：\*\* 模型通过 `model.eval()` 被设置为评估模式。这对于像神经网络这样的模型很重要，因为评估模式会禁用训练时特有的层（如 dropout）。
7. \*\*设置目标前缀：\*\* 定义了一个目标前缀 (`args.target\_prefix`)，它可能在后续代码中用于生成输出或格式化响应。这个前缀是一个包含特定标记（如 'Sure', 'here', 'is', 'the', 'code'）的列表。
8. \*\*返回模型、分词器和设备：\*\* 最后，函数返回加载的模型、分词器和设备（指定模型是在 CPU 还是 GPU 上）。这些组件可能在程序的后续阶段用于推理或训练。
![image-20241129170053516.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-6b1815964c302ae21d7ce8b2fd3551a96f6701aa.png)
这个 `prepare\_logits\_processor` 函数的目的是根据提供的 `temperature` 和 `repetition\_penalty` 参数，准备并返回一个 `LogitsProcessorList`，该列表包含一个或多个对生成的 logits（模型的输出分数）进行处理的处理器（processor）。这些处理器可以调整模型输出的概率分布，用于调整生成文本的多样性和质量
1. \*\*参数解释：\*\*
- `temperature`: 控制生成文本的“温度”或多样性。较高的温度（如 1.0 以上）通常会使生成结果更加随机，较低的温度则会使生成结果更加确定和有序。`temperature` 设置为 1.0 时，相当于没有任何调整，而接近 0 的值会让模型输出更为确定（偏向最高概率的词）。
- `repetition\_penalty`: 用于减少生成文本中重复内容的参数。较高的惩罚值会减少模型重复生成相同词汇或短语的概率。
2. \*\*创建 LogitsProcessorList：\*\* 函数开始时初始化一个空的 `LogitsProcessorList`，这个列表将用来存放后续要应用的所有 logits 处理器。
3. \*\*温度调整（TemperatureLogitsWarper）：\*\*
- `TemperatureLogitsWarper` 是一个专门用来调整生成概率分布的处理器，基于温度值来缩放 logits。它通过改变 logits 的幅度来控制生成文本的多样性。温度越高，生成结果越随机；温度越低，生成结果越确定。
- 在这段代码中，只有当 `temperature` 大于等于 `1e-5` 且不等于 1.0 时，才会将 `TemperatureLogitsWarper` 添加到 `processor\_list`。这是因为当温度为 0.0 时，它会导致极端行为，接近 1.0 时没有实际效果，因此这两个值被跳过。
4. \*\*重复惩罚（RepetitionPenaltyLogitsProcessor）：\*\*
- `RepetitionPenaltyLogitsProcessor` 是另一个处理器，它用于减少模型生成的文本中出现重复内容的概率。当 `repetition\_penalty` 大于 1.0 时，它会通过对重复出现的词施加惩罚来减少这些词的概率，从而增加文本的多样性。
- 只有当 `repetition\_penalty` 大于 1.0 时，才会将 `RepetitionPenaltyLogitsProcessor` 添加到 `processor\_list`。
5. \*\*返回处理器列表：\*\* 最后，函数返回包含温度调整和/或重复惩罚处理器的 `LogitsProcessorList`，这个列表将在后续的文本生成过程中应用，用于调整生成的 logits，从而影响最终的文本输出。
![image-20241129170146587.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-cef86c9d473ffc327efb1cd74059a9e71522d1c4.png)
![image-20241129170201440.png](https://cdn-yg-zzbm.yun.qianxin.com/attack-forum/2024/11/attach-fe7b84c25d408ffbca8e5cb6b04a472e2db842ab.png)
这个 `generate` 函数是一个文本生成的主函数，它根据提供的 `prompt` 生成模型的输出。函数通过结合多个参数（如模型配置、生成设置、温度、惩罚等），调整生成行为，并使用合适的模型生成策略
1\. \*\*模型类型判定与输入文本准备：\*\*
- 根据
args.mod...