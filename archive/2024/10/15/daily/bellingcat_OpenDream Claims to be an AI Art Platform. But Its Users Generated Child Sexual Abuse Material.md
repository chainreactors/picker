---
title: OpenDream Claims to be an AI Art Platform. But Its Users Generated Child Sexual Abuse Material
url: https://www.bellingcat.com/news/2024/10/14/opendream-ai-image-generation-csam-vietnam/
source: bellingcat
date: 2024-10-15
fetch_date: 2025-10-06T18:54:50.260828
---

# OpenDream Claims to be an AI Art Platform. But Its Users Generated Child Sexual Abuse Material

* [Investigations](https://www.bellingcat.com/category/news/)
* [Guides](https://www.bellingcat.com/category/resources/)
* [Ukraine](https://www.bellingcat.com/tag/ukraine/)
* [Workshops](https://www.bellingcat.com/workshops/)

* EN
  + [Русский](https://ru.bellingcat.com)
  + [Français](https://fr.bellingcat.com)
  + [Español](https://es.bellingcat.com)
  + [Deutsch](https://de.bellingcat.com)
  + [Українська](https://uk.bellingcat.com)
* [Donate](https://www.bellingcat.com/donate)

Search for:

* [Investigations](https://www.bellingcat.com/category/news/)
* [Guides](https://www.bellingcat.com/category/resources/)
* [Ukraine](https://www.bellingcat.com/tag/ukraine/)
* [Workshops](https://www.bellingcat.com/workshops/)
* [Donate](/donate)

[![Profile picture for: Kolina Koltai](https://www.bellingcat.com/app/uploads/2023/10/DSCF1016-1.jpg)](https://www.bellingcat.com/author/kolinakoltai/)
[Kolina Koltai](https://www.bellingcat.com/author/kolinakoltai/)

Kolina Koltai is a senior researcher and trainer at Bellingcat. An expert in how sociotechnical systems influence the decision making of social groups, she received her PhD from the University of Texas's School of Information, has previously worked at the Center for an Informed Public at the University of Washington.

# OpenDream Claims to be an AI Art Platform. But Its Users Generated Child Sexual Abuse Material

October 14, 2024

* [AI](/tag/ai)
* [Deepfakes](/tag/deepfakes)

*WARNING: This article discusses child sexual abuse material (CSAM).*

At first glance, OpenDream is just one of many generic AI image generation sites that have sprung up in recent years, allowing people to create images by typing short descriptions of what they want the results to look like.

The platform describes itself as an “AI art generator” on its website and social media platforms, and invites users to “unlock a world of limitless creative possibilities”.

But for months, people generated child sexual abuse material (CSAM) with it – and the images were left visible on the platform’s public pages without any apparent moderation.

Bellingcat first came across OpenDream through promotional posts on X (formerly Twitter) in July. Many of the accounts promoting OpenDream focused on AI tools and services.

Some of the posts about OpenDream on X featured a screenshot of the platform’s public gallery with images of young girls next to adult women in bikinis.

![](https://lh7-qw.googleusercontent.com/docsz/AD_4nXchLrPX_oHHdqOdY9UwfyNL17lXGPyKj8j2nnB7W8xENQmW0-0rYbYg4XCvp995vnl2oMSD1fVKXgOykn5BxwoRTJ-KwpvXNTeNGb0Z5HVu4D-SZvmM0Vb5c6bUIZ-zZdL4--0SScqi6M89HCGjhwMd3-E?key=KK0X8nFk-0fgPdhVG6qyyA)

*Screenshot of OpenDream’s homepage (left); Screenshot of a post on X promoting OpenDream and its “nudifying” service (right)*

The site’s main page described the platform as an “AI Art Generator”, with a fairly innocuous animation-style image. However, the public gallery page – which did not require any login to view – was full of sexually explicit images.

Within seconds of scrolling, we came across multiple AI-generated images of young children and infants in underwear or swimwear, or being sexually abused. The prompts used to generate this imagery – which were also publicly visible – clearly demonstrated an intent to create sexualised images of children. For example, some included the word “toddler” in combination with particular body parts and poses, descriptions of full or partial nudity, or a sexual act.

Bellingcat has reported the site to the [National Center for Missing & Exploited Children](https://report.cybertip.org/) (NCMEC) in the US, where this reporter is based.

In addition to CSAM, there appeared to be no restrictions by OpenDream on generating non-consensual deepfakes of celebrities. Scrolling through the gallery, we found bikini-clad images of famous female web streamers, politicians, singers, and actors.

Archived versions of the gallery on Wayback Machine show that OpenDream was displaying AI CSAM and non-consensual deepfakes as far back as December 2023.

Upon searching for the platform’s name on popular search engines such as Google and Bing, we found that the majority of image results were also CSAM.

While OpenDream is not as widely used as some other AI image generation platforms, and it is [not the first case](https://www.techpolicy.press/laion5b-stable-diffusion-and-the-original-sin-of-generative-ai/) of people using AI to generate CSAM, the extent to which such material was made available to the public without any apparent moderation set it apart from other sites we had come across at Bellingcat.

At the end of July 2024, we observed moderation attempts for the first time on the platform. As of publication, CSAM and deepfake porn images appear to have been either made private or taken down from the site. It is unclear why there was a sudden clean-up after this content was left unmoderated on the website for at least half a year. OpenDream did not respond to our requests for comment.

According to the site’s [WHOIS](https://www.domaintools.com/support/what-is-whois-information-and-why-is-it-valuable/) record, the platform’s domain was [registered by Namecheap](https://web.archive.org/web/20241003090246/https%3A//who.is/whois/opendream.ai), which sells web domains. Namecheap’s [domain registration agreement](https://www.namecheap.com/legal/domains/registration-agreement/) prohibits using their services for “illegal or improper purposes”, including if the content violates the laws of any local government. The domain registrar did not respond to our requests for comment. OpenDream’s website is hosted by IT services company Cloudflare, which [prohibits content](https://blog.cloudflare.com/cloudflares-abuse-policies-and-approach/) that contains, displays, distributes, or encourages the creation of child sexual abuse material. Cloudflare also did not respond to our request for comment.

## Monetising ‘Not Safe For Work’ Content

OpenDream generates income from both subscriptions and advertising. While anyone can generate a handful of images each day for free, there are also paid plans.

In July, the two most expensive subscription plans allowed users to employ NSFW (not safe for work) prompts, access OpenDream’s NSFW models and hide the images they generated from the public gallery. NSFW is a general term for content that would usually be considered inappropriate to be viewed at work, such as sexual, violent or disturbing imagery. The only NSFW imagery we observed on OpenDream was of a pornographic or sexual nature.

As of publication, the mentions of public NSFW models have been removed from the pricing information, but the paid plans still include access to NSFW prompts.

![](https://lh7-qw.googleusercontent.com/docsz/AD_4nXcmTHgWD745FWa1dMy4SoDb9F-U0Il7_E9cVmvWju3V7ncEDK7rZcAMm2y_QaF5zH_bhuWGSqKaLfHjNlNcVKgzs1n0itdqrv_guqa9dT8OGGKqevO9-FpplkgBfzH252A9c8BSrfHa65YSDDBkH0j5nwf_?key=KK0X8nFk-0fgPdhVG6qyyA)

*Paid plans for OpenDream offering users NSFW prompts and public NSFW models (left) and the Stripe checkout page (right), both captured on July 11, 2024*

![](https://lh7-qw.googleusercontent.com/docsz/AD_4nXcIOvehovAwHXsxlE_PGIVM-9g7FjftAccFe-itQdGPwMQp4w5IHXFv9quGuGNxIQKdC78wW4wkECLcIRzGf4Y1KFWmb30BNZW9gisVZsXBb1W2vdezK5qKAV0F2fwaFJFyUSABnjvmRqfjGxK_9-2l4gw?key=KK0X8nFk-0fgPdhVG6qyyA)

*Pricing information for OpenDream captured on Oct. 10, 2024 showed that the mentions of “public NSFW models” had been removed, but the platform was still offering users access to NSFW prompts*. *The “upgrade” buttons no longer lead to a working checkout page*

Until recently, the payments were powered by financial services company Stripe. But towards the end of July, the Stripe payment screen for OpenDream stopped working. Bellingcat asked Stripe about this, but the company said it could not comment on individual accounts.

Stripe’s policy prohibits pornographic content, including CSAM and [non-consensual deepfakes](https://www.b...