---
title: 马斯克等人联名签署：《暂停巨型人工智能实验： 一封公开信》
url: https://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499404&idx=1&sn=07e07eb316f0fa9b4e93a9bcd13c34dc&chksm=97e94366a09eca7014db310d7b638c8e2423af97c9baf5334a3bae74c819ae1ef031fc052515&scene=58&subscene=0#rd
source: 网安寻路人
date: 2023-03-30
fetch_date: 2025-10-04T11:09:18.205195
---

# 马斯克等人联名签署：《暂停巨型人工智能实验： 一封公开信》

![cover_image](https://mmbiz.qpic.cn/mmbiz_jpg/jErr674f9m9WpjctX5xGVDFUjTUXCA4gwljomic3ibhPh4JeicgnPgJDrjGCVUljA9bgbSyicCIYQQicSTPIGMUwZ6w/0?wx_fmt=jpeg)

# 马斯克等人联名签署：《暂停巨型人工智能实验： 一封公开信》

洪延青

网安寻路人

**编者按**

关于LLMs（大型语言模型）的风险和监管，本公号发布过以下文章：

1. [ChatGPT是网络上的一个模糊的JPEG文件（外专观点）](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499037&idx=1&sn=edf919c148180dbce5fab395d737cb90&chksm=97e940f7a09ec9e1e5f9c62de2ba580d3e35c5d5a5c093c07b7dea83425d4fa6aeee24ca3c0f&scene=21#wechat_redirect)
2. [ChatGPT是如何劫持民主进程（外专观点）](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499047&idx=1&sn=d03eecb21b9ebc57280cb2f07fc467a3&chksm=97e940cda09ec9db66b84ad7178bd7e9a52a2e71cb238e3b91ae495eb55acc94aaafd311d95c&scene=21#wechat_redirect)
3. [ChatGPT：欧洲禁止Replika "虚拟伴侣"聊天机器人应用（外媒编译）](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499054&idx=1&sn=b53342145b2eaacdee56a017294201aa&chksm=97e940c4a09ec9d297cf8792919a347b9e119ab8e32c9fd89f2dd1083370f519faed22ce8c17&scene=21#wechat_redirect)
4. [ChatGPT背后的核心技术【好文转载】](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499104&idx=1&sn=c65ee739966ef5eaa307aa3d4fcad333&chksm=97e9408aa09ec99c49648d0e8cff4c10c4464ae52624c42d8a0631ffa4dd23ae33aeb2b2d626&scene=21#wechat_redirect)
5. [基辛格、施密特等：ChatGPT预示着一场智力革命（外媒编译）](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499227&idx=1&sn=2e6e03d91c0c0011b435bbd187f9407d&chksm=97e94031a09ec927c1d179c35810e4964ce622e472bf1ca40d27983b25520ade36597e408345&scene=21#wechat_redirect)
6. [OpenAI数据处理协议-全文翻译](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499306&idx=1&sn=e8ec9c1cef1e529b0c8b8d4fa243c4e4&chksm=97e943c0a09ecad670e23f6009a4c13f0548345fe92abf98c4964d8aeae2d980ba4c48893da7&scene=21#wechat_redirect)
7. [欧盟《人工智能法案》如何监管GPT模型：选译](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499319&idx=1&sn=b2500f095599060cc4edd04c5802e637&chksm=97e943dda09ecacbb7d9b6921c64fcb88dac64b9ceb3af745555ae95ec0ee424021b53338fc9&scene=21#wechat_redirect)
8. [《GPT-4 ：通用人工智能的火花》论文内容精选与翻译（好文转载）](http://mp.weixin.qq.com/s?__biz=MzIxODM0NDU4MQ==&mid=2247499393&idx=1&sn=b31493fcc0611ca08590f25645717740&chksm=97e9436ba09eca7d7c4ce9a9709ccfc6415c6e9553884af13513766a29f24582dd5d18e27cff&scene=21#wechat_redirect)

今天和大家分享的马斯克等千人联名签署的公开信《暂停巨型人工智能实验：一封公开信》。具体网页见：https://futureoflife.org/open-letter/pause-giant-ai-experiments/

![](https://mmbiz.qpic.cn/mmbiz_png/jErr674f9m9WpjctX5xGVDFUjTUXCA4g1qHRuicTf2OJKA12PxoSPzB5SBrRpib1WbXUkkxZ6nAsicFkia39HvI5iaw/640?wx_fmt=png)

具有与人类相媲美的智能的人工智能系统，可能对社会和人类产生深远的风险，这已经得到了大量研究[1]的证实，并得到了顶级AI实验室的支持[2]。正如广泛认可的阿西洛马人工智能原则所述，先进的人工智能可能会给地球生命历史带来深刻的变革，因此应该用相应的关注和资源进行规划和管理。遗憾的是，即使在近几个月里，AI实验室在开发和部署越来越强大的数字智能方面陷入了失控的竞赛，而这些数字智能甚至连它们的创造者都无法理解、预测或可靠地控制，这种规划和管理的水平也没有得到实现。

当代AI系统现已在通用任务方面具有与人类竞争的能力[3]，我们必须问自己：我们是否应该让机器充斥着我们的信息渠道，传播宣传和虚假信息？我们是否应该将所有工作自动化，包括那些有成就感的工作？我们是否应该开发可能最终超越、取代我们的非人类智能？我们是否应该冒着失去对文明的控制的风险？这些决策不能交由未经选举的科技领袖来做。只有在我们确信AI系统的影响将是积极的，风险将是可控的情况下，才能开发强大的AI系统。这种信心必须有充分的理由，并随着系统潜在影响的增大而增加。OpenAI最近关于通用人工智能的声明指出：“在某些时候，可能需要在开始训练未来系统之前获得独立审查，而且各个最先进的系统训练尝试，应该就限制用于创建新模型的计算增长速度达成一致意见。”我们同意。那个时候就是现在。

因此，我们呼吁所有AI实验室立即暂停至少6个月的时间，停止训练比GPT-4更强大的AI系统。这种暂停应该是公开的、可验证的，并包括所有关键参与者。如果这样的暂停无法迅速实施，政府应该介入并实行禁令。

AI实验室和独立专家应利用这段暂停时间共同制定并实施一套先进的、共享的AI设计和开发的安全协议，这些协议应由独立的外部专家严格审计和监督。这些协议应确保遵守它们的系统在排除合理怀疑之后是安全的[4]。这并不意味着暂停AI发展，而只是从危险的竞赛中退一步，避免发展出具有突现能力的更大、更不可预测的黑箱模型。

AI研究和发展应该重新关注如何使当今强大的、处于技术前沿的系统更加精确、安全、可解释、透明、稳健、一致、值得信赖和忠诚。

与此同时，AI开发者必须与政策制定者合作，大力加快AI治理系统的发展。这些至少应包括：致力于AI的新的、有能力的监管机构；对高能力AI系统和大量计算能力的监督和追踪；源头和水印系统，以帮助区分真实与合成以及追踪模型泄漏；强大的审计和认证生态系统；对AI造成的损害承担责任；技术AI安全研究的充足公共资金支持；以及应对AI带来的严重经济和政治颠覆（特别是对民主制度）的有充足资源的机构。

人类可以与AI共享繁荣的未来。在成功创造出强大的AI系统后，我们现在可以享受一个“AI夏天”，在这个时期，我们可以收获硕果，为所有人带来明确的利益，让社会有机会适应。社会已经在其他可能对社会产生灾难性影响的技术上按下了暂停键[5]。我们在这里也可以做到。让我们享受一个漫长的AI夏天，而不是毫无准备地跳入秋天。

[1]

Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S.
(2021, March). On the Dangers of Stochastic Parrots: Can Language Models
Be Too Big?. In Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency (pp. 610-623).

Bostrom, N. (2016). Superintelligence. Oxford University Press.

Bucknall, B. S., & Dori-Hacohen, S. (2022, July). Current and near-term AI as a potential existential risk factor. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 119-129).

Carlsmith, J. (2022). Is Power-Seeking AI an Existential Risk?. arXiv preprint arXiv:2206.13353.

Christian, B. (2020). The Alignment Problem: Machine Learning and human values. Norton & Company.

Cohen, M. et al. (2022). Advanced Artificial Agents Intervene in the Provision of Reward. AI Magazine, 43(3) (pp. 282-293).

Eloundou, T., et al. (2023). GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.

Hendrycks, D., & Mazeika, M. (2022). X-risk Analysis for AI Research. arXiv preprint arXiv:2206.05862.

Ngo, R. (2022). The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626.

Russell, S. (2019). Human Compatible: Artificial Intelligence and the Problem of Control. Viking.

Tegmark, M. (2017). Life 3.0: Being Human in the Age of Artificial Intelligence. Knopf.

Weidinger, L. et al (2021). Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.

[2]

Ordonez, V. et al. (2023, March 16). OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks: 'A little bit scared of this'. ABC News.

Perrigo, B. (2023, January 12). DeepMind CEO Demis Hassabis Urges Caution on AI. Time.

[3]

Bubeck, S. et al. (2023). Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv:2303.12712.

OpenAI (2023). GPT-4 Technical Report. arXiv:2303.08774.

[4]

Ample legal precedent exists – for example, the widely adopted OECD AI Principles require that AI systems "function appropriately and do not pose unreasonable safety risk".

[5]

Examples include human cloning, human germline modification, gain-of-function research, and eugenics.

AI systems with human-competitive intelligence can pose profound risks to society and humanity, as shown by extensive research[1] and acknowledged by top AI labs.[2] As stated in the widely-endorsed Asilomar AI Principles, *Advanced
AI could represent a profound change in the history of life on Earth,
and should be planned for and managed with commensurate care and
resources*. Unfortunately, this level of planning and management is
not happening, even though recent months have seen AI labs locked in an
out-of-control race to develop and deploy ever more powerful digital
minds that no one – not even their creators – can understand, predict,
or reliably control.

Contemporary AI systems are now becoming human-competitive at general tasks,[3] and we must ask ourselves: *Should* we let machines flood our information channels with propaganda and untruth? *Should* we automate away all the jobs, including the fulfilling ones? *Should* we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? *Should* we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. **Powerful
AI systems should be developed only once we are confident that their
effects will be positive and their risks will be manageable.** This confidence must be well justified and increase with the magnitude of a system's potential effects. OpenAI's recent statement regarding artificial general intelligence, states that *"At
some point, it may be important to get independent review before
starting to train future systems, and for the most advanced efforts to
agree to limit the rate of growth of compute used for creating new
models."* We agree. That point is now.

Therefore, **we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4**.
This pause should be public and verifiable, and include all key actors.
If such a pause cannot be enacted quickly, governments should step in
and institute a moratorium.

AI labs and independent experts should u...